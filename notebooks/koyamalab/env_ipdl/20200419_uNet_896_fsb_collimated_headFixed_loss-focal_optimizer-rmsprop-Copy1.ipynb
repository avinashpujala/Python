{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import relevant code\n",
    "import os, sys, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import dask\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import models\n",
    "from skimage.util import montage\n",
    "import glob\n",
    "\n",
    "#--- Import my code\n",
    "codeDir = r'V:/code/python/code'\n",
    "sys.path.append(codeDir)\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "import apCode.SignalProcessingTools as spt\n",
    "from apCode.machineLearning.unet import model\n",
    "from apCode.behavior import FreeSwimBehavior as fsb\n",
    "from apCode import geom\n",
    "import apCode.hdf as hdf\n",
    "from apCode import util\n",
    "from rsNeuronsProj import util as rsp\n",
    "\n",
    "#--- Setting seed for reproducability\n",
    "seed = 143\n",
    "np.random.seed = seed\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "#--- Auto-reload modules\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(time.ctime())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Load a compiled U-net model and train on data, or load a pre-trained network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_unet = r'Y:\\Avinash\\Ablations and Behavior'\n",
    "learning_rate = 'default'\n",
    "\n",
    "if learning_rate is 'default':\n",
    "    print('Default learning rate')\n",
    "    rmsprop = keras.optimizers.rmsprop()\n",
    "else:\n",
    "    rmsprop = keras.optimizers.rmsprop(learning_rate=learning_rate)\n",
    "unet = model.get_unet(img_width=896, img_height=896, img_channels=1, optimizer=rmsprop, loss=model.focal_loss)\n",
    "\n",
    "\n",
    "# file_weights = ft.findAndSortFilesInDir(dir_unet, ext='hdf', search_str='best_weights_fsb')[-1]\n",
    "# unet.load_weights(os.path.join(dir_unet, file_weights))\n",
    "\n",
    "# path_unet = glob.glob(os.path.join(dir_unet, 'trainedU*.h5'))[-1]\n",
    "# print(path_unet)\n",
    "# unet = mlearn.loadPreTrainedUnet(path_unet) # Load pre-trained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Read images and masks for training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dir_xls_train = r'Y:\\Avinash\\Ablations and Behavior'\n",
    "file_xls_train = 'Paths_to_fish_training_images.xlsx'\n",
    "sheet_name = 'Uncropped'\n",
    "xls_train = pd.read_excel(os.path.join(dir_xls_train, file_xls_train), sheet_name=sheet_name)\n",
    "xls_train = xls_train.loc[xls_train.exptType=='fsb']\n",
    "\n",
    "imgDims = unet.input_shape[1:3]\n",
    "imgs_train, masks_train = mlearn.read_training_images_and_masks(np.array(xls_train.pathToImages), \n",
    "                                                    np.array(xls_train.pathToMasks), imgDims=imgDims)\n",
    "masks_train = (masks_train>0).astype(int)\n",
    "print(f'Training on {imgs_train.shape[0]} of dimensions {imgs_train.shape[1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Evaluate pre-training performance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = unet.evaluate(imgs_train[..., None], masks_train[..., None], batch_size=6, verbose=1)\n",
    "print(np.c_[unet.metrics_names, metrics])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Randomly select and plot images and masks for checking* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.permutation(np.arange(imgs_train.shape[0]))\n",
    "ind=inds[0]\n",
    "m = montage((imgs_train[ind], masks_train[ind]), rescale_intensity=True, grid_shape=(1,2))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(m, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Define Tensorboard as Checkpoint callbacks for later evaluation and storing best weights from training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Tensorflow callback for later evaluation\n",
    "# dir_log = os.path.join(dir_unet, f'unet_training_logs_headFixed_{util.timestamp()}')\n",
    "# tensorboard = TensorBoard(log_dir=dir_log, histogram_freq=1, write_images=True, batch_size=6)\n",
    "# keras_callbacks = [tensorboard]\n",
    "\n",
    "\n",
    "#%% Checkpointer callback for storing best weights\n",
    "fp = os.path.join(dir_unet, f'best_weights_fsb_collimated_headFixed_{util.timestamp()}.hdf')\n",
    "checkpointer = ModelCheckpoint(filepath=fp, monitor='val_dice_coef', verbose=1,\\\n",
    "                               save_best_only=True, mode='max', save_weights_only=True)\n",
    "\n",
    "# keras_callbacks.append(checkpointer)\n",
    "keras_callbacks = [checkpointer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Augment images before training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Augment before training\n",
    "upSample=5\n",
    "aug_set=('rn', 'sig', 'log', 'inv', 'heq', 'rot', 'rs')\n",
    "%time imgs_aug, masks_aug, augs = mlearn.augmentImageData(imgs_train, masks_train,\\\n",
    "                                                          upsample=upSample, aug_set=aug_set)\n",
    "\n",
    "imgs_aug = mlearn.prepare_imgs_for_unet(imgs_aug, unet)\n",
    "masks_aug = mlearn.prepare_imgs_for_unet(masks_aug, unet)\n",
    "masks_aug= (masks_aug>0).astype(int)\n",
    "print(f'Augmentation: {len(imgs_train)} --> {len(imgs_aug)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Evaluate pre-training performance on augmented images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = unet.evaluate(imgs_aug, masks_aug, batch_size=6, verbose=1)\n",
    "print(np.c_[unet.metrics_names, metrics])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.permutation(np.arange(imgs_aug.shape[0]))\n",
    "ind=inds[0]\n",
    "m = montage((imgs_aug[ind][..., 0], masks_aug[ind][..., 0]), rescale_intensity=True, grid_shape=(1,2))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(m, cmap='viridis')\n",
    "plt.title(augs[ind], fontsize=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Train with callbacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 6 # For 1024 x 1024 images I can't help but use this size\n",
    "epochs = 150\n",
    "validation_split = 0.1\n",
    "checkPoint = True\n",
    "\n",
    "his = unet.fit(imgs_aug, masks_aug, epochs=epochs, batch_size=batch_size,\\\n",
    "               validation_split=validation_split, callbacks=keras_callbacks, verbose=1)\n",
    "\n",
    "# his = unet.fit(imgs_aug, masks_aug, epochs=epochs, batch_size=batch_size,\\\n",
    "#                validation_split=validation_split, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Load the best weight and save the model as HDF file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the best weights and save\n",
    "fn = ft.findAndSortFilesInDir(dir_unet, search_str='best_weights_fsb_collimated_headFixed', ext='hdf')[-1]\n",
    "unet.load_weights(os.path.join(dir_unet, fn))\n",
    "\n",
    "#%% Save the U-net\n",
    "# dir_unet = r'Y:\\Avinash\\Ablations and Behavior'\n",
    "fn = f'trainedU_fsb_collimated_headFixed_{unet.input_shape[1]}x{unet.input_shape[2]}_{util.timestamp()}.h5'\n",
    "unet.save(os.path.join(dir_unet, fn))\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Re-evaluate metrics after training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = unet.evaluate(imgs_aug, masks_aug, batch_size=6, verbose=1)\n",
    "print(np.c_[unet.metrics_names, metrics])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Plot training metrics* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his = unet.history.history\n",
    "print(his.keys())\n",
    "plt.style.use(('seaborn-poster', 'seaborn-white'))\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(his['val_dice_coef'],'.', label='validation set')\n",
    "plt.plot(his['dice_coef'], label='training set')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Dice coefficient', fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(his['val_loss'],'.', label ='validation set')\n",
    "plt.plot(his['loss'], label = 'training set')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Binary cross-entropy loss', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Look at outputs/activations of different layers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Look at activations of layers\n",
    "ind_layer = 35\n",
    "ind_img = np.random.permutation(len(imgs_aug))[0]\n",
    "\n",
    "img = imgs_aug[ind_img][None,...]\n",
    "layer_outputs = [layer.output for layer in unet.layers]\n",
    "layer = unet.layers[ind_layer]\n",
    "activation_model = models.Model(inputs=unet.input, outputs=layer_outputs[ind_layer])\n",
    "activations = np.transpose(np.squeeze(activation_model.predict(img)), (2, 0, 1))\n",
    "\n",
    "img_rs = volt.img.resize(np.squeeze(img), activations.shape[-2:])\n",
    "foo =  np.concatenate((img_rs[None,...], activations), axis=0)\n",
    "\n",
    "nCols=8\n",
    "nRows = (len(activations)//nCols)+1\n",
    "m  = montage(foo, rescale_intensity=True, grid_shape=(nRows, nCols))\n",
    "plt.figure(figsize=(20, 20*nCols/nRows))\n",
    "plt.imshow(m, cmap='viridis')\n",
    "plt.title(f'Example image (# {ind_img} top left) and outputs at layer {ind_layer}({layer.name})',\\\n",
    "          fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters, biases =[], []\n",
    "# for layer in unet.layers:\n",
    "#     if 'conv' in layer.name:\n",
    "#         f, b = layer.get_weights()\n",
    "#         filters.append(f)\n",
    "# #         print(f.shape)\n",
    "# f = np.transpose(np.squeeze(filters[0]), (2, 0, 1))\n",
    "# m = montage(f, rescale_intensity=True)\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# plt.imshow(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Read a contiguous set of unseen images for predicting with U-net*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_imgs = r'Y:\\Avinash\\Head-fixed tail free\\GCaMP imaging\\2019-12-31\\f2\\001_h\\behav\\Autosave0_[00-11-1c-f1-75-10]_20191231_032423_AM'\n",
    "\n",
    "dir_imgs = r'N:\\Avinash\\Ablations and Behavior\\Ventral RS\\20160523\\Fish1_ctrl1\\fastDir_08-18-16-182847\\vib'\n",
    "%time imgNames = ft.findAndSortFilesInDir(dir_imgs, ext='bmp')[:750]\n",
    "\n",
    "imgs = volt.img.readImagesInDir(dir_imgs, imgNames=imgNames)\n",
    "imgs_rs = volt.img.resize(imgs, unet.input_shape[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Generate probability maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_prob = np.squeeze(unet.predict(imgs_rs[..., None], batch_size=6, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Make a movie to demonstrate segmentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "merge_ch = 0\n",
    "fps = 50\n",
    "cropSize = (256, 256)\n",
    "\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "fp = fsb.track.findFish(-imgs_rs*imgs_prob, back_img=None)\n",
    "fp_interp = spt.interp.nanInterp1d(fp)\n",
    "imgs_rs_crop = volt.img.cropImgsAroundPoints(imgs_rs, fp_interp, cropSize=cropSize)\n",
    "imgs_prob_crop = volt.img.cropImgsAroundPoints(imgs_prob, fp_interp, cropSize=cropSize)\n",
    "\n",
    "inds = np.arange(50, 750)\n",
    "imgs_prob_255 = (imgs_prob_crop*255).astype(int)\n",
    "imgs_rs_rgb = np.array([gray2rgb(_, alpha=0.5) for _ in imgs_rs_crop])\n",
    "\n",
    "imgs_rs_rgb[..., merge_ch] = (alpha*imgs_rs_rgb[..., merge_ch] + (1-alpha)*imgs_prob_255).astype(int) \n",
    "ani =volt.animate_images(imgs_rs_rgb[inds], fps=fps, fig_size=(15, 15))\n",
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *From segmented fish images to tail curvature timeseries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time imgs_fish = fsb.fish_imgs_from_raw(imgs_rs, unet)[0]\n",
    "%time midlines, inds_kept_midlines = fsb.track.midlines_from_binary_imgs(imgs_fish)\n",
    "kappas = fsb.track.curvaturesAlongMidline(midlines, n=50)\n",
    "tailAngles = np.cumsum(kappas, axis=0)\n",
    "ta = hf.cleanTailAngles(tailAngles)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Plot tail angles extracted from segmented fish*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot tail angles\n",
    "\n",
    "from matplotlib.colors import DivergingNorm\n",
    "norm = DivergingNorm(0, vmin=-100, vmax=100)\n",
    "fh, ax = plt.subplots(2,1, figsize=(20,10), sharex=True)\n",
    "\n",
    "ax[0].imshow(ta[:, inds], aspect='auto', norm=norm, cmap='coolwarm', vmin=-100, vmax=100)\n",
    "ax[0].set_yticks([0, 24, 49])\n",
    "ax[0].set_yticklabels(['Head', 'Middle', 'Tail'])\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_title('Cumulative curvature along the tail')\n",
    "\n",
    "ax[1].plot(ta[-1][inds])\n",
    "ax[1].set_xlim(0, len(inds))\n",
    "ax[1].set_xticks([0, len(inds)//2, len(inds)])\n",
    "ax[1].set_xlabel('Image frame #')\n",
    "ax[1].set_ylabel('Tail bend amplitude ($^o$)')\n",
    "ax[1].set_title('Tail tail curvature timeseries');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Try Focal Loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Instantiate U-net with focal loss specified during compilation\n",
    "unet_fl = model.get_unet(img_width=896, img_height=896, img_channels=1, loss=model.focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Checkpointer callback for storing best weights\n",
    "fp = os.path.join(dir_unet, f'best_weights_headFixed_{util.timestamp()}.hdf')\n",
    "checkpointer = ModelCheckpoint(filepath=fp, monitor='val_dice_coef', verbose=1,\\\n",
    "                               save_best_only=True, mode='max', save_weights_only=True)\n",
    "\n",
    "keras_callbacks = [checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Augment before training\n",
    "upSample=4\n",
    "aug_set=('rn', 'sig', 'log', 'inv', 'heq', 'rot', 'rs')\n",
    "# aug_set=('rn', 'sig', 'log', 'inv', 'heq', 'rot')\n",
    "%time imgs_aug, masks_aug, augs = mlearn.augmentImageData(imgs_train, masks_train,\\\n",
    "                                                          upsample=upSample, aug_set=aug_set)\n",
    "\n",
    "imgs_aug = mlearn.prepare_imgs_for_unet(imgs_aug, unet)\n",
    "masks_aug = mlearn.prepare_imgs_for_unet(masks_aug, unet)\n",
    "print(f'Augmentation: {len(imgs_train)} --> {len(imgs_aug)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 6 # For 1024 x 1024 images I can't help but use batch_size=6\n",
    "epochs = 25\n",
    "validation_split = 0.1\n",
    "checkPoint = True\n",
    "\n",
    "his = unet_fl.fit(imgs_aug, masks_aug, epochs=epochs, batch_size=batch_size,\\\n",
    "                   validation_split=validation_split, callbacks=keras_callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his = unet_fl.history.history\n",
    "print(his.keys())\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.style.use(('seaborn-poster','fivethirtyeight', 'seaborn-white'))\n",
    "plt.subplot(121)\n",
    "plt.plot(his['val_dice_coef'],'.', label='validation set')\n",
    "plt.plot(his['dice_coef'], label='training set')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Dice coefficient', fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(his['val_loss'],'.', label ='validation set')\n",
    "plt.plot(his['loss'], label = 'training set')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Foal loss ($\\gamma = 2, unbalanced$)', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_prob = np.squeeze(unet_fl.predict(imgs_rs[..., None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "merge_ch = 0\n",
    "fps = 50\n",
    "inds = np.arange(450, 3000)\n",
    "imgs_prob_255 = (imgs_prob*255).astype(int)\n",
    "imgs_rs_rgb = np.array([gray2rgb(_, alpha=0.5) for _ in imgs_rs])\n",
    "\n",
    "imgs_rs_rgb[..., merge_ch] = (alpha*imgs_rs_rgb[..., merge_ch] + (1-alpha)*imgs_prob_255).astype(int) \n",
    "ani =volt.animate_images(imgs_rs_rgb[inds], fps=fps, fig_size=(15, 15))\n",
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time imgs_fish = fsb.fish_imgs_from_raw(imgs_rs, unet_fl)[0]\n",
    "%time midlines, inds_kept_midlines = fsb.track.midlines_from_binary_imgs(imgs_fish)\n",
    "kappas = fsb.track.curvaturesAlongMidline(midlines, n=50)\n",
    "tailAngles = np.cumsum(kappas, axis=0)\n",
    "ta = hf.cleanTailAngles(tailAngles)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot tail angles\n",
    "\n",
    "from matplotlib.colors import DivergingNorm\n",
    "norm = DivergingNorm(0, vmin=-100, vmax=100)\n",
    "fh, ax = plt.subplots(2,1, figsize=(20,10), sharex=True)\n",
    "\n",
    "ax[0].imshow(ta[:, inds], aspect='auto', norm=norm, cmap='coolwarm', vmin=-100, vmax=100)\n",
    "ax[0].set_yticks([0, 24, 49])\n",
    "ax[0].set_yticklabels(['Head', 'Middle', 'Tail'])\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_title('Cumulative curvature along the tail')\n",
    "\n",
    "ax[1].plot(ta[-1][inds])\n",
    "ax[1].set_xlim(0, len(inds))\n",
    "ax[1].set_xticks([0, len(inds)//2, len(inds)])\n",
    "ax[1].set_xlabel('Image frame #')\n",
    "ax[1].set_ylabel('Tail bend amplitude ($^o$)')\n",
    "ax[1].set_title('Tail tail curvature timeseries');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Free swim behavior*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
