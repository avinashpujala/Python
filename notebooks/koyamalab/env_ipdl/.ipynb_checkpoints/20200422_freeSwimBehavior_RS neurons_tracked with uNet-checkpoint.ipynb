{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Change log_\n",
    "1. Using a U-net trained on a much larger dataset of free swimming images. Assessment revealed great performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tifffile as tff\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "codeDir = r'\\\\dm11\\koyamalab/code/python/code'\n",
    "sys.path.append(codeDir)\n",
    "\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "from apCode import util as util\n",
    "import rsNeuronsProj.util as rsp\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducability\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load dataframe containing paths to data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_df = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations'\n",
    "path_df = glob.glob(os.path.join(dir_df, 'Ablation data summary*.pkl'))[-1]\n",
    "\n",
    "df = pd.read_pickle(path_df)\n",
    "dir_save = os.path.join(dir_df, f'session_{util.timestamp()}')\n",
    "os.makedirs(dir_save, exist_ok=True)\n",
    "df_orig = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load pretrained U-net (loss_func = focal_loss; optimizer='Adam')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_unet = r'Y:\\Avinash\\Ablations and Behavior'\n",
    "file_unet = ft.findAndSortFilesInDir(dir_unet, ext='h5', search_str='trainedU_fsb_896x896_2020')[-1]\n",
    "print(f'Loading U-net: {file_unet}')\n",
    "path_unet = os.path.join(dir_unet, file_unet)\n",
    "\n",
    "unet = mlearn.loadPreTrainedUnet(path_unet)\n",
    "print(f'U net dims = {unet.input_shape}')\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Track fish in each of the datasets and create a larger dataframe that also contains tail angles info*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Create another dataframe with tail angles info and merge with original dataframe using common FishIdx column\n",
    "trlLen = 750\n",
    "df = df_orig.loc[df_orig.Illumination=='diffuse']\n",
    "motion_thresh_perc=None\n",
    "\n",
    "paths_retrack = []\n",
    "nFish = df.shape[0]\n",
    "trackedWithNN = np.zeros_like(df.TrackedWithNN)\n",
    "hFilePaths = []\n",
    "for iPath in range(df.shape[0]):\n",
    "    print(f'{iPath+1}/{nFish}')\n",
    "    df_ = df.iloc[iPath]\n",
    "    path_imgs = rsp.remove_suffix_from_paths(df_.Path, suffix='proc')[()]\n",
    "    try:\n",
    "        hFilePath = fsb.tail_angles_from_raw_imgs_using_unet(path_imgs, unet,\\\n",
    "                                                             motion_thresh_perc=motion_thresh_perc)\n",
    "        trackedWithNN[iPath]=1\n",
    "        hFilePaths.append(hFilePath)\n",
    "    except Exception:\n",
    "        print('Tracking failed!')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hFilePaths), len(df), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].Path_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsb.tail_angles_from_raw_imgs_using_unet??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Filter dataframe based on quality of tracking, then interpolate tail angles for rest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thr_track = 85\n",
    "trlLen=750\n",
    "\n",
    "df = dataFrame_orig.loc[dataFrame_orig.perc_tailAngles_tracked>=thr_track]\n",
    "ta_full=[]\n",
    "for iFish in range(df.shape[0]):    \n",
    "    df_ = df.iloc[iFish]\n",
    "#     print(f'Fish # {iFish+1}/{df.shape[0]}, perc_tracked = {df_.perc_tailAngles_tracked} %')\n",
    "    ta_ = df_.tailAngles\n",
    "    inds_ta = df_.inds_tailAngles\n",
    "    nTot = np.maximum(df_.totalNumPts, inds_ta.max()+1)\n",
    "    ta_nan = np.zeros((ta_.shape[0], nTot), dtype='float')*np.nan\n",
    "    ta_nan[:, inds_ta] = ta_\n",
    "#     print('Interpolating...')\n",
    "#     taf = spt.interp.nanInterp2d(ta_nan, method='nearest')\n",
    "    taf = dask.delayed(spt.interp.nanInterp2d)(ta_nan, method='nearest')\n",
    "    ta_full.append(taf)\n",
    "with ProgressBar():\n",
    "    ta_full = dask.compute(*ta_full, scheduler='processes')\n",
    "ta_full_clip, ta_tot = [], []\n",
    "for ta_ in ta_full:\n",
    "    nTrls = ta_.shape[1]//trlLen\n",
    "    n = nTrls*trlLen\n",
    "    ta_ = ta_[:, :n]\n",
    "    ta_full_clip.append(ta_)\n",
    "    ta_tot.append(ta_[-1])\n",
    "ta_full = ta_full_clip\n",
    "df = df.assign(tailAngles=ta_full_clip, tailAngles_tot=ta_tot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Clean up tail angles in the dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_behav = 1/500\n",
    "nFish = len(ta_full)\n",
    "ta_full_ser = np.concatenate(ta_full, axis=1)\n",
    "%time ta_ser_clean, _, svd = hf.cleanTailAngles(ta_full_ser, dt=dt_behav)\n",
    "\n",
    "tLens = np.cumsum(np.array([ta_.shape[1] for ta_ in ta_full]))\n",
    "ta_full_clean = np.hsplit(ta_ser_clean, tLens)\n",
    "ta_full_clean.pop()\n",
    "\n",
    "ta_tot = []\n",
    "for ta_ in ta_full_clean:\n",
    "    ta_tot.append(ta_[-1])\n",
    "df = df.assign(tailAngles=ta_full_clean, tailAngles_tot=ta_tot)\n",
    "ta_full = ta_full_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Save the dataframe for future use_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'dataFrame_rsNeurons_ablations_svdClean_{util.timestamp()}.pkl'\n",
    "%time df.to_pickle(os.path.join(dir_save, fname))\n",
    "print(f'Saved at\\n{dir_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iFish = (0, 0)\n",
    "xl = (-20, 320) # ms\n",
    "yl = (-200, 200)\n",
    "alpha = 0.3\n",
    "\n",
    "inds_ctrl = np.where((df.AblationGroup=='ventralRS') & (df.Treatment=='ctrl'))[0]\n",
    "inds_abl = np.where((df.AblationGroup=='ventralRS') & (df.Treatment=='abl'))[0]\n",
    "\n",
    "fh, ax = plt.subplots(3, 1, figsize=(20, 12), sharex=True, sharey=False)\n",
    "\n",
    "y = ta_tot[inds_ctrl[iFish[0]]]*1.3\n",
    "y = spt.chebFilt(y, 1/500, 5, btype='highpass')\n",
    "y_trl = y.reshape(-1, trlLen)\n",
    "t = (np.arange(y_trl.shape[1])-50)*(1/500)*1000\n",
    "if len(y_trl)<25:\n",
    "    y_trl = util.CombineItems(N=25).fit(y_trl).transform(y_trl)\n",
    "\n",
    "ax[0].plot(t, y_trl.T, alpha=alpha)\n",
    "ax[0].set_title('$Control$', fontsize=16)\n",
    "ax[1].set_ylim(yl)\n",
    "\n",
    "env = spt.emd.envelopesAndImf(y)['env']['diff']\n",
    "env = env.reshape(-1, trlLen)\n",
    "env = env-env[:, :40].mean(axis=1)[:, None]\n",
    "mu = env.mean(axis=0)\n",
    "sem = env.std(axis=0)/env.shape[0]**0.5\n",
    "ax[2].fill_between(t, mu-sem, mu+sem, color=plt.cm.tab10(0), alpha=0.5, label='Control')\n",
    "ax[2].set_ylim(-20, yl[1]*0.55)\n",
    "\n",
    "y = ta_tot[inds_abl[iFish[1]]]\n",
    "y = spt.chebFilt(y, 1/500, 10, btype='highpass')\n",
    "y_trl = y.reshape(-1, trlLen)\n",
    "if len(y_trl)<25:\n",
    "    y_trl = util.CombineItems(N=25).fit(y_trl).transform(y_trl)\n",
    "    \n",
    "   \n",
    "    \n",
    "ax[1].plot(t, y_trl.T, alpha=alpha)\n",
    "ax[1].set_title('$Ablated$', fontsize=16)\n",
    "ax[1].set_xlim(xl)\n",
    "ax[1].set_ylim(yl)\n",
    "ax[1].set_ylabel('$Bend \\ amp (^o) $', fontsize=16)\n",
    "\n",
    "env = spt.emd.envelopesAndImf(y)['env']['diff']\n",
    "env = env.reshape(-1, trlLen)\n",
    "env = env-env[:, :40].mean(axis=1)[:, None]\n",
    "mu = env.mean(axis=0)\n",
    "sem = env.std(axis=0)/env.shape[0]**0.5\n",
    "ax[2].fill_between(t, mu-sem, mu+sem, color=plt.cm.tab10(1), alpha=0.5, label='Control')\n",
    "ax[2].set_xlabel('$Time\\ (ms)$', fontsize=16)\n",
    "ax[2].set_ylim(-20, yl[1]*0.55)\n",
    "ax[2].legend(fontsize=16)\n",
    "ax[2].set_title('$Envelopes (\\mu \\pm \\sigma/\\sqrt{n})$', fontsize=16)\n",
    "\n",
    "fh.suptitle('Ablation Group: Ventral RS', fontsize=20)\n",
    "\n",
    "# for ext in ['pdf', 'png']:\n",
    "#     fh.savefig(os.path.join(dir_save, f'Fig-{util.timestamp(\"day\")}_ventral RS_ctrl-vs-abl' + f'.{ext}'), dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the trained model\n",
    "dir_group = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging\\Group'\n",
    "# file_model = 'gmm_svd-3_env_pca-9_gmm-20_20200129-18.pkl'\n",
    "# path_gmm_model = os.path.join(dir_group, file_model)\n",
    "path_gmm_model = glob.glob(os.path.join(dir_group, 'gmm_svd-3_env_pca-9_gmm-20*.pkl'))[-1]\n",
    "gmm_model = joblib.load(path_gmm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iTrl = 10\n",
    "y = ta_full[inds_abl[iFish[0]]]\n",
    "y_trl = np.array(np.hsplit(y, y.shape[1]/trlLen))\n",
    "lbls = gmm_model.predict(y_trl[iTrl])[0]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(t, y_trl[iTrl][-1])\n",
    "for lbl in np.unique(lbls):\n",
    "    inds = np.where(lbls==lbl)[0]\n",
    "    plt.scatter(t[inds][::4], y_trl[iTrl][-1][inds][::4], c=np.array(plt.cm.tab20(lbl))[None, :], marker=f'${str(lbl)}$', s=200)\n",
    "plt.xlim(-20, 700)\n",
    "y_trl.shape\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.pcolor(t, range(y_trl.shape[1]), np.flipud(y_trl[iTrl]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0, 10, 20)\n",
    "_, inds = np.unique(a, return_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ta_trl[6][-1]\n",
    "def envelopes_allPks(x, triplicate: bool = False, interp_kind: str = 'slinear'):\n",
    "    from apCode.SignalProcessingTools import timeseries as ts\n",
    "    from apCode.SignalProcessingTools import findPeaks\n",
    "    from scipy.interpolate import interp1d\n",
    "    if triplicate:\n",
    "        x = ts.triplicate(x)\n",
    "        indVec = np.arange(len(x))\n",
    "    pks = findPeaks(x, pol=0)[0]\n",
    "    xp = np.hstack((0, pks.ravel(), len(x)))\n",
    "    _, inds = np.unique(xp, return_index=True)\n",
    "    xp = xp[inds]\n",
    "    fp = np.hstack((x[0], x[pks], x[-1]))\n",
    "    fp = np.abs(fp[inds])\n",
    "    xx = np.arange(0, len(x))\n",
    "    f = interp1d(xp, fp, kind=interp_kind)\n",
    "    env = f(xx)\n",
    "    if triplicate:\n",
    "        env = ts.middleThird(env)\n",
    "    return env\n",
    "env = spt.emd.envelopesAndImf(y, triplicate=True)['env']   \n",
    "env_diff = env['diff']\n",
    "env_max = env['max']\n",
    "env_abs = envelopes_allPks(y, triplicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(y, label='Original signal')\n",
    "plt.plot(env_diff, label=f\"Envelopes: Avi's method\")\n",
    "# plt.plot(env_max)\n",
    "plt.plot(env_abs, label=\"Envelopes: Minoru's method\")\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlim(0, 350)\n",
    "plt.savefig(os.path.join(dir_save, f'Fig-{util.timestamp(\"second\")}_Comparison of envelopes.png'), dpi='figure')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Reload dataframe to continue from here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = glob.glob(os.path.join(dir_xls, 'dataFrame_rsNeurons_ablations_clean_*.pkl'))[-1]\n",
    "dataFrame_orig = pd.read_pickle(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Load trained GMM model and predict on tail angles from dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the trained model\n",
    "dir_group = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging\\Group'\n",
    "# file_model = 'gmm_svd-3_env_pca-9_gmm-20_20200129-18.pkl'\n",
    "# path_gmm_model = os.path.join(dir_group, file_model)\n",
    "path_gmm_model = glob.glob(os.path.join(dir_group, 'gmm_svd-3_env_pca-9_gmm-20*.pkl'))[-1]\n",
    "gmm_model = joblib.load(path_gmm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Predict labels using GMM\n",
    "Fs=500\n",
    "\n",
    "ta_trl = [np.array(_) for _ in df.tailAngles]\n",
    "ta = np.concatenate(ta_trl, axis=1)\n",
    "ta_tot = spt.chebFilt(ta[-1], 1/Fs, (5, 60), btype='bandpass')\n",
    "fishIdx_ser = [np.ones((ta_.shape[1],))*fi for fi, ta_ in zip(df.FishIdx, df.tailAngles)]\n",
    "fishIdx_ser = np.concatenate(fishIdx_ser, axis=0).astype(int)\n",
    "%time labels, features = gmm_model.predict(ta)\n",
    "%time envelopes = spt.emd.envelopesAndImf(ta_tot)['env']\n",
    "env_max, env_diff = envelopes['max'], envelopes['diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot to check out what the labels look like\n",
    "xl = (10e3, 15e3)\n",
    "\n",
    "inds = np.arange(*xl, dtype='int')\n",
    "plt.figure(figsize=(20, 5))\n",
    "# plt.plot(ta_clean[-1])\n",
    "x = np.arange(len(inds))\n",
    "# plt.plot(x, ta_tot[inds])\n",
    "plt.plot(x, env_diff[inds])\n",
    "clrs = plt.cm.tab20(labels[inds])\n",
    "for x_ in x[::10]:\n",
    "    plt.scatter(x_, env_diff[inds][x_], c=clrs[x_][None, :], marker=r'${}$'.format(labels[inds][x_]), s=200)\n",
    "plt.xlim(0, len(inds))\n",
    "# plt.xlim(2000, 3e3)\n",
    "plt.ylim(-10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Detection of swim onsets and offsets followed by modificaton of dataframe to include episodes of tail angles_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tPre=25e-3\n",
    "tPost = 100e-3\n",
    "thr_bend=8\n",
    "Fs = 500\n",
    "\n",
    "print('Computing swim on- and offsets...')\n",
    "ons, offs, _ = fsb.swimOnAndOffsets(ta_tot)\n",
    "ons = ons-int(tPre*Fs)\n",
    "offs = offs+int(tPost*Fs)\n",
    "\n",
    "if len(ons)!=len(offs):\n",
    "    raise Exception(\"Onsets and offsets don't match\")\n",
    "\n",
    "# Remove episodes too close to beginning or end\n",
    "inds_del = np.where(ons<0)[0]\n",
    "ons = np.delete(ons, inds_del, axis=0)\n",
    "offs = np.delete(offs, inds_del, axis=0)\n",
    "inds_el = np.where(offs>(len(ta_tot)-1))[0]\n",
    "ons = np.delete(ons, inds_del, axis=0)\n",
    "offs = np.delete(offs, inds_del, axis=0)\n",
    "\n",
    "# Episodic tail angles to be incorporated into the data\n",
    "ta_tot_ep, ta_ep, env_diff_ep, env_max_ep = [], [], [], []\n",
    "bendNum, bendAmp, bendInt =[], [], []\n",
    "fishIdx_ep = []\n",
    "for on, off in zip(ons, offs):\n",
    "    y=ta_tot[on:off]\n",
    "    pks = spt.findPeaks(y, pol=0, thr=thr_bend, thrType='rel')[0]\n",
    "    if len(pks)>1:\n",
    "        nBends = len(pks)\n",
    "        bendNum.append(np.arange(nBends))\n",
    "        bendAmp.append(y[pks])\n",
    "        bendInt.append(np.round(np.gradient(pks)*(1000/Fs)))\n",
    "    else:\n",
    "        bendNum.append([np.nan])\n",
    "        bendAmp.append([np.nan])\n",
    "        bendInt.append([np.nan])\n",
    "    ta_tot_ep.append(y)\n",
    "    ta_ep.append(ta[:,on:off])\n",
    "    env_diff_ep.append(env_diff[on:off])\n",
    "    env_max_ep.append(env_max[on:off])\n",
    "    fishIdx_ep.append(int(fishIdx_ser[on]))\n",
    "dict_list = dict(tailAngles=ta_ep, tailAngles_tot=ta_tot_ep, tailAngles_env_max=env_max_ep,\n",
    "                 tailAngles_env_diff=env_diff_ep, FishIdx=fishIdx_ep,\n",
    "                 episodeNum_glob=list(range(len(ons))), bendNum=bendNum,\n",
    "                 bendAmp=bendAmp, bendInt=bendInt)\n",
    "df_now = pd.DataFrame(dict_list)\n",
    "\n",
    "dataFrame_drop = dataFrame_orig.drop(columns=['tailAngles', 'tailAngles_tot'])\n",
    "dataFrame = pd.merge(dataFrame_drop, df_now, on='FishIdx')\n",
    "print(dataFrame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Expand dataframe on bends*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict(episodeNum_glob=[], bendNum=[], bendAmp=[], bendInt=[], nBends=[])\n",
    "for iEp in range(dataFrame.shape[0]):\n",
    "    ep = dataFrame.iloc[iEp]\n",
    "    bendNum = np.array(ep.bendNum)\n",
    "    epNum = ep.episodeNum_glob*np.ones_like(bendNum)\n",
    "    nBends = len(bendNum)*np.ones_like(bendNum)\n",
    "    dic['episodeNum_glob'].extend(epNum)\n",
    "    dic['bendNum'].extend(bendNum)\n",
    "    dic['bendAmp'].extend(np.array(ep.bendAmp))\n",
    "    dic['bendInt'].extend(np.array(ep.bendInt))  \n",
    "    dic['nBends'].extend(nBends)\n",
    "df_now = pd.DataFrame(dic)\n",
    "df_old = dataFrame.drop(columns=['bendNum', 'bendAmp', 'bendInt'])\n",
    "dataFrame = pd.merge(df_old, df_now, on='episodeNum_glob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Save dataframe for later use_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'dataFrame_rsNeurons_ablations_clean_bends_{util.timestamp()}.pkl'\n",
    "%time dataFrame.to_pickle(os.path.join(dir_xls, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Flippity do\n",
    "grps = np.array(dataFrame.AblationGroup)\n",
    "trts = np.array(dataFrame.Treatment)\n",
    "inds_ctrl = np.where((grps=='ventralRS') & (trts=='ctrl'))[0]\n",
    "inds_abl = np.where((grps=='ventralRS') & (trts=='abl'))[0]\n",
    "trts[inds_ctrl] = 'abl'\n",
    "trts[inds_abl] ='ctrl'\n",
    "dataFrame_flp = dataFrame.copy()\n",
    "dataFrame_flp = dataFrame_flp.assign(Treatment=trts)\n",
    "\n",
    "fname = f'dataFrame_rsNeurons_ablations_clean_bends_venFlp_{util.timestamp()}.pkl'\n",
    "%time dataFrame_flp.to_pickle(os.path.join(dir_xls, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Reload dataframe to continue from here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = glob.glob(os.path.join(dir_xls, 'dataFrame_rsNeurons_ablations_clean_bends_venFlp*.pkl'))[-1]\n",
    "dataFrame = pd.read_pickle(fname)\n",
    "dataFrame_orig = dataFrame.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _First, just look at number of bends per episode for each ablation group, comparing ctrl vs abl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBends_max = 40\n",
    "nBends_min = 5\n",
    "df_sub = dataFrame.loc[(dataFrame.bendNum<=nBends_max) & (dataFrame.nBends<=nBends_max)\n",
    "                       & (dataFrame.nBends>=nBends_min)]\n",
    "sns.catplot(data=df_sub, x='AblationGroup', y='nBends', hue='Treatment', kind='boxen',\n",
    "            aspect=2, order= ['mHom', 'intermediateRS', 'ventralRS'], hue_order=['ctrl', 'abl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBends_max = 35\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "ablGrps = ['mHom', 'intermediateRS', 'ventralRS']\n",
    "nGrps = len(ablGrps)\n",
    "bendNums = np.arange(nBends_max)\n",
    "for iGrp, ablGrp in enumerate(ablGrps):\n",
    "    grp = df_sub.loc[df_sub.AblationGroup==ablGrp]\n",
    "    trts = ['ctrl', 'abl']\n",
    "    p_ctrl =[]\n",
    "    p_abl = []\n",
    "    for iTrt, trt in enumerate(trts):\n",
    "        foo = grp.loc[grp.Treatment==trt]\n",
    "        nEps = len(foo.loc[foo.bendNum==0])\n",
    "        for bn in bendNums:\n",
    "            n = len(foo.loc[foo.bendNum==bn])\n",
    "            p = n/nEps\n",
    "            if trt == 'ctrl':\n",
    "                p_ctrl.append(p)\n",
    "            else:\n",
    "                p_abl.append(p)\n",
    "    p_ctrl, p_abl = np.array(p_ctrl), np.array(p_abl)     \n",
    "    plt.subplot(nGrps, 1, iGrp+1)\n",
    "    plt.plot(bendNums+1, p_ctrl,'o-', label='Control', alpha=0.5)\n",
    "    plt.plot(bendNums+1, p_abl,'o-', label = 'Ablated', alpha=0.5)\n",
    "    plt.xlim(0, bendNums.max())\n",
    "    plt.title(ablGrp, fontsize=14)\n",
    "    if iGrp==0:\n",
    "        plt.legend()\n",
    "    plt.ylabel('$P(bend)$')\n",
    "plt.xlabel('Bend #')\n",
    "plt.subplots_adjust(hspace=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create a dataframe that includes swimming information (fish position, swim distance, etc)\n",
    "%time df = rsp.append_fishPos_to_xls(xls)\n",
    "\n",
    "#%% Compute onset latencies\n",
    "%time df = rsp.detect_noisy_trials(df)\n",
    "\n",
    "df_den = df.loc[df.noisyTrlInds_swimVel==0]\n",
    "df = rsp.append_latency_to_df(df_den, zThr=0.5)\n",
    "inds_del = np.where(np.isnan(df.onsetLatency))[0]\n",
    "df = df.set_index(np.arange(df.shape[0]))\n",
    "df = df.drop(index=inds_del)\n",
    "df = df.assign(onsetLatency_log=np.log(df.onsetLatency))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust swim distances and velocities to make consistent across ablation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dist = 1.25\n",
    "sf_vel = 1.4\n",
    "\n",
    "inds = np.where(df.AblationGroup != 'mHom')[0]\n",
    "swimDist_total = np.array(df.swimDist_total)\n",
    "swimDist_total[inds] = swimDist_total[inds]*sf_dist\n",
    "\n",
    "swimVel_max = np.array(df.swimVel_max)\n",
    "swimVel_max[inds] = swimVel_max[inds]*sf_vel\n",
    "\n",
    "\n",
    "df = df.assign(swimDist_total_adj=swimDist_total,\\\n",
    "               swimDist_total_adj_log=np.log2(swimDist_total),\n",
    "               swimVel_max_adj=swimVel_max,\\\n",
    "               swimVel_max_adj_log=np.log2(swimVel_max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Total swim distance\n",
    "saveDir = f'Y:\\Avinash\\Projects\\RS recruitment\\Figures\\{util.timestamp(\"day\")}'\n",
    "#%% Filter extreme values\n",
    "q_max = 99\n",
    "q_min = 5\n",
    "\n",
    "if not os.path.exists(saveDir):\n",
    "    os.mkdir(saveDir)\n",
    "thr_low = np.percentile(df.swimDist_total, q_min)\n",
    "thr_high = np.percentile(df.swimDist_total, q_max)\n",
    "\n",
    "df_sub = df.loc[(df.swimDist_total > thr_low) & (df.swimDist_total < thr_high)]\n",
    "\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Total swim distance_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimDist_total_adj',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment', dodge = True,\n",
    "                 order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Total swim distance_log_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimDist_total_adj_log',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment', dodge = True,\n",
    "                 order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "thr_low = np.percentile(df.swimVel_max, q_min)\n",
    "thr_high = np.percentile(df.swimVel_max, q_max)\n",
    "\n",
    "df_sub = df.loc[(df.swimVel_max > thr_low) & (df.swimVel_max < thr_high)]\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Max swim vel_ctrl-vs-abl_mHom-interRS-ventralRS_boxen'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimVel_max_adj',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment',\\\n",
    "                 dodge = True, order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Max swim vel_log_ctrl-vs-abl_mHom-interRS-ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimVel_max_adj_log',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment',\\\n",
    "                 dodge = True, order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Plot latencies\n",
    "minLat = 5\n",
    "maxLat = 30\n",
    "\n",
    "    \n",
    "fn = f'Fig-{util.timestamp()}_Onset latencies_log_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "df_now = df.loc[(df.onsetLatency >= minLat) & (df.onsetLatency <= maxLat)]\n",
    "\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='boxen', dodge=True, aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_boxen.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_boxen.png'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='swarm', dodge=True, aspect=2, alpha=0.5)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_swarm.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_swarm.png'))\n",
    "\n",
    "plt.show()\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='box', dodge=True, aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_box.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_box.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV/pickle file file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "saveDir = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = f'Dataframe with swim trajectories and global swim parameters_{util.timestamp()}'\n",
    "%time df.to_csv(os.path.join(saveDir,fn + '.csv'), columns=df.keys(), index = False)\n",
    "%time df.to_pickle(os.path.join(saveDir, fn + '.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_csv = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = ft.findAndSortFilesInDir(dir_csv, search_str='Dataframe with swim trajectories', ext = '.pkl')[-1]\n",
    "df = pd.read_pickle(os.path.join(dir_csv, fn))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume after loading the saved csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = ft.findAndSortFilesInDir(saveDir, ext = 'csv', search_str= 'mHomologs_intermediateRS_ventralRS')\n",
    "if len(fn)>0:\n",
    "    fn = fn[-1]\n",
    "df = pd.read_csv(os.path.join(saveDir,fn))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Bend amplitude vs bend number\n",
    "\n",
    "figDir = f'Y:\\Avinash\\Projects\\RS recruitment\\Figures\\{util.timestamp()}'\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "nBends = 10\n",
    "\n",
    "# plt.style.use(('seaborn-paper', 'seaborn-whitegrid'))\n",
    "amps = np.array(df.bendAmp_rel)\n",
    "amps = np.delete(amps, np.where(np.isnan(amps)))\n",
    "amp_low = np.percentile(amps,5)\n",
    "amp_high = np.percentile(amps,99)\n",
    "\n",
    "pers = np.array(df.bendInt)\n",
    "pers = np.delete(pers, np.where(np.isnan(pers)))\n",
    "per_low = np.percentile(pers,5)\n",
    "per_high = np.percentile(pers,95)\n",
    "\n",
    "df_now = df.loc[(df.bendAmp_rel>= amp_low) & (df.bendAmp_rel <= amp_high) & (df.bendInt >= per_low) & \\\n",
    "               (df.bendInt <= per_high) & (df.bendNum>=0) & (df.bendNum <=nBends)]\n",
    "\n",
    "# df_now = df.loc[(df.bendAmp_rel>= 40) & (df.bendAmp_rel <= amp_high) & \\\n",
    "#                 (df.bendInt <= 60) & (df.bendNum>=0) & (df.bendNum <=10)]\n",
    "\n",
    "plt.style.use(('seaborn-ticks', 'seaborn-paper'))\n",
    "fh = sns.catplot(data = df_now, x = 'bendNum', y = 'bendAmp_rel', hue = 'treatment', col = 'ablationGroup',\\\n",
    "            col_order=['mHom', 'intermediateRS', 'ventralRS'], aspect = 1.5, dodge = True,\\\n",
    "            sharex=True, sharey=True, kind = 'point', palette=plt.cm.tab10(np.arange(10)),\\\n",
    "           hue_order = ['ctrl','abl'], height = 3, ci  =99, scale = 0.5)\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Bend amplitude vs bend number for ctrl and abl_mHom_intermediate_ventral'\n",
    "fh.savefig(os.path.join(figDir, figName + '.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(figDir, figName + '.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "print(f'Saved at {figDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Bend interval\n",
    "nBends = 10\n",
    "\n",
    "df_now = df.loc[(df.bendAmp_rel>= amp_low) & (df.bendAmp_rel <= amp_high) & \\\n",
    "                (df.bendInt <= per_high) & (df.bendNum>=0) & (df.bendNum <=nBends)]\n",
    "\n",
    "\n",
    "# plt.style.use(('seaborn-white', 'seaborn-paper'))\n",
    "fh = sns.catplot(data = df_now, x = 'bendNum', y = 'bendInt', hue = 'treatment', col = 'ablationGroup',\\\n",
    "            col_order=['mHom', 'intermediateRS', 'ventralRS'], aspect = 1.5, dodge = True,\\\n",
    "            sharex=True, sharey=True, kind = 'point', palette=plt.cm.tab10(np.arange(10)),\\\n",
    "           hue_order = ['ctrl','abl'], height = 3, ci  =99, scale = 0.5)\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Bend interval vs bend number for ctrl and abl_mHom_intermediate_ventral'\n",
    "fh.savefig(os.path.join(figDir, figName + '.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(figDir, figName + '.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "print(f'Saved at {figDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
