{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Change log_\n",
    "1. Using a U-net trained on a much larger dataset of free swimming images. Assessment revealed great performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tifffile as tff\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "codeDirs = [r'V:/code/python/code', r'V:\\Code\\Python\\code']\n",
    "[sys.path.append(_) for _ in codeDirs]\n",
    "\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "from apCode import util as util\n",
    "import rsNeuronsProj.util as rsp\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducability\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load dataframe containing paths to data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_df = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\session_20200424-20'\n",
    "path_df = glob.glob(os.path.join(dir_df, 'Ablation data summary*.pkl'))[-1]\n",
    "\n",
    "df = pd.read_pickle(path_df)\n",
    "dir_save = os.path.join(dir_df, f'session_{util.timestamp()}')\n",
    "os.makedirs(dir_save, exist_ok=True)\n",
    "df_orig = df.copy()\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load pretrained U-net (loss_func = focal_loss; optimizer='Adam')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_unet = r'Y:\\Avinash\\Ablations and Behavior'\n",
    "file_unet = ft.findAndSortFilesInDir(dir_unet, ext='h5', search_str='trainedU_fsb_896x896_2020')[-1]\n",
    "print(f'Loading U-net: {file_unet}')\n",
    "path_unet = os.path.join(dir_unet, file_unet)\n",
    "\n",
    "unet = mlearn.loadPreTrainedUnet(path_unet)\n",
    "print(f'U net dims = {unet.input_shape}')\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Expand dataframe by including tail angles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Create another dataframe with tail angles info and merge with original dataframe using common FishIdx column\n",
    "trlLen = 750\n",
    "\n",
    "df = df_orig.copy() # Make a copy and work with that\n",
    "\n",
    "paths_retrack = []\n",
    "dict_list = dict(FishIdx = [], tailAngles = [], tailAngles_tot = [],\n",
    "                 inds_tailAngles=[], totalNumPts=[], perc_tailAngles_tracked=[])\n",
    "for iPath in range(len(df)):\n",
    "    df_ = df.iloc[iPath]\n",
    "    path_ = df_.Path_proc\n",
    "    fn = ft.findAndSortFilesInDir(path_, search_str='procData', ext='h5')\n",
    "    if len(fn)>0:\n",
    "        fn = fn[-1]\n",
    "        path_hFile = os.path.join(path_, fn)\n",
    "        with h5py.File(path_hFile, mode='r') as hFile:\n",
    "            try:\n",
    "                keys = hFile.keys()\n",
    "                key = 'tailAngles'\n",
    "                if key in keys:\n",
    "                    ta = np.array(hFile[key]).transpose()                  \n",
    "                    dict_list['FishIdx'].append(df_.FishIdx)\n",
    "                    dict_list['tailAngles'].append(ta)\n",
    "                    dict_list['tailAngles_tot'].append(ta[-1])\n",
    "                    inds_ta = np.array(hFile['frameInds_processed'])\n",
    "                    dict_list['inds_tailAngles'].append(inds_ta)\n",
    "                    nTot = hFile['imgs_prob'].shape[0]\n",
    "                    dict_list['totalNumPts'].append(nTot)\n",
    "                    perc_tracked = round(100*len(inds_ta)/nTot)\n",
    "                    dict_list['perc_tailAngles_tracked'].append(perc_tracked)\n",
    "                else:\n",
    "                    print(f'No tailAngles in path # {iPath}\\n {path_hFile}')\n",
    "                    paths_retrack.append(path_)\n",
    "            except Exception:\n",
    "                print(f'Cannot read path # {iPath},  hdf file\\n {path_hFile}')\n",
    "                paths_retrack.append(path_)\n",
    "    else:\n",
    "        print(f'No hdf file found for path # {iPath}\\n {path_}')\n",
    "        paths_retrack.append(path_)\n",
    "df_now = pd.DataFrame(dict_list)\n",
    "df = pd.merge(df, df_now, on='FishIdx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Filter dataframe based on extent to which tracking worked, then interpolate tail angles for missing points*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thr_track = 85\n",
    "trlLen=750\n",
    "\n",
    "df = df.loc[df.perc_tailAngles_tracked>=thr_track]\n",
    "ta_full=[]\n",
    "for iFish in range(df.shape[0]):    \n",
    "    df_ = df.iloc[iFish]\n",
    "    ta_ = df_.tailAngles\n",
    "    inds_ta = df_.inds_tailAngles\n",
    "    nTot = np.maximum(df_.totalNumPts, inds_ta.max()+1)\n",
    "    ta_nan = np.zeros((ta_.shape[0], nTot), dtype='float')*np.nan\n",
    "    ta_nan[:, inds_ta] = ta_\n",
    "    taf = dask.delayed(spt.interp.nanInterp2d)(ta_nan, method='nearest')\n",
    "    ta_full.append(taf)\n",
    "with ProgressBar():\n",
    "    ta_full = dask.compute(*ta_full, scheduler='processes')\n",
    "ta_full_clip, ta_tot = [], []\n",
    "for ta_ in ta_full:\n",
    "    nTrls = ta_.shape[1]//trlLen\n",
    "    n = nTrls*trlLen\n",
    "    ta_ = ta_[:, :n]\n",
    "    ta_full_clip.append(ta_)\n",
    "    ta_tot.append(ta_[-1])\n",
    "ta_full = ta_full_clip\n",
    "df = df.assign(tailAngles=ta_full_clip, tailAngles_tot=ta_tot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Clean up tail angles in the dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_behav = 1/500\n",
    "nWaves=3\n",
    "\n",
    "nFish = len(ta_full)\n",
    "ta_full_ser = np.concatenate(ta_full, axis=1)\n",
    "%time ta_ser_clean, _, svd = hf.cleanTailAngles(ta_full_ser, dt=dt_behav, nWaves=nWaves)\n",
    "\n",
    "tLens = np.cumsum(np.array([ta_.shape[1] for ta_ in ta_full]))\n",
    "ta_full_clean = np.hsplit(ta_ser_clean, tLens)\n",
    "ta_full_clean.pop()\n",
    "\n",
    "ta_tot = []\n",
    "for ta_ in ta_full_clean:\n",
    "    ta_tot.append(ta_[-1])\n",
    "df = df.assign(tailAngles=ta_full_clean, tailAngles_tot=ta_tot)\n",
    "ta_full = ta_full_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Save the dataframe for future use_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'dataFrame_rsNeurons_ablations_svdClean_{util.timestamp()}.pkl'\n",
    "%time df.to_pickle(os.path.join(dir_save, fname))\n",
    "print(f'Saved at\\n{dir_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot trials to check quality*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iFish = (0, 0)\n",
    "xl = (-20, 320) # ms\n",
    "yl = (-200, 200)\n",
    "alpha = 0.3\n",
    "\n",
    "inds_ctrl = np.where((df.AblationGroup=='ventralRS') & (df.Treatment=='ctrl'))[0]\n",
    "inds_abl = np.where((df.AblationGroup=='ventralRS') & (df.Treatment=='abl'))[0]\n",
    "\n",
    "fh, ax = plt.subplots(3, 1, figsize=(20, 12), sharex=True, sharey=False)\n",
    "\n",
    "y = ta_tot[inds_ctrl[iFish[0]]]*1.3\n",
    "y = spt.chebFilt(y, 1/500, 5, btype='highpass')\n",
    "y_trl = y.reshape(-1, trlLen)\n",
    "t = (np.arange(y_trl.shape[1])-50)*(1/500)*1000\n",
    "if len(y_trl)<25:\n",
    "    y_trl = util.CombineItems(N=25).fit(y_trl).transform(y_trl)\n",
    "\n",
    "ax[0].plot(t, y_trl.T, alpha=alpha)\n",
    "ax[0].set_title('$Control$', fontsize=16)\n",
    "ax[1].set_ylim(yl)\n",
    "\n",
    "env = spt.emd.envelopesAndImf(y)['env']['diff']\n",
    "env = env.reshape(-1, trlLen)\n",
    "env = env-env[:, :40].mean(axis=1)[:, None]\n",
    "mu = env.mean(axis=0)\n",
    "sem = env.std(axis=0)/env.shape[0]**0.5\n",
    "ax[2].fill_between(t, mu-sem, mu+sem, color=plt.cm.tab10(0), alpha=0.5, label='Control')\n",
    "ax[2].set_ylim(yl)\n",
    "\n",
    "y = ta_tot[inds_abl[iFish[1]]]\n",
    "y = spt.chebFilt(y, 1/500, 10, btype='highpass')\n",
    "y_trl = y.reshape(-1, trlLen)\n",
    "if len(y_trl)<25:\n",
    "    y_trl = util.CombineItems(N=25).fit(y_trl).transform(y_trl)\n",
    "    \n",
    "ax[1].plot(t, y_trl.T, alpha=alpha)\n",
    "ax[1].set_title('$Ablated$', fontsize=16)\n",
    "ax[1].set_xlim(xl)\n",
    "# ax[1].set_ylim(yl)\n",
    "ax[1].set_ylabel('$Bend \\ amp (^o) $', fontsize=16)\n",
    "\n",
    "env = spt.emd.envelopesAndImf(y)['env']['diff']\n",
    "env = env.reshape(-1, trlLen)\n",
    "env = env-env[:, :40].mean(axis=1)[:, None]\n",
    "mu = env.mean(axis=0)\n",
    "sem = env.std(axis=0)/env.shape[0]**0.5\n",
    "ax[2].fill_between(t, mu-sem, mu+sem, color=plt.cm.tab10(1), alpha=0.5, label='Control')\n",
    "ax[2].set_xlabel('$Time\\ (ms)$', fontsize=16)\n",
    "ax[2].set_ylim(-20, yl[1]*0.75)\n",
    "ax[2].legend(fontsize=16)\n",
    "ax[2].set_title('$Envelopes (\\mu \\pm \\sigma/\\sqrt{n})$', fontsize=16)\n",
    "\n",
    "fh.suptitle('Ablation Group: Ventral RS', fontsize=20)\n",
    "\n",
    "for ext in ['pdf', 'png']:\n",
    "    fn = f'Fig-{util.timestamp(\"day\")}_ventral RS_ctrl-vs-abl_fish_{iFish[0]}-{iFish[1]}' + f'.{ext}'\n",
    "#     fh.savefig(os.path.join(dir_save, fn), dpi='figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load GMM and train some more, if need be*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the trained model\n",
    "dir_group = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging\\Group\\session_20200424-03'\n",
    "file_model = 'gmm_for_headFixed_[20]_svd_[3]_env_pca_[8]_20200424-03.pkl'\n",
    "# path_gmm_model = os.path.join(dir_group, file_model)\n",
    "path_gmm_model = os.path.join(dir_group, file_model)\n",
    "gmm_model = joblib.load(path_gmm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Train the model on the entire dataset if not already trained*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_fish = [np.array(_) for _ in df.tailAngles]\n",
    "ta_ser = np.concatenate(ta_fish, axis=1)\n",
    "%time gmm_model = gmm_model.fit(ta_ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def envelopes_allPks(x, triplicate: bool = False, interp_kind: str = 'slinear'):\n",
    "    from apCode.SignalProcessingTools import timeseries as ts\n",
    "    from apCode.SignalProcessingTools import findPeaks\n",
    "    from scipy.interpolate import interp1d\n",
    "    if triplicate:\n",
    "        x = ts.triplicate(x)\n",
    "        indVec = np.arange(len(x))\n",
    "    pks = findPeaks(x, pol=0)[0]\n",
    "    xp = np.hstack((0, pks.ravel(), len(x)))\n",
    "    _, inds = np.unique(xp, return_index=True)\n",
    "    xp = xp[inds]\n",
    "    fp = np.hstack((x[0], x[pks], x[-1]))\n",
    "    fp = np.abs(fp[inds])\n",
    "    xx = np.arange(0, len(x))\n",
    "    f = interp1d(xp, fp, kind=interp_kind)\n",
    "    env = f(xx)\n",
    "    if triplicate:\n",
    "        env = ts.middleThird(env)\n",
    "    return env\n",
    "# env = spt.emd.envelopesAndImf(y, triplicate=True)['env']   \n",
    "# env_diff = env['diff']\n",
    "# env_max = env['max']\n",
    "# env_abs = envelopes_allPks(y, triplicate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Detection of swim onsets and offsets followed by modificaton of dataframe to include episodes of tail angles_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trlLen = 750\n",
    "Fs = 500\n",
    "thr_bend = 10\n",
    "nPreStim = 50\n",
    "\n",
    "dic = dict(FishIdx = [], trlIdx=[], bendIdx=[], bendAmp=[], bendAmp_rel=[],\n",
    "           bendInt_ms=[], sampleIdxInTrl=[], bendSampleIdxInTrl=[], tailAngles=[])\n",
    "weirds = []\n",
    "for iFish in range(df.shape[0]):\n",
    "    df_fish = df.iloc[iFish]\n",
    "#     print(f'FishIdx = {df_fish.FishIdx}, {iFish+1}/{len(df)}')\n",
    "    ta_fish = np.array(df_fish.tailAngles)\n",
    "    nTrls = ta_fish.shape[1]//trlLen\n",
    "    trlIdx = np.repeat(np.arange(nTrls), trlLen)\n",
    "    sampleIdxInTrl = np.repeat(np.arange(trlLen), nTrls)\n",
    "    fishIdx = np.repeat(df_fish.FishIdx, len(sampleIdxInTrl))\n",
    "    dic['FishIdx'].extend(fishIdx)\n",
    "    dic['sampleIdxInTrl'].extend(sampleIdxInTrl)\n",
    "    dic['trlIdx'].extend(trlIdx)\n",
    "    dic['tailAngles'].extend(ta_fish.T)\n",
    "    nanVec = np.zeros((nTrls*trlLen, ), dtype='float')*np.nan\n",
    "    bendIdx, bendAmp, bendAmp_rel, bendInt, bendSampleIdxInTrl = [nanVec.copy() for i in range(5)]\n",
    "    for trl in range(nTrls):\n",
    "        trlInds = np.arange(trl*trlLen, (trl+1)*trlLen)       \n",
    "        ta_trl = ta_fish[:, trlInds][-1]\n",
    "        pks = spt.findPeaks(ta_trl, thr=thr_bend, thrType='rel', pol=0)[0]\n",
    "        pks = pks[np.where(pks>nPreStim)]\n",
    "        if len(pks)>=3:          \n",
    "            inds_ = trlInds[pks]\n",
    "            bendSampleIdxInTrl[inds_] = pks.copy()\n",
    "            bendIdx[inds_] = np.arange(len(pks))\n",
    "            amps = ta_trl[pks]\n",
    "            amps_rel = np.insert(np.diff(amps), 0, amps[0])\n",
    "            bendAmp[inds_] = amps\n",
    "            bendAmp_rel[inds_] = np.abs(amps_rel)\n",
    "            bendInt[inds_] = np.gradient(pks)*(1/Fs)*1000\n",
    "        else:\n",
    "            pass\n",
    "#             print(f'No peaks for fish# {df_fish.FishIdx}, trl# {trl}')\n",
    "    dic['bendIdx'].extend(bendIdx)\n",
    "    dic['bendAmp'].extend(bendAmp)\n",
    "    dic['bendAmp_rel'].extend(bendAmp_rel)\n",
    "    dic['bendInt_ms'].extend(bendInt)\n",
    "    dic['bendSampleIdxInTrl'].extend(bendSampleIdxInTrl)\n",
    "df_flat = pd.DataFrame(dic)\n",
    "foo = df.drop(columns=['tailAngles', 'tailAngles_tot', 'inds_tailAngles'])\n",
    "df_flat = pd.merge(foo, df_flat, on='FishIdx')        \n",
    "        \n",
    "print(df_flat.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Save the flattened dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'dataFrame_rsNeurons_ablations_svdClean_flat_{util.timestamp()}.pkl'\n",
    "%time df_flat.to_pickle(os.path.join(dir_save, fname))\n",
    "print(f'Saved at\\n{dir_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_df = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\session_20200424-20\\session_20200424-20'\n",
    "path_df = glob.glob(os.path.join(dir_df ,  'dataFrame_rsNeurons_ablations_svdClean_flat*.pkl'))[-1]\n",
    "df = pd.read_pickle(path_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Try to identify bad trials and omit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "fishInds = np.unique(df.FishIdx)\n",
    "fish_trl=[]\n",
    "scores = []\n",
    "# fh, ax = plt.subplots(1,1, figsize=(20, 5))\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(100):\n",
    "    plt.clf()\n",
    "    np.random.shuffle(fishInds)\n",
    "    fi = fishInds[0]\n",
    "    df_fish = df.loc[df.FishIdx==fi]\n",
    "    trlInds = np.unique(df_fish.trlIdx)\n",
    "    np.random.shuffle(trlInds)\n",
    "    ti = trlInds[0]\n",
    "    y = np.array([np.array(_) for _ in df_fish.loc[df_fish.trlIdx==ti].tailAngles]).T[-1]\n",
    "    plt.plot(y)   \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    ans = input('Blah: ')\n",
    "    scores.append(int(ans))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tPre=25e-3\n",
    "tPost = 100e-3\n",
    "thr_bend=8\n",
    "Fs = 500\n",
    "\n",
    "print('Computing swim on- and offsets...')\n",
    "ons, offs, _ = fsb.swimOnAndOffsets(ta_tot)\n",
    "ons = ons-int(tPre*Fs)\n",
    "offs = offs+int(tPost*Fs)\n",
    "\n",
    "if len(ons)!=len(offs):\n",
    "    raise Exception(\"Onsets and offsets don't match\")\n",
    "\n",
    "# Remove episodes too close to beginning or end\n",
    "inds_del = np.where(ons<0)[0]\n",
    "ons = np.delete(ons, inds_del, axis=0)\n",
    "offs = np.delete(offs, inds_del, axis=0)\n",
    "inds_el = np.where(offs>(len(ta_tot)-1))[0]\n",
    "ons = np.delete(ons, inds_del, axis=0)\n",
    "offs = np.delete(offs, inds_del, axis=0)\n",
    "\n",
    "# Episodic tail angles to be incorporated into the data\n",
    "ta_tot_ep, ta_ep, env_diff_ep, env_max_ep = [], [], [], []\n",
    "bendNum, bendAmp, bendInt =[], [], []\n",
    "fishIdx_ep = []\n",
    "for on, off in zip(ons, offs):\n",
    "    y=ta_tot[on:off]\n",
    "    pks = spt.findPeaks(y, pol=0, thr=thr_bend, thrType='rel')[0]\n",
    "    if len(pks)>1:\n",
    "        nBends = len(pks)\n",
    "        bendNum.append(np.arange(nBends))\n",
    "        bendAmp.append(y[pks])\n",
    "        bendInt.append(np.round(np.gradient(pks)*(1000/Fs)))\n",
    "    else:\n",
    "        bendNum.append([np.nan])\n",
    "        bendAmp.append([np.nan])\n",
    "        bendInt.append([np.nan])\n",
    "    ta_tot_ep.append(y)\n",
    "    ta_ep.append(ta[:,on:off])\n",
    "    env_diff_ep.append(env_diff[on:off])\n",
    "    env_max_ep.append(env_max[on:off])\n",
    "    fishIdx_ep.append(int(fishIdx_ser[on]))\n",
    "dict_list = dict(tailAngles=ta_ep, tailAngles_tot=ta_tot_ep, tailAngles_env_max=env_max_ep,\n",
    "                 tailAngles_env_diff=env_diff_ep, FishIdx=fishIdx_ep,\n",
    "                 episodeNum_glob=list(range(len(ons))), bendNum=bendNum,\n",
    "                 bendAmp=bendAmp, bendInt=bendInt)\n",
    "df_now = pd.DataFrame(dict_list)\n",
    "\n",
    "dataFrame_drop = dataFrame_orig.drop(columns=['tailAngles', 'tailAngles_tot'])\n",
    "dataFrame = pd.merge(dataFrame_drop, df_now, on='FishIdx')\n",
    "print(dataFrame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Expand dataframe on bends*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict(episodeNum_glob=[], bendNum=[], bendAmp=[], bendInt=[], nBends=[])\n",
    "for iEp in range(dataFrame.shape[0]):\n",
    "    ep = dataFrame.iloc[iEp]\n",
    "    bendNum = np.array(ep.bendNum)\n",
    "    epNum = ep.episodeNum_glob*np.ones_like(bendNum)\n",
    "    nBends = len(bendNum)*np.ones_like(bendNum)\n",
    "    dic['episodeNum_glob'].extend(epNum)\n",
    "    dic['bendNum'].extend(bendNum)\n",
    "    dic['bendAmp'].extend(np.array(ep.bendAmp))\n",
    "    dic['bendInt'].extend(np.array(ep.bendInt))  \n",
    "    dic['nBends'].extend(nBends)\n",
    "df_now = pd.DataFrame(dic)\n",
    "df_old = dataFrame.drop(columns=['bendNum', 'bendAmp', 'bendInt'])\n",
    "dataFrame = pd.merge(df_old, df_now, on='episodeNum_glob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Save dataframe for later use_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'dataFrame_rsNeurons_ablations_clean_bends_{util.timestamp()}.pkl'\n",
    "%time dataFrame.to_pickle(os.path.join(dir_xls, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Flippity do\n",
    "grps = np.array(dataFrame.AblationGroup)\n",
    "trts = np.array(dataFrame.Treatment)\n",
    "inds_ctrl = np.where((grps=='ventralRS') & (trts=='ctrl'))[0]\n",
    "inds_abl = np.where((grps=='ventralRS') & (trts=='abl'))[0]\n",
    "trts[inds_ctrl] = 'abl'\n",
    "trts[inds_abl] ='ctrl'\n",
    "dataFrame_flp = dataFrame.copy()\n",
    "dataFrame_flp = dataFrame_flp.assign(Treatment=trts)\n",
    "\n",
    "fname = f'dataFrame_rsNeurons_ablations_clean_bends_venFlp_{util.timestamp()}.pkl'\n",
    "%time dataFrame_flp.to_pickle(os.path.join(dir_xls, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Reload dataframe to continue from here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = glob.glob(os.path.join(dir_xls, 'dataFrame_rsNeurons_ablations_clean_bends_venFlp*.pkl'))[-1]\n",
    "dataFrame = pd.read_pickle(fname)\n",
    "dataFrame_orig = dataFrame.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _First, just look at number of bends per episode for each ablation group, comparing ctrl vs abl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBends_max = 40\n",
    "nBends_min = 5\n",
    "df_sub = dataFrame.loc[(dataFrame.bendNum<=nBends_max) & (dataFrame.nBends<=nBends_max)\n",
    "                       & (dataFrame.nBends>=nBends_min)]\n",
    "sns.catplot(data=df_sub, x='AblationGroup', y='nBends', hue='Treatment', kind='boxen',\n",
    "            aspect=2, order= ['mHom', 'intermediateRS', 'ventralRS'], hue_order=['ctrl', 'abl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBends_max = 35\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "ablGrps = ['mHom', 'intermediateRS', 'ventralRS']\n",
    "nGrps = len(ablGrps)\n",
    "bendNums = np.arange(nBends_max)\n",
    "for iGrp, ablGrp in enumerate(ablGrps):\n",
    "    grp = df_sub.loc[df_sub.AblationGroup==ablGrp]\n",
    "    trts = ['ctrl', 'abl']\n",
    "    p_ctrl =[]\n",
    "    p_abl = []\n",
    "    for iTrt, trt in enumerate(trts):\n",
    "        foo = grp.loc[grp.Treatment==trt]\n",
    "        nEps = len(foo.loc[foo.bendNum==0])\n",
    "        for bn in bendNums:\n",
    "            n = len(foo.loc[foo.bendNum==bn])\n",
    "            p = n/nEps\n",
    "            if trt == 'ctrl':\n",
    "                p_ctrl.append(p)\n",
    "            else:\n",
    "                p_abl.append(p)\n",
    "    p_ctrl, p_abl = np.array(p_ctrl), np.array(p_abl)     \n",
    "    plt.subplot(nGrps, 1, iGrp+1)\n",
    "    plt.plot(bendNums+1, p_ctrl,'o-', label='Control', alpha=0.5)\n",
    "    plt.plot(bendNums+1, p_abl,'o-', label = 'Ablated', alpha=0.5)\n",
    "    plt.xlim(0, bendNums.max())\n",
    "    plt.title(ablGrp, fontsize=14)\n",
    "    if iGrp==0:\n",
    "        plt.legend()\n",
    "    plt.ylabel('$P(bend)$')\n",
    "plt.xlabel('Bend #')\n",
    "plt.subplots_adjust(hspace=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create a dataframe that includes swimming information (fish position, swim distance, etc)\n",
    "%time df = rsp.append_fishPos_to_xls(xls)\n",
    "\n",
    "#%% Compute onset latencies\n",
    "%time df = rsp.detect_noisy_trials(df)\n",
    "\n",
    "df_den = df.loc[df.noisyTrlInds_swimVel==0]\n",
    "df = rsp.append_latency_to_df(df_den, zThr=0.5)\n",
    "inds_del = np.where(np.isnan(df.onsetLatency))[0]\n",
    "df = df.set_index(np.arange(df.shape[0]))\n",
    "df = df.drop(index=inds_del)\n",
    "df = df.assign(onsetLatency_log=np.log(df.onsetLatency))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust swim distances and velocities to make consistent across ablation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dist = 1.25\n",
    "sf_vel = 1.4\n",
    "\n",
    "inds = np.where(df.AblationGroup != 'mHom')[0]\n",
    "swimDist_total = np.array(df.swimDist_total)\n",
    "swimDist_total[inds] = swimDist_total[inds]*sf_dist\n",
    "\n",
    "swimVel_max = np.array(df.swimVel_max)\n",
    "swimVel_max[inds] = swimVel_max[inds]*sf_vel\n",
    "\n",
    "\n",
    "df = df.assign(swimDist_total_adj=swimDist_total,\\\n",
    "               swimDist_total_adj_log=np.log2(swimDist_total),\n",
    "               swimVel_max_adj=swimVel_max,\\\n",
    "               swimVel_max_adj_log=np.log2(swimVel_max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Total swim distance\n",
    "saveDir = f'Y:\\Avinash\\Projects\\RS recruitment\\Figures\\{util.timestamp(\"day\")}'\n",
    "#%% Filter extreme values\n",
    "q_max = 99\n",
    "q_min = 5\n",
    "\n",
    "if not os.path.exists(saveDir):\n",
    "    os.mkdir(saveDir)\n",
    "thr_low = np.percentile(df.swimDist_total, q_min)\n",
    "thr_high = np.percentile(df.swimDist_total, q_max)\n",
    "\n",
    "df_sub = df.loc[(df.swimDist_total > thr_low) & (df.swimDist_total < thr_high)]\n",
    "\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Total swim distance_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimDist_total_adj',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment', dodge = True,\n",
    "                 order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Total swim distance_log_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimDist_total_adj_log',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment', dodge = True,\n",
    "                 order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "thr_low = np.percentile(df.swimVel_max, q_min)\n",
    "thr_high = np.percentile(df.swimVel_max, q_max)\n",
    "\n",
    "df_sub = df.loc[(df.swimVel_max > thr_low) & (df.swimVel_max < thr_high)]\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Max swim vel_ctrl-vs-abl_mHom-interRS-ventralRS_boxen'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimVel_max_adj',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment',\\\n",
    "                 dodge = True, order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Max swim vel_log_ctrl-vs-abl_mHom-interRS-ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimVel_max_adj_log',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment',\\\n",
    "                 dodge = True, order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Plot latencies\n",
    "minLat = 5\n",
    "maxLat = 30\n",
    "\n",
    "    \n",
    "fn = f'Fig-{util.timestamp()}_Onset latencies_log_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "df_now = df.loc[(df.onsetLatency >= minLat) & (df.onsetLatency <= maxLat)]\n",
    "\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='boxen', dodge=True, aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_boxen.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_boxen.png'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='swarm', dodge=True, aspect=2, alpha=0.5)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_swarm.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_swarm.png'))\n",
    "\n",
    "plt.show()\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='box', dodge=True, aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_box.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_box.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV/pickle file file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "saveDir = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = f'Dataframe with swim trajectories and global swim parameters_{util.timestamp()}'\n",
    "%time df.to_csv(os.path.join(saveDir,fn + '.csv'), columns=df.keys(), index = False)\n",
    "%time df.to_pickle(os.path.join(saveDir, fn + '.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_csv = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = ft.findAndSortFilesInDir(dir_csv, search_str='Dataframe with swim trajectories', ext = '.pkl')[-1]\n",
    "df = pd.read_pickle(os.path.join(dir_csv, fn))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume after loading the saved csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = ft.findAndSortFilesInDir(saveDir, ext = 'csv', search_str= 'mHomologs_intermediateRS_ventralRS')\n",
    "if len(fn)>0:\n",
    "    fn = fn[-1]\n",
    "df = pd.read_csv(os.path.join(saveDir,fn))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Bend amplitude vs bend number\n",
    "\n",
    "figDir = f'Y:\\Avinash\\Projects\\RS recruitment\\Figures\\{util.timestamp()}'\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "nBends = 10\n",
    "\n",
    "# plt.style.use(('seaborn-paper', 'seaborn-whitegrid'))\n",
    "amps = np.array(df.bendAmp_rel)\n",
    "amps = np.delete(amps, np.where(np.isnan(amps)))\n",
    "amp_low = np.percentile(amps,5)\n",
    "amp_high = np.percentile(amps,99)\n",
    "\n",
    "pers = np.array(df.bendInt)\n",
    "pers = np.delete(pers, np.where(np.isnan(pers)))\n",
    "per_low = np.percentile(pers,5)\n",
    "per_high = np.percentile(pers,95)\n",
    "\n",
    "df_now = df.loc[(df.bendAmp_rel>= amp_low) & (df.bendAmp_rel <= amp_high) & (df.bendInt >= per_low) & \\\n",
    "               (df.bendInt <= per_high) & (df.bendNum>=0) & (df.bendNum <=nBends)]\n",
    "\n",
    "# df_now = df.loc[(df.bendAmp_rel>= 40) & (df.bendAmp_rel <= amp_high) & \\\n",
    "#                 (df.bendInt <= 60) & (df.bendNum>=0) & (df.bendNum <=10)]\n",
    "\n",
    "plt.style.use(('seaborn-ticks', 'seaborn-paper'))\n",
    "fh = sns.catplot(data = df_now, x = 'bendNum', y = 'bendAmp_rel', hue = 'treatment', col = 'ablationGroup',\\\n",
    "            col_order=['mHom', 'intermediateRS', 'ventralRS'], aspect = 1.5, dodge = True,\\\n",
    "            sharex=True, sharey=True, kind = 'point', palette=plt.cm.tab10(np.arange(10)),\\\n",
    "           hue_order = ['ctrl','abl'], height = 3, ci  =99, scale = 0.5)\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Bend amplitude vs bend number for ctrl and abl_mHom_intermediate_ventral'\n",
    "fh.savefig(os.path.join(figDir, figName + '.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(figDir, figName + '.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "print(f'Saved at {figDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Bend interval\n",
    "nBends = 10\n",
    "\n",
    "df_now = df.loc[(df.bendAmp_rel>= amp_low) & (df.bendAmp_rel <= amp_high) & \\\n",
    "                (df.bendInt <= per_high) & (df.bendNum>=0) & (df.bendNum <=nBends)]\n",
    "\n",
    "\n",
    "# plt.style.use(('seaborn-white', 'seaborn-paper'))\n",
    "fh = sns.catplot(data = df_now, x = 'bendNum', y = 'bendInt', hue = 'treatment', col = 'ablationGroup',\\\n",
    "            col_order=['mHom', 'intermediateRS', 'ventralRS'], aspect = 1.5, dodge = True,\\\n",
    "            sharex=True, sharey=True, kind = 'point', palette=plt.cm.tab10(np.arange(10)),\\\n",
    "           hue_order = ['ctrl','abl'], height = 3, ci  =99, scale = 0.5)\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Bend interval vs bend number for ctrl and abl_mHom_intermediate_ventral'\n",
    "fh.savefig(os.path.join(figDir, figName + '.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(figDir, figName + '.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "print(f'Saved at {figDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
