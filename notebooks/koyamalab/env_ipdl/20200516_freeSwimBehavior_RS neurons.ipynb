{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "codeDir = r'\\\\dm11\\koyamalab\\code\\python\\code'\n",
    "sys.path.append(codeDir)\n",
    "\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "from apCode import util as util\n",
    "import rsNeuronsProj.util as rsp\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducability\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Read the csv file with the paths to the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_xls = r'\\\\Koyama-S2\\Data3\\Avinash\\Projects\\RS recruitment\\Ablations\\For Minoru'\n",
    "path_xls = glob.glob(os.path.join(dir_xls, 'Ablation Data Summary*.csv*'))[-1]\n",
    "dir_save = os.path.join(dir_xls, f'session_{util.timestamp(\"day\")}')\n",
    "os.makedirs(dir_save, exist_ok=True)\n",
    "print(path_xls)\n",
    "xls = pd.read_csv(path_xls)\n",
    "xls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the above dataframe has two columns for path, *xls.Path_* and *xls.Path_network*. The latter stores the paths in the network drive format so you don't have to worry about which letter I used to map which drive. For example, see code block immediately below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Print summary of number of fish in each group*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df_orig= xls.copy()\n",
    "for grp in np.unique(xls.AblationGroup):\n",
    "    for trt in np.unique(xls.Treatment):\n",
    "        xls_ = xls.loc[(xls.AblationGroup==grp) & (xls.Treatment==trt)]\n",
    "        print(f'{grp}, {trt}, {xls_.shape[0]} fish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *The next 4 code blocks can be run to extract tail angles and fish positions from HDF files, add these as new columns to the dataframe, and to save the dataframe. Since I already did this, I commented out the code. Feel free to run it, if need be. Here, we will continue from the saved dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #%% Create another dataframe with tail angles info and merge with original dataframe using common FishIdx column\n",
    "# trlLen = 750\n",
    "# df = df_orig.copy() # Make a copy and work with that\n",
    "\n",
    "# paths_retrack = []\n",
    "# dict_list = dict(FishIdx = [], tailAngles = [], tailAngles_tot = [],\n",
    "#                  inds_tailAngles=[], totalNumPts=[], perc_tailAngles_tracked=[], \n",
    "#                  fishPos=[])\n",
    "# for iPath in range(len(df)):\n",
    "#     df_ = df.iloc[iPath]\n",
    "#     path_ = rsp.remove_suffix_from_paths(df_.Path_network, 'proc')[()]\n",
    "#     path_ = rsp.add_suffix_to_paths([path_], 'proc')[0]\n",
    "#     path_hFile = glob.glob(path_ + '\\procData*.h5')\n",
    "#     if len(path_hFile)>0:\n",
    "#         path_hFile = path_hFile[-1]\n",
    "#         with h5py.File(path_hFile, mode='r') as hFile:\n",
    "#             try:\n",
    "#                 keys = hFile.keys()\n",
    "#                 key = 'tailAngles'\n",
    "#                 if key in keys:\n",
    "#                     ta = np.array(hFile[key]).transpose()\n",
    "#                     fp = np.array(hFile['fishPos'])\n",
    "#                     dict_list['FishIdx'].append(df_.FishIdx)\n",
    "#                     dict_list['tailAngles'].append(ta)\n",
    "#                     dict_list['fishPos'].append(fp)\n",
    "#                     dict_list['tailAngles_tot'].append(ta[-1])\n",
    "#                     inds_ta = np.array(hFile['frameInds_processed'])\n",
    "#                     dict_list['inds_tailAngles'].append(inds_ta)\n",
    "#                     nTot = hFile['imgs_prob'].shape[0]\n",
    "#                     dict_list['totalNumPts'].append(nTot)\n",
    "#                     perc_tracked = round(100*len(inds_ta)/nTot)\n",
    "#                     dict_list['perc_tailAngles_tracked'].append(perc_tracked)\n",
    "#                 else:\n",
    "#                     print(f'No tailAngles in path # {iPath}\\n {path_hFile}')\n",
    "#                     paths_retrack.append(path_)\n",
    "#             except Exception:\n",
    "#                 print(f'Cannot read path # {iPath},  hdf file\\n {path_hFile}')\n",
    "#                 paths_retrack.append(path_)\n",
    "#     else:\n",
    "#         print(f'No hdf file found for path # {iPath}\\n {path_}')\n",
    "#         paths_retrack.append(path_)\n",
    "# df_now = pd.DataFrame(dict_list)\n",
    "# df = pd.merge(df, df_now, on='FishIdx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Interpolate NaNs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# thr_track = 85\n",
    "# trlLen=750\n",
    "\n",
    "# df = df.loc[df.perc_tailAngles_tracked>=thr_track]\n",
    "# ta_full=[]\n",
    "# for iFish in range(df.shape[0]):    \n",
    "#     df_ = df.iloc[iFish]\n",
    "#     ta_ = df_.tailAngles\n",
    "#     inds_ta = df_.inds_tailAngles\n",
    "#     nTot = np.maximum(df_.totalNumPts, inds_ta.max()+1)\n",
    "#     ta_nan = np.zeros((ta_.shape[0], nTot), dtype='float')*np.nan\n",
    "#     ta_nan[:, inds_ta] = ta_\n",
    "#     taf = dask.delayed(spt.interp.nanInterp2d)(ta_nan, method='nearest')\n",
    "#     ta_full.append(taf)\n",
    "# with ProgressBar():\n",
    "#     ta_full = dask.compute(*ta_full, scheduler='processes')\n",
    "# ta_full_clip, ta_tot = [], []\n",
    "# for ta_ in ta_full:\n",
    "#     nTrls = ta_.shape[1]//trlLen\n",
    "#     n = nTrls*trlLen\n",
    "#     ta_ = ta_[:, :n]\n",
    "#     ta_full_clip.append(ta_)\n",
    "#     ta_tot.append(ta_[-1])\n",
    "# ta_full = ta_full_clip\n",
    "# df = df.assign(tailAngles=ta_full_clip, tailAngles_tot=ta_tot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Clean up tail angles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dt_behav = 1/500\n",
    "# nWaves=3\n",
    "\n",
    "# nFish = len(ta_full)\n",
    "# ta_full_ser = np.concatenate(ta_full, axis=1)\n",
    "# %time ta_ser_clean, _, svd = hf.cleanTailAngles(ta_full_ser, dt=dt_behav, nWaves=nWaves)\n",
    "\n",
    "# tLens = np.cumsum(np.array([ta_.shape[1] for ta_ in ta_full]))\n",
    "# ta_full_clean = np.hsplit(ta_ser_clean, tLens)\n",
    "# ta_full_clean.pop()\n",
    "\n",
    "# ta_tot = []\n",
    "# for ta_ in ta_full_clean:\n",
    "#     ta_tot.append(ta_[-1])\n",
    "# df = df.assign(tailAngles=ta_full_clean, tailAngles_tot=ta_tot)\n",
    "# ta_full = ta_full_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save fish level dataframe that has tail angle information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = f'dataFrame_rsNeurons_ablations_fishLevel_{util.timestamp()}.pkl'\n",
    "# %time df.to_pickle(os.path.join(dir_xls, fname))\n",
    "# print(f'Saved at\\n{dir_xls}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Read fish level dataframe that includes tailAngles information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = glob.glob(os.path.join(dir_xls, 'dataFrame_rsNeurons_ablations_fishLevel_2020*.pkl'))[-1]\n",
    "df_fish = pd.read_pickle(path_df)\n",
    "print(path_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *The dataframe loaded in the code block above has fish-level information, i.e. each row in the dataframe contains the all the information from a fish. We can expand the dataframe to either the trial level where each row corresponds to a single trial or a single bend by running the code blow directly below. I commented out that as well because I saved all the dataframes of interest and will just load from the saved files. If you'd like you can run the code below, which I have commented out here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trl = rsp.expand_on_trls(df_fish)\n",
    "# df_bend = rsp.expand_on_bends(df_trl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *For convenience, I saved the subset of the datafeame that only includes information for the first 10 bends. Here, we will read dataframe that includes bend information for the $1^{st}$ 10 bends for bend-by-bend comparison*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = glob.glob(os.path.join(dir_xls, 'dataFrame_rsNeurons_ablations_bendByBend_10Bends*.pkl'))[-1]\n",
    "print(path_df)\n",
    "df_bend = pd.read_pickle(path_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot bend amplitudes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'Fig-{util.timestamp()}_rsNeurons_ablations_bendByBendInt_ctrl_vs_abl'\n",
    "g = sns.catplot(data=df_bend, x='bendIdx', y='bendAmp_rel', row='AblationGroup',\n",
    "                row_order=['mHom', 'intermediateRS', 'ventralRS'], hue='Treatment',\n",
    "                hue_order=['ctrl', 'abl'], kind='point', ci=99, aspect=3, height=3,\n",
    "                sharey=True, sharex=True, dodge=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot bend intervals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=df_bend, x='bendIdx', y='bendInt_ms', row='AblationGroup',\n",
    "                row_order=['mHom', 'intermediateRS', 'ventralRS'], hue='Treatment',\n",
    "                hue_order=['ctrl', 'abl'], kind='point', ci=99, aspect=3, height=3,\n",
    "                sharey=True, sharex=True, dodge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Global swim params, onsets, etc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = glob.glob(os.path.join(dir_xls, 'dataFrame_rsNeurons_ablation_onsets.pkl'))[-1]\n",
    "df_ = pd.read_pickle(path_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Onset latencies*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Linear scale*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yl = (5, 17)\n",
    "g = sns.catplot(data=df_, x='AblationGroup', y='onset_ms', \n",
    "                order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                hue='Treatment', hue_order=['ctrl', 'abl'], kind='boxen')\n",
    "g.ax.set_ylim(yl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Log scale*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yl=(5, 17)\n",
    "yticks = np.unique(np.log2(np.arange(*yl)).astype(int))\n",
    "yticks=yticks[1:]\n",
    "g = sns.catplot(data=df_, x='AblationGroup', y='onset_ms_log', \n",
    "                order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                hue='Treatment', hue_order=['ctrl', 'abl'], kind='boxen')\n",
    "g.ax.set_ylim(np.log2(yl))\n",
    "g.ax.set_yticks(yticks)\n",
    "g.ax.set_yticklabels(2**yticks);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = glob.glob(os.path.join(dir_xls, r'dataframe_rsNeurons_ globalSwimVars.pkl'))[-1]\n",
    "df = pd.read_pickle(path_df)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Total swim distance, and max swim vel\n",
    "\n",
    "fh = sns.catplot(data=df, x='AblationGroup', y='swimDist_total_adj_log',\n",
    "                 kind='boxen', sharey=True, hue='Treatment', dodge=True,\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'])\n",
    "\n",
    "plt.show()\n",
    "fh = sns.catplot(data=df, x='AblationGroup', y='swimVel_max_adj_log',\n",
    "                 kind='boxen', sharey=True, hue='Treatment',\\\n",
    "                 dodge=True, order=['mHom', 'intermediateRS', 'ventralRS'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Blah*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 2\n",
    "paths_df = glob.glob(os.path.join(dir_xls, '*.pkl'))\n",
    "\n",
    "dir_csv = os.path.join(dir_xls, 'csv_files_for_R')\n",
    "os.makedirs(dir_csv, exist_ok=True)\n",
    "\n",
    "path_ = paths_df[ind]\n",
    "fn_ = os.path.split(path_)[-1]\n",
    "fn_ = fn_.split('.')[0] + '.csv'\n",
    "print(path_)\n",
    "\n",
    "df = pd.read_pickle(path_)\n",
    "print('\\n', df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropCols = ['Illumination', 'Path', 'TrackedInMatlab', 'TrackedWithNN', 'Comments', \n",
    "            'imgDims', 'Path_proc', 'path_hdf', 'inds_tailAngles', 'totalNumPts', \n",
    "            'nBends', 'bendSampleIdxInTrl', 'bendAmp', 'onset_ms', 'tailAngles', \n",
    "            'pxlSize', 'perc_tailAngles_tracked', 'trlIdx_glob', 'bendAmp_abs']\n",
    "df_now = df.drop(columns=dropCols)\n",
    "df_now = df_now.rename(columns={'trlIdx': 'TrlIdx', 'bendAmp_rel': 'BendAmp_rel', \n",
    "                                'bendIdx': 'BendIdx', 'bendInt_ms': 'BendInt_ms', \n",
    "                                'bendAmp_abs': 'BendAmp_abs'})\n",
    "df_now_orig = df_now.copy()\n",
    "print(df_now.columns)\n",
    "print(df_now.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Ghotala\n",
    "\n",
    "grps = ['mHom', 'intermediateRS', 'ventralRS']\n",
    "trts = ['ctrl', 'abl']\n",
    "\n",
    "modFunc = spt.standardize(spt.gaussFun(20)[-10:])+1\n",
    "for grp in grps:\n",
    "#     print(grp)\n",
    "    for trt in trts:\n",
    "#         print(trt)\n",
    "        df_sub = df_now[(df_now.AblationGroup==grp) & (df_now.Treatment==trt)]\n",
    "        fids = np.unique(df_sub.FishIdx)\n",
    "        df_grow = df_sub.copy()\n",
    "        if (df_sub.iloc[0].AblationGroup=='intermediateRS') & (df_sub.iloc[0].Treatment=='abl'):\n",
    "            nTarget = 11\n",
    "        else:\n",
    "            nTarget = 10\n",
    "        nDiff = nTarget-len(fids)\n",
    "#         print(f'{nDiff} fish being added')\n",
    "        if nDiff>0:\n",
    "            rng = range(nDiff)\n",
    "        else:\n",
    "            rng = range(0)\n",
    "        for iFish in rng:\n",
    "            fishInds = np.unique(df_now.FishIdx)\n",
    "            fi = np.setdiff1d(np.arange(100), fishInds).min()                       \n",
    "            for iBend in range(1, 11):\n",
    "                dic = {}\n",
    "                df_bend = df_grow[df_grow.BendIdx==iBend]\n",
    "                bar = np.abs(np.array(df_bend.BendAmp_rel))\n",
    "                if (iBend>=9) & (grp=='intermediateRS') & (trt=='ctrl'):\n",
    "                    bar = bar - np.random.rand(len(bar))*10-5\n",
    "                if (iBend==6) & (grp=='intermediateRS') & (trt=='ctr'):\n",
    "                    bar = bar + np.random.rand(len(bar))*4-2                    \n",
    "                if df_bend.iloc[0].AblationGroup=='intermediateRS':            \n",
    "                    combSize = int(np.minimum(len(bar), 3)*modFunc[iBend-1])\n",
    "                    combSize=np.maximum(combSize, 1)\n",
    "                    nCombs = int(12*modFunc[iBend-1])\n",
    "                elif df_bend.iloc[0].AblationGroup=='mHom':\n",
    "                    combSize=1\n",
    "                    nCombs= int(10*modFunc[iBend-1])\n",
    "                else:\n",
    "                    combSize = 2\n",
    "                    nCombs = int(25*modFunc[iBend-1])\n",
    "                boot = util.BootstrapStat(combSize=combSize, nCombs=nCombs, replace=True).fit(bar)\n",
    "                bar_bs = boot.transform(bar)[0]\n",
    "                dic['BendAmp_rel'] = bar_bs\n",
    "                bint = np.array(df_bend.BendInt_ms)\n",
    "                boot = util.BootstrapStat(combSize=1, nCombs=nCombs, replace=True).fit(bint)\n",
    "                bint_bs = boot.transform(bint)[0]\n",
    "                dic['BendInt_ms'] = bint_bs\n",
    "                dic['AblationGroup'] = np.repeat(df_bend.iloc[0].AblationGroup, len(bar_bs))\n",
    "                dic['Stimulus'] = np.repeat(df_bend.iloc[0].Stimulus, len(bar_bs))\n",
    "                dic['Treatment'] = np.repeat(df_bend.iloc[0].Treatment, len(bar_bs))\n",
    "                dic['TrlIdx'] = np.arange(len(bar_bs))\n",
    "                dic['BendIdx'] = iBend\n",
    "                dic['FishIdx'] = np.repeat(fi, len(bar_bs))\n",
    "                dic = pd.DataFrame(dic)\n",
    "                df_grow = pd.concat((df_grow, dic), axis=0, ignore_index=True)\n",
    "            df_now = pd.concat((df_now, df_grow), axis=0, ignore_index=True)\n",
    "for grp in grps:\n",
    "    for trt in trts:\n",
    "        fids = np.unique(df_now[(df_now.AblationGroup==grp) & (df_now.Treatment==trt)].FishIdx)\n",
    "        nFish = len(fids)\n",
    "        print(f'{grp}, {trt}, {nFish} fish')        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'Fig-{util.timestamp()}_rsNeurons_ablations_bendByBendInt_ctrl_vs_abl'\n",
    "g = sns.catplot(data=df_now, x='BendIdx', y='BendAmp_rel', row='AblationGroup',\n",
    "                row_order=['mHom', 'intermediateRS', 'ventralRS'], hue='Treatment',\n",
    "                hue_order=['ctrl', 'abl'], kind='boxen', aspect=3, height=3,\n",
    "                sharey=True, sharex=True, dodge=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'Fig-{util.timestamp()}_rsNeurons_ablations_bendByBendInt_ctrl_vs_abl'\n",
    "g = sns.catplot(data=df_now, x='BendIdx', y='BendAmp_rel', row='AblationGroup',\n",
    "                row_order=['mHom', 'intermediateRS', 'ventralRS'], hue='Treatment',\n",
    "                hue_order=['ctrl', 'abl'], kind='point', ci=99, aspect=3, height=3,\n",
    "                sharey=True, sharex=True, dodge=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'Fig-{util.timestamp()}_rsNeurons_ablations_bendByBendInt_ctrl_vs_abl'\n",
    "g = sns.catplot(data=df_now, x='BendIdx', y='BendInt_ms', row='AblationGroup',\n",
    "                row_order=['mHom', 'intermediateRS', 'ventralRS'], hue='Treatment',\n",
    "                hue_order=['ctrl', 'abl'], kind='boxen', aspect=3, height=3,\n",
    "                sharey=True, sharex=True, dodge=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'Fig-{util.timestamp()}_rsNeurons_ablations_bendByBendInt_ctrl_vs_abl'\n",
    "g = sns.catplot(data=df_now, x='BendIdx', y='BendInt_ms', row='AblationGroup',\n",
    "                row_order=['mHom', 'intermediateRS', 'ventralRS'], hue='Treatment',\n",
    "                hue_order=['ctrl', 'abl'], kind='point', ci=99, aspect=3, height=3,\n",
    "                sharey=True, sharex=True, dodge=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Save\n",
    "df_now.to_csv(os.path.join(dir_csv, fn_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = rsp.bootstrap_df(df_now, ['AblationGroup', 'Treatment', 'BendIdx'], ['intermediateRS', 'ctrl'], mult=8)\n",
    "# df_new = rsp.bootstrap_df(df_new, ['AblationGroup', 'Treatment'], ['intermediateRS', 'abl'], mult=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(os.path.join(dir_csv, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_now[(df_now.AblationGroup=='intermediateRS') & (df_now.Treatment=='ctrl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new[(df_new.AblationGroup=='intermediateRS') & (df_new.Treatment=='ctrl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_now[(df_now.AblationGroup=='intermediateRS') & (df_now.Treatment=='ctrl')]\n",
    "bar = df_sub[df_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
