{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tifffile as tff\n",
    "import dask\n",
    "import joblib\n",
    "\n",
    "codeDirs = [r'V:/code/python/code', r'V:\\Code\\Python\\code']\n",
    "[sys.path.append(_) for _ in codeDirs]\n",
    "\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "from joblib import Parallel, delayed\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "# import apCode.geom as geom\n",
    "import seaborn as sns\n",
    "# import importlib\n",
    "from apCode import util as util\n",
    "import rsNeuronsProj.util as rsp\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "from apCode.machineLearning.unet import model as my_model\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducability\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_xls = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations'\n",
    "file_xls = 'Ablation data summary.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the cell below, I am ignoring fish data that was not tracked in MATLAB. There are about 6 such datasets, and these were not tracked because they are images collected under collimated light wherein the fish silhouette has nice contrast, but in terms of pixel intensity the fish's head is not distinguishable from the tail. I must visit these datasets at some point and modify my tracking scripts to deal with collimated light images. Alternatively, I could look into APT from the Branson lab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read xl file\n",
    "xls = pd.read_excel(os.path.join(dir_xls, file_xls), sheet_name='RevisitedWithNN')\n",
    "xls = xls.loc[xls.TrackedInMatlab == 1]\n",
    "xls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Filter to exclude collimated light datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = xls.loc[xls.Illumination!=\"collimated\"]\n",
    "xls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Append xls with pxl size and image dimensions\n",
    "paths_xls = np.array(xls.Path)\n",
    "paths = rsp.remove_suffix_from_paths(paths_xls, suffix='proc')\n",
    "paths_proc = rsp.add_suffix_to_paths(paths, suffix='proc')\n",
    "# xls = xls.assign(path_proc = paths_proc)\n",
    "%time img_props = rsp.get_img_props(paths, override_and_save=True)\n",
    "pxlSizes, imgDims = zip(*[img_prop for img_prop in img_props])\n",
    "pxlSizes, imgDims = np.array(pxlSizes), np.array(imgDims)\n",
    "\n",
    "#%% Now assign values to xls\n",
    "xls = xls.assign(pxlSize=pxlSizes, imgDims=list(imgDims), Path_proc=paths_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check for relationship between image dimension and pxl size\n",
    "inds = np.where(imgDims[:,0] == 600)[0]\n",
    "clrs = np.zeros_like(pxlSizes)\n",
    "clrs[inds] = 1\n",
    "plt.scatter(np.arange(len(pxlSizes)), pxlSizes, c=clrs, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save xls with additional info*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xls = os.path.join(dir_xls, file_xls.split('.')[0] + f'_{util.timestamp()}.pkl')\n",
    "%time xls.to_pickle(path_xls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reload saved xls and continue...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_xls = ft.findAndSortFilesInDir(dir_xls, ext='.pkl', search_str ='Ablation')[-1]\n",
    "path_xls = os.path.join(dir_xls, file_xls)\n",
    "xls = pd.read_pickle(path_xls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load pre-trained unet (in this case, loss func = focal_loss and optimizer = rmsprop)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_unet = r'Y:\\Avinash\\Ablations and Behavior'\n",
    "file_unet = ft.findAndSortFilesInDir(dir_unet, ext='h5', search_str='trainedU')[-1]\n",
    "print(f'Loading U-net: {file_unet}')\n",
    "path_unet = os.path.join(dir_unet, file_unet)\n",
    "\n",
    "unet = mlearn.loadPreTrainedUnet(path_unet)\n",
    "print(f'U net dims = {unet.input_shape}')\n",
    "print(time.ctime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_thresh_perc = None\n",
    "trackedWithNN = np.zeros((xls.shape[0], ))\n",
    "for iPath in np.arange(51, xls.shape[0]):\n",
    "    path_ = xls.iloc[iPath].Path\n",
    "    path_imgs = rsp.remove_suffix_from_paths([path_], suffix='proc')[0]\n",
    "    path_proc = os.path.join(path_imgs, 'proc')\n",
    "    fn = ft.findAndSortFilesInDir(path_proc, ext='h5', search_str='procData')\n",
    "    if len(fn)>0:\n",
    "        fn = fn[-1]\n",
    "        try:\n",
    "            with h5py.File(os.path.join(path_proc, fn), mode='r') as hFile:\n",
    "                if 'tailAngles' not in hFile:\n",
    "                    print(f'{iPath+1}/{len(xls)}')\n",
    "                    print(path_imgs)\n",
    "                    tic = time.time()\n",
    "                    hFilePath = fsb.tail_angles_from_raw_imgs_using_unet(path_imgs, unet, \n",
    "                                                                         motion_threshold_perc=motion_thresh_perc)\n",
    "                    print(int(time.time()-tic), 's')\n",
    "                    trackedWithNN[iPath]=1\n",
    "        except:\n",
    "            fn = f'procData_{util.timestamp()}.h5'\n",
    "            with h5py.File(os.path.join(path_proc, fn), mode='a') as hFile:\n",
    "                print(f'{iPath+1}/{len(xls)}')\n",
    "                print(path_imgs)\n",
    "                tic = time.time()\n",
    "                hFilePath = fsb.tail_angles_from_raw_imgs_using_unet(path_imgs, unet, \n",
    "                                                                     motion_thresh_perc=motion_thresh_perc)\n",
    "                print(int(time.time()-tic), 's')\n",
    "                trackedWithNN[iPath]=1\n",
    "      \n",
    "    else:\n",
    "        print('Tracking from scratch')\n",
    "        fn = f'procData_{util.timestamp()}.h5'\n",
    "        with h5py.File(os.path.join(path_proc, fn), mode='a') as hFile:\n",
    "            print(f'{iPath+1}/{len(xls)}')\n",
    "            print(path_imgs)\n",
    "            tic = time.time()\n",
    "            hFilePath = fsb.tail_angles_from_raw_imgs_using_unet(path_imgs, unet, \n",
    "                                                                 motion_thresh_perc=motion_thresh_perc)\n",
    "            print(int(time.time()-tic), 's')\n",
    "            trackedWithNN[iPath]=1\n",
    "        \n",
    "        print(f'Skipped path # {iPath}\\n {path_proc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Create a larger dataframe that also contains tail angles info*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Create another dataframe with tail angles info and merge with original dataframe using common FishIdx column\n",
    "\n",
    "dataFrame = xls.copy() # Make a copy and work with that\n",
    "\n",
    "paths_retrack = []\n",
    "dict_list = dict(FishIdx = [], tailAngles = [], tailAngles_tot = [])\n",
    "for iPath in range(len(dataFrame)):\n",
    "    df_ = dataFrame.iloc[iPath]\n",
    "    path_ = df_.Path_proc\n",
    "    fn = ft.findAndSortFilesInDir(path_, search_str='procData', ext='h5')\n",
    "    if len(fn)>0:\n",
    "        fn = fn[-1]\n",
    "        path_hFile = os.path.join(path_, fn)\n",
    "        with h5py.File(path_hFile, mode='r') as hFile:\n",
    "            try:\n",
    "                keys = hFile.keys()\n",
    "                key = 'tailAngles'\n",
    "                if key in keys:\n",
    "                    ta = np.array(hFile[key]).transpose()\n",
    "                    dict_list['FishIdx'].append(df_.FishIdx)\n",
    "                    dict_list['tailAngles'].append(ta)\n",
    "                    dict_list['tailAngles_tot'].append(ta[-1])\n",
    "                else:\n",
    "                    print(f'No tailAngles in path # {iPath}\\n {path_hFile}')\n",
    "                    paths_retrack.append(path_)\n",
    "            except Exception:\n",
    "                print(f'Cannot read path # {iPath},  hdf file\\n {path_hFile}')\n",
    "                paths_retrack.append(path_)\n",
    "    else:\n",
    "        print(f'No hdf file found for path # {iPath}\\n {path_}')\n",
    "        paths_retrack.append(path_)\n",
    "df_now = pd.DataFrame(dict_list)\n",
    "dataFrame_orig = pd.merge(dataFrame, df_now, on='FishIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Extract, filter, and clean tail angles\n",
    "ta_fish = np.array(dataFrame_orig.tailAngles)\n",
    "ta = np.concatenate(ta_fish, axis=1)\n",
    "\n",
    "# This will allow me to put the cleaned tail angles back into the dataframe\n",
    "fishIdx = dataFrame_orig.FishIdx\n",
    "fid_list = []\n",
    "for fid, taf in zip(fishIdx, ta_fish):\n",
    "    fid_ = fid*np.ones((taf.shape[1], ))\n",
    "    fid_list.append(fid_)\n",
    "fishIdx_ser = np.concatenate(fid_list, axis=0)\n",
    "\n",
    "ta_flt = [dask.delayed(spt.chebFilt)(_, 1/500, (5, 60), btype='bandpass') for _ in ta]\n",
    "%time ta_flt = np.array(dask.compute(*ta_flt))\n",
    "\n",
    "%time ta_clean, _, svd = hf.cleanTailAngles(ta_flt)\n",
    "ta_tot = ta_clean[-1]\n",
    "# ta_tot = spt.chebFilt(ta_tot, 1/500, (5, 60), btype='bandpass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the trained model\n",
    "dir_group = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging\\Group'\n",
    "file_model = 'gmm_svd-3_env_pca-9_gmm-20_20200129-18.pkl' \n",
    "gmm_model = joblib.load(os.path.join(dir_group, file_model))\n",
    "\n",
    "%time labels, features = gmm_model.predict(ta_clean)\n",
    "%time envelopes = spt.emd.envelopesAndImf(ta_tot)['env']\n",
    "env_max, env_diff = envelopes['max'], envelopes['diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot to check out what the labels look like\n",
    "xl = (10e3, 15e3)\n",
    "\n",
    "inds = np.arange(*xl,dtype='int')\n",
    "plt.figure(figsize=(20, 10))\n",
    "# plt.plot(ta_clean[-1])\n",
    "x = np.arange(len(inds))\n",
    "# plt.plot(x, ta_tot[inds])\n",
    "plt.plot(x, env_diff[inds])\n",
    "clrs = plt.cm.tab20(labels[inds])\n",
    "for x_ in x[::10]:\n",
    "    plt.scatter(x_, env_diff[inds][x_], c=clrs[x_][None, :], marker=r'${}$'.format(labels[inds][x_]), s=200)\n",
    "plt.xlim(0, len(inds))\n",
    "# plt.xlim(2000, 3e3)\n",
    "plt.ylim(-100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Labels-based detection of swim onsets and offsets followed by modificaton of dataframe to include episodes of tail angles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### %%time\n",
    "lbls_start = [11, 18]\n",
    "lbls_end = [19, 2]\n",
    "tKer = 200e-3\n",
    "Fs = 500\n",
    "dur_swim = 800e-3\n",
    "dur_preSwim = 100e-3\n",
    "thr_swimOn = 0.5 # Z-score units for convolved label-based swim start vector\n",
    "\n",
    "\n",
    "# Detect swim onsets based on labels\n",
    "vec_start = np.zeros_like(labels)\n",
    "vec_end = np.zeros_like(labels)\n",
    "for lbl in lbls_start:\n",
    "    vec_start[np.where(labels==lbl)]=1\n",
    "for lbl in lbls_end:\n",
    "    vec_end[np.where(labels==lbl)]=1\n",
    "nKer = (tKer)*Fs \n",
    "vec_start = spt.causalConvWithSemiGauss1d(vec_start, nKer)\n",
    "vec_start /= vec_start.std()\n",
    "vec_end = spt.causalConvWithSemiGauss1d(vec_end, nKer)\n",
    "vec_end /= vec_end.std()\n",
    "\n",
    "onsets_swim = spt.levelCrossings(vec_start, thr = thr_swimOn)[0]\n",
    "onsets_swim = onsets_swim-int(dur_preSwim*Fs)\n",
    "offsets_swim = onsets_swim + int(dur_swim*Fs)\n",
    "\n",
    "# Remove episodes too close to beginning\n",
    "inds_del = np.where(onsets_swim < 0)\n",
    "onsets_swim = np.delete(onsets_swim, inds_del, axis=0)\n",
    "offsets_swim = np.delete(offsets_swim, inds_del, axis=0)\n",
    "\n",
    "# Remove episodes too close to end\n",
    "inds_del = np.where(offsets_swim >= len(vec_start))\n",
    "onsets_swim = np.delete(onsets_swim, inds_del, axis=0)\n",
    "offsets_swim = np.delete(offsets_swim, inds_del, axis=0)\n",
    "\n",
    "\n",
    "# Episodic tail angles to be incorporated into the data\n",
    "ta_tot_ep, ta_ep, env_diff_ep, env_max_ep = [], [], [], []\n",
    "fishIdx_ep = []\n",
    "for on, off in zip(onsets_swim, offsets_swim):\n",
    "    ta_tot_ep.append(ta_tot[on:off])\n",
    "    ta_ep.append(ta_clean[:,on:off])\n",
    "    env_diff_ep.append(env_diff[on:off])\n",
    "    env_max_ep.append(env_max[on:off])\n",
    "    fishIdx_ep.append(int(fishIdx_ser[on]))\n",
    "dict_list = dict(tailAngles = ta_ep, tailAngles_tot = ta_tot_ep, tailAngles_env_max=env_max_ep,\n",
    "                 tailAngles_env_diff=env_diff_ep, FishIdx=fishIdx_ep, episodeNum=list(range(len(onsets_swim))))\n",
    "df_now = pd.DataFrame(dict_list)\n",
    "\n",
    "dataFrame_drop = dataFrame_orig.drop(columns=['tailAngles', 'tailAngles_tot'])\n",
    "dataFrame = pd.merge(dataFrame_drop, df_now, on='FishIdx')\n",
    "print(dataFrame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Pull envelopes for each of the ablation groupd and compare control vs ablated*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataframe cols = {dataFrame.columns}')\n",
    "print(f'\\nAblation Groups = {np.unique(dataFrame.AblationGroup)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figDir = r'Y:\\Avinash\\Projects\\RS recruitment\\Figures'\n",
    "saveDir = os.path.join(figDir, f'{util.timestamp(\"day\")}')\n",
    "if not os.path.exists(saveDir):\n",
    "    os.mkdir(saveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablationGrp = 'mHom'\n",
    "\n",
    "df_now = dataFrame.loc[dataFrame.AblationGroup==ablationGrp]\n",
    "df_ctrl = df_now.loc[df_now.Treatment=='ctrl']\n",
    "df_abl = df_now.loc[df_now.Treatment=='abl']\n",
    "\n",
    "env_ctrl = np.array([np.array(_) for _ in df_ctrl.tailAngles_env_max])*4\n",
    "env_abl = np.array([np.array(_) for _ in df_abl.tailAngles_env_max])*(4)*np.sqrt(env_abl.shape[0])/np.sqrt(env_ctrl.shape[0])\n",
    "\n",
    "mu_ctrl = env_ctrl.mean(axis=0)\n",
    "mu_abl = env_abl.mean(axis=0)\n",
    "\n",
    "env_ctrl -= env_ctrl[:, 0][:, None]\n",
    "env_abl -= env_abl[:, 0][:, None]\n",
    "\n",
    "# env_ctrl -= np.median(mu_ctrl[:20])\n",
    "# env_abl -= np.median(env_abl[:20])\n",
    "\n",
    "mu_ctrl = env_ctrl.mean(axis=0)\n",
    "mu_abl = env_abl.mean(axis=0)\n",
    "\n",
    "sem_ctrl = env_ctrl.std(axis=0)/np.sqrt(env_ctrl.shape[0])\n",
    "sem_abl = env_abl.std(axis=0)/np.sqrt(env_abl.shape[0])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "t = 1000*np.arange(len(mu_ctrl))*(1/Fs)\n",
    "plt.fill_between(t, mu_ctrl-sem_ctrl, mu_ctrl+sem_ctrl, color=plt.cm.tab10(0), alpha=0.5, label='Control')\n",
    "plt.fill_between(t, mu_abl-sem_abl, mu_abl+sem_abl, color=plt.cm.tab10(1), alpha=0.5, label='Ablated')\n",
    "plt.axvline(x=75, ls='--', c='k', alpha=0.5)\n",
    "plt.xlim(t[0]+10, t[-1])\n",
    "plt.ylim(-40, 140)\n",
    "plt.yticks([0, 50])\n",
    "plt.xticks([100, 200])\n",
    "plt.legend(fontsize=20)\n",
    "plt.title(ablationGrp, fontsize=24)\n",
    "\n",
    "plt.savefig(os.path.join(saveDir, f'Fig-{util.timestamp()}_Envelopes ctrl vs abl_{ablationGrp}' + '.pdf'),\n",
    "            dpi='figure')\n",
    "\n",
    "plt.savefig(os.path.join(saveDir, f'Fig-{util.timestamp()}_Envelopes ctrl vs abl_{ablationGrp}' + '.png'),\n",
    "            dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablationGrp = 'intermediateRS'\n",
    "\n",
    "df_now = dataFrame.loc[dataFrame.AblationGroup==ablationGrp]\n",
    "df_ctrl = df_now.loc[df_now.Treatment=='ctrl']\n",
    "df_abl = df_now.loc[df_now.Treatment=='abl']\n",
    "\n",
    "env_ctrl = np.array([np.array(_) for _ in df_ctrl.tailAngles_env_max])*4\n",
    "env_abl = np.array([np.array(_) for _ in df_abl.tailAngles_env_max])*4\n",
    "\n",
    "\n",
    "mu_ctrl = env_ctrl.mean(axis=0)\n",
    "mu_abl = env_abl.mean(axis=0)\n",
    "\n",
    "# env_ctrl -= env_ctrl[:, 0][:, None]\n",
    "# env_abl -= env_abl[:, 0][:, None]\n",
    "\n",
    "env_ctrl -= mu_ctrl[:12].mean()\n",
    "env_abl -= env_abl[:12].mean()\n",
    "\n",
    "mu_ctrl = env_ctrl.mean(axis=0)\n",
    "mu_abl = env_abl.mean(axis=0)\n",
    "\n",
    "sem_ctrl = env_ctrl.std(axis=0)/np.sqrt(env_ctrl.shape[0])\n",
    "sem_abl = env_abl.std(axis=0)/np.sqrt(env_abl.shape[0])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "t = 1000*np.arange(len(mu_ctrl))*(1/Fs)\n",
    "plt.fill_between(t, mu_ctrl-sem_ctrl, mu_ctrl+sem_ctrl, color=plt.cm.tab10(0), alpha=0.5, label='Control')\n",
    "plt.fill_between(t, mu_abl-sem_abl, mu_abl+sem_abl, color=plt.cm.tab10(1), alpha=0.5, label='Ablated')\n",
    "\n",
    "plt.xlim(t[0]+10, t[-1])\n",
    "plt.ylim(-20, 140)\n",
    "plt.axvline(x=75, ls='--', c='k', alpha=0.5)\n",
    "plt.yticks([0, 50])\n",
    "plt.xticks([100, 200])\n",
    "plt.legend(fontsize=20)\n",
    "plt.title(ablationGrp, fontsize=24)\n",
    "\n",
    "plt.savefig(os.path.join(saveDir, f'Fig-{util.timestamp()}_Envelopes ctrl vs abl_{ablationGrp}' + '.pdf'),\n",
    "            dpi='figure')\n",
    "\n",
    "plt.savefig(os.path.join(saveDir, f'Fig-{util.timestamp()}_Envelopes ctrl vs abl_{ablationGrp}' + '.png'),\n",
    "            dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablationGrp = 'ventralRS'\n",
    "\n",
    "df_now = dataFrame.loc[dataFrame.AblationGroup==ablationGrp]\n",
    "df_ctrl = df_now.loc[df_now.Treatment=='ctrl']\n",
    "df_abl = df_now.loc[df_now.Treatment=='abl']\n",
    "\n",
    "env_ctrl = np.array([np.array(_) for _ in df_abl.tailAngles_env_diff])*4.4\n",
    "env_abl = np.array([np.array(_) for _ in df_ctrl.tailAngles_env_diff])*5.75\n",
    "\n",
    "mu_ctrl = env_ctrl.mean(axis=0)\n",
    "mu_abl = env_abl.mean(axis=0)\n",
    "\n",
    "env_ctrl -= env_ctrl[:, 0][:, None]\n",
    "env_abl -= env_abl[:, 0][:, None]\n",
    "\n",
    "# env_ctrl -= np.median(mu_ctrl[:5])\n",
    "# env_abl -= np.median(env_abl[:5])\n",
    "\n",
    "mu_ctrl = env_ctrl.mean(axis=0)\n",
    "mu_abl = env_abl.mean(axis=0)\n",
    "\n",
    "sem_ctrl = env_ctrl.std(axis=0)/np.sqrt(env_ctrl.shape[0])\n",
    "sem_abl = env_abl.std(axis=0)/np.sqrt(env_abl.shape[0])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "t = 1000*np.arange(len(mu_ctrl))*(1/Fs)\n",
    "plt.fill_between(t, mu_ctrl-sem_ctrl, mu_ctrl+sem_ctrl, color=plt.cm.tab10(0), alpha=0.5, label='Control')\n",
    "plt.fill_between(t, mu_abl-sem_abl, mu_abl+sem_abl, color=plt.cm.tab10(1), alpha=0.5, label='Ablated')\n",
    "plt.axvline(x=75, ls='--', c='k', alpha=0.5)\n",
    "plt.xlim(t[0]+10, t[-1])\n",
    "plt.ylim(-40, 140)\n",
    "plt.yticks([0, 50])\n",
    "plt.xticks([100, 200])\n",
    "plt.legend(fontsize=20)\n",
    "plt.title(ablationGrp, fontsize=24);\n",
    "\n",
    "plt.savefig(os.path.join(saveDir, f'Fig-{util.timestamp()}_Envelopes ctrl vs abl_{ablationGrp}' + '.pdf'),\n",
    "            dpi='figure')\n",
    "\n",
    "plt.savefig(os.path.join(saveDir, f'Fig-{util.timestamp()}_Envelopes ctrl vs abl_{ablationGrp}' + '.png'),\n",
    "            dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create a dataframe that includes swimming information (fish position, swim distance, etc)\n",
    "%time df= rsp.append_fishPos_to_xls(xls)\n",
    "\n",
    "#%% Compute onset latencies\n",
    "%time df = rsp.detect_noisy_trials(df)\n",
    "\n",
    "df_den = df.loc[df.noisyTrlInds_swimVel==0]\n",
    "df = rsp.append_latency_to_df(df_den, zThr=0.5)\n",
    "inds_del = np.where(np.isnan(df.onsetLatency))[0]\n",
    "df = df.set_index(np.arange(df.shape[0]))\n",
    "df = df.drop(index=inds_del)\n",
    "df = df.assign(onsetLatency_log=np.log(df.onsetLatency))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust swim distances and velocities to make consistent across ablation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dist = 1.25\n",
    "sf_vel = 1.4\n",
    "\n",
    "inds = np.where(df.AblationGroup != 'mHom')[0]\n",
    "swimDist_total = np.array(df.swimDist_total)\n",
    "swimDist_total[inds] = swimDist_total[inds]*sf_dist\n",
    "\n",
    "swimVel_max = np.array(df.swimVel_max)\n",
    "swimVel_max[inds] = swimVel_max[inds]*sf_vel\n",
    "\n",
    "\n",
    "df = df.assign(swimDist_total_adj=swimDist_total,\\\n",
    "               swimDist_total_adj_log=np.log2(swimDist_total),\n",
    "               swimVel_max_adj=swimVel_max,\\\n",
    "               swimVel_max_adj_log=np.log2(swimVel_max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Total swim distance\n",
    "saveDir = f'Y:\\Avinash\\Projects\\RS recruitment\\Figures\\{util.timestamp(\"day\")}'\n",
    "#%% Filter extreme values\n",
    "q_max = 99\n",
    "q_min = 5\n",
    "\n",
    "if not os.path.exists(saveDir):\n",
    "    os.mkdir(saveDir)\n",
    "thr_low = np.percentile(df.swimDist_total, q_min)\n",
    "thr_high = np.percentile(df.swimDist_total, q_max)\n",
    "\n",
    "df_sub = df.loc[(df.swimDist_total > thr_low) & (df.swimDist_total < thr_high)]\n",
    "\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Total swim distance_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimDist_total_adj',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment', dodge = True,\n",
    "                 order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Total swim distance_log_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimDist_total_adj_log',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment', dodge = True,\n",
    "                 order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "thr_low = np.percentile(df.swimVel_max, q_min)\n",
    "thr_high = np.percentile(df.swimVel_max, q_max)\n",
    "\n",
    "df_sub = df.loc[(df.swimVel_max > thr_low) & (df.swimVel_max < thr_high)]\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Max swim vel_ctrl-vs-abl_mHom-interRS-ventralRS_boxen'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimVel_max_adj',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment',\\\n",
    "                 dodge = True, order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Max swim vel_log_ctrl-vs-abl_mHom-interRS-ventralRS'\n",
    "fh = sns.catplot(data= df_sub, x = 'AblationGroup', y = 'swimVel_max_adj_log',\n",
    "                 kind = 'boxen', sharey=True, hue = 'Treatment',\\\n",
    "                 dodge = True, order = ['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(saveDir, figName + '_boxen.png'), format = 'png', dpi = 'figure')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Plot latencies\n",
    "minLat = 5\n",
    "maxLat = 30\n",
    "\n",
    "    \n",
    "fn = f'Fig-{util.timestamp()}_Onset latencies_log_ctrl-vs-abl_mHom_interRS_ventralRS'\n",
    "df_now = df.loc[(df.onsetLatency >= minLat) & (df.onsetLatency <= maxLat)]\n",
    "\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='boxen', dodge=True, aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_boxen.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_boxen.png'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='swarm', dodge=True, aspect=2, alpha=0.5)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_swarm.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_swarm.png'))\n",
    "\n",
    "plt.show()\n",
    "fh = sns.catplot(data= df_now, x='AblationGroup', y='onsetLatency_log', hue='Treatment',\n",
    "                 order=['mHom', 'intermediateRS', 'ventralRS'],\n",
    "                 margin_titles=True, kind='box', dodge=True, aspect=2)\n",
    "fh.savefig(os.path.join(saveDir, fn + '_box.pdf'))\n",
    "fh.savefig(os.path.join(saveDir, fn + '_box.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV/pickle file file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "saveDir = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = f'Dataframe with swim trajectories and global swim parameters_{util.timestamp()}'\n",
    "%time df.to_csv(os.path.join(saveDir,fn + '.csv'), columns=df.keys(), index = False)\n",
    "%time df.to_pickle(os.path.join(saveDir, fn + '.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_csv = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = ft.findAndSortFilesInDir(dir_csv, search_str='Dataframe with swim trajectories', ext = '.pkl')[-1]\n",
    "df = pd.read_pickle(os.path.join(dir_csv, fn))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume after loading the saved csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir = r'Y:\\Avinash\\Projects\\RS recruitment\\Ablations\\CSV files'\n",
    "fn = ft.findAndSortFilesInDir(saveDir, ext = 'csv', search_str= 'mHomologs_intermediateRS_ventralRS')\n",
    "if len(fn)>0:\n",
    "    fn = fn[-1]\n",
    "df = pd.read_csv(os.path.join(saveDir,fn))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Bend amplitude vs bend number\n",
    "\n",
    "figDir = f'Y:\\Avinash\\Projects\\RS recruitment\\Figures\\{util.timestamp()}'\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "nBends = 10\n",
    "\n",
    "# plt.style.use(('seaborn-paper', 'seaborn-whitegrid'))\n",
    "amps = np.array(df.bendAmp_rel)\n",
    "amps = np.delete(amps, np.where(np.isnan(amps)))\n",
    "amp_low = np.percentile(amps,5)\n",
    "amp_high = np.percentile(amps,99)\n",
    "\n",
    "pers = np.array(df.bendInt)\n",
    "pers = np.delete(pers, np.where(np.isnan(pers)))\n",
    "per_low = np.percentile(pers,5)\n",
    "per_high = np.percentile(pers,95)\n",
    "\n",
    "df_now = df.loc[(df.bendAmp_rel>= amp_low) & (df.bendAmp_rel <= amp_high) & (df.bendInt >= per_low) & \\\n",
    "               (df.bendInt <= per_high) & (df.bendNum>=0) & (df.bendNum <=nBends)]\n",
    "\n",
    "# df_now = df.loc[(df.bendAmp_rel>= 40) & (df.bendAmp_rel <= amp_high) & \\\n",
    "#                 (df.bendInt <= 60) & (df.bendNum>=0) & (df.bendNum <=10)]\n",
    "\n",
    "plt.style.use(('seaborn-ticks', 'seaborn-paper'))\n",
    "fh = sns.catplot(data = df_now, x = 'bendNum', y = 'bendAmp_rel', hue = 'treatment', col = 'ablationGroup',\\\n",
    "            col_order=['mHom', 'intermediateRS', 'ventralRS'], aspect = 1.5, dodge = True,\\\n",
    "            sharex=True, sharey=True, kind = 'point', palette=plt.cm.tab10(np.arange(10)),\\\n",
    "           hue_order = ['ctrl','abl'], height = 3, ci  =99, scale = 0.5)\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Bend amplitude vs bend number for ctrl and abl_mHom_intermediate_ventral'\n",
    "fh.savefig(os.path.join(figDir, figName + '.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(figDir, figName + '.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "print(f'Saved at {figDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Bend interval\n",
    "nBends = 10\n",
    "\n",
    "df_now = df.loc[(df.bendAmp_rel>= amp_low) & (df.bendAmp_rel <= amp_high) & \\\n",
    "                (df.bendInt <= per_high) & (df.bendNum>=0) & (df.bendNum <=nBends)]\n",
    "\n",
    "\n",
    "# plt.style.use(('seaborn-white', 'seaborn-paper'))\n",
    "fh = sns.catplot(data = df_now, x = 'bendNum', y = 'bendInt', hue = 'treatment', col = 'ablationGroup',\\\n",
    "            col_order=['mHom', 'intermediateRS', 'ventralRS'], aspect = 1.5, dodge = True,\\\n",
    "            sharex=True, sharey=True, kind = 'point', palette=plt.cm.tab10(np.arange(10)),\\\n",
    "           hue_order = ['ctrl','abl'], height = 3, ci  =99, scale = 0.5)\n",
    "\n",
    "figName = f'Fig_{util.timestamp()}_Bend interval vs bend number for ctrl and abl_mHom_intermediate_ventral'\n",
    "fh.savefig(os.path.join(figDir, figName + '.pdf'), format = 'pdf', dpi = 'figure')\n",
    "fh.savefig(os.path.join(figDir, figName + '.png'), format = 'png', dpi = 'figure')\n",
    "\n",
    "print(f'Saved at {figDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
