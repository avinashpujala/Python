{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, warnings, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import dask\n",
    "import dask.array as darr\n",
    "import caiman as cm\n",
    "import h5py\n",
    "import tifffile as tff\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.util import montage\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "codeDir = r'\\\\dm11\\koyamalab\\code\\python\\code'\n",
    "sys.path.append(codeDir)\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "from apCode.machineLearning.unet import model\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "import apCode.geom as geom\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "from apCode import util as util\n",
    "import apCode.ephys as ephys\n",
    "from apCode import hdf\n",
    "from apCode.imageAnalysis.spim import regress\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Let's first try the simpler version of the code where where are directly reading from a single fish path instead of reading from within the path stored in the excel file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_fish = r'Y:\\Avinash\\Head-fixed tail free\\GCaMP imaging\\2019-12-31\\f1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Read peristimulus  $Ca^{2+}$ images, store in an HDF file and get path the the HDF file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time hFilePath = hf.read_and_store_ca_imgs(dir_fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Read the raw $Ca^{2+}$ images from the HDF file, register and store in the same HDF file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "patchPerc = (60, ) # Default (40, )\n",
    "patchOverlapPerc = (80, ) # Default (70, ) \n",
    "hFilePath = hf.register_piecewise_from_hdf(hFilePath, patchPerc=patchPerc, \n",
    "                                           patchOverlapPerc=patchOverlapPerc)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Resume processing/analysis from here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Start by seeing if the data in the HDF is accessible*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hFilePath = glob.glob(os.path.join(dir_fish, 'procData*.h5'))[-1] #Load the latest file\n",
    "print(hFilePath)\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    print(hFile.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load a slice from the raw and registered stacks and play as movie to check registration looks ok!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "iSlc = 14 # Slice index (0-29; useless top slice discarded during processing)\n",
    "fr=10 # Movie frame rate\n",
    "\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    slc_raw = np.array(hFile['ca_raw'][iSlc])\n",
    "    slc_reg = np.array(hFile['ca_reg'][iSlc])\n",
    "a = spt.zscore(slc_raw[:,::2, ::2])\n",
    "b = spt.zscore(slc_reg[:, ::2, ::2]) \n",
    "mov = cm.movie(np.concatenate((a, b), axis=1), fr=fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Play the movie\n",
    "mov.play(magnification=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Optionally, save registered slices as tif files for more careful examination with ImageJ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save each of the registered slices in a separate .tif file\n",
    "dir_save = os.path.join(dir_fish, f'registered_slices_cpwr_{util.timestamp()}')\n",
    "if not os.path.exists(dir_save):\n",
    "    os.mkdir(dir_save) \n",
    "\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    for iSlc, slc in enumerate(hFile['ca_reg']):\n",
    "        slc_int = slc.astype('uint16')\n",
    "        tff.imsave(os.path.join(dir_save, r'slice_{:03d}.tif'.format(iSlc)), slc_int)\n",
    "print(f'Saved at\\n{dir_save}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *If ROIs have not already been drawn, create temporally averaged $Ca^{2+}$ image volume on which ROIs can be drawn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ca_reg_avg = []\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    for z in range(hFile['ca_reg'].shape[0]):\n",
    "        ca_reg_avg.append(np.array(hFile['ca_reg'][z]).mean(axis=0))\n",
    "ca_reg_avg = np.array(ca_reg_avg).astype('uint16')\n",
    "tff.imsave(os.path.join(dir_fish, 'averageCaImgVol.tif'), data=ca_reg_avg)\n",
    "print(f'Saved at\\n{dir_fish}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *After ROIs have been drawn on the time averaged slices, read the ROIs and exract timeseries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hFilePath= glob.glob(dir_fish + '/procData*.h5')[-1]\n",
    "dir_rois = glob.glob(dir_fish + '/RoiSet*.zip')[-1]\n",
    "\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    stackDims = hFile['ca_reg'].shape\n",
    "imgDims = stackDims[-2:]\n",
    "volDims = (stackDims[0], *imgDims)\n",
    "\n",
    "rois = mlearn.readImageJRois(dir_rois, imgDims, multiLevel=False)[1]\n",
    "masks, roiNames = hf.consolidate_rois(rois, volDims)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *A quick glance at z-projected ROIs in the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(('fivethirtyeight', 'seaborn-talk', 'seaborn-ticks'))\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(np.zeros(masks.shape[-2:]), cmap='gray')\n",
    "for iMask, mask in enumerate(masks):\n",
    "    img = mask.max(axis=0)\n",
    "    inds = np.where(img)    \n",
    "    plt.scatter(inds[1], inds[0], c=np.array(plt.cm.tab20(iMask))[None,], alpha=0.2)\n",
    "    plt.scatter(inds[1][0], inds[0][0], c=np.array(plt.cm.tab20(iMask))[None,], label=f'{roiNames[iMask]}')\n",
    "plt.grid('')\n",
    "leg =plt.legend(fontsize=24, ncol=2, framealpha=0)\n",
    "for txt in leg.get_texts():\n",
    "    plt.setp(txt, color='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Extract timeseries for ROIs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "key='/ca_reg'\n",
    "\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    ca_reg = np.array(hFile[key])\n",
    "    trlLen = np.array(hFile['nImgsInTrl_ca'])[0]\n",
    "    trlIdx = np.array(hFile['trlIdx_ca'])\n",
    "    stimLoc = util.to_utf(np.array(hFile['stimLoc']))\n",
    "    sessionIdx = np.array(hFile['sessionIdx'])\n",
    "ca_reg = ca_reg.swapaxes(0, 1)    \n",
    "arr = darr.from_array(ca_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def func_now(imgs, mask):\n",
    "    nPxls = mask[np.where(mask==1)].sum()\n",
    "    ts = np.apply_over_axes(np.sum, imgs*mask[None, ...], [1, 2, 3]).flatten()\n",
    "    ts = ts/nPxls\n",
    "    return ts\n",
    "\n",
    "roi_ts = []\n",
    "print('Extracting roi timeseries...')\n",
    "for iMask, mask in enumerate(masks):\n",
    "    print(f'{roiNames[iMask]}')\n",
    "    nPxls = mask[np.where(mask==1)].sum()\n",
    "    ts = arr*mask[None,...]\n",
    "    ts = ts.sum(axis=-1).sum(axis=-1).sum(axis=-1)/nPxls\n",
    "    roi_ts.append(ts.compute())\n",
    "roi_ts = np.array(roi_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Put data in a dataframe and save*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'dataframe_roi_ts.pkl'\n",
    "roi_ts_trl = roi_ts.reshape(len(roiNames), -1, trlLen)\n",
    "trlIdx_trl = trlIdx.reshape(-1, trlLen)[:, 0]\n",
    "stimLoc_trl = stimLoc.reshape(-1, trlLen)[:, 0]\n",
    "sessionIdx_trl=sessionIdx.reshape(-1, trlLen)[:, 0]\n",
    "nTrls = len(stimLoc_trl)\n",
    "nRois = len(roiNames)\n",
    "df = {}\n",
    "df['roiName'] = np.repeat(roiNames, nTrls)\n",
    "df['trlIdx'] = np.tile(trlIdx_trl, nRois)\n",
    "df['sessionIdx'] = np.tile(sessionIdx_trl, nRois)\n",
    "df['stimLoc'] = np.tile(stimLoc_trl, nRois)\n",
    "df['ts'] = list(roi_ts_trl.reshape(-1, trlLen))\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "df.to_pickle(os.path.join(dir_fish, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot trial averaged $Ca^{2+}$ responses for head and tail stimulation trials for all ROIs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCols=3\n",
    "Fs_ca = 2 # Frame rate\n",
    "nPre=3\n",
    "nRows = int(np.ceil(nRois/nCols))\n",
    "\n",
    "fh, ax = plt.subplots(nRows, nCols, figsize=(20, 20*nRows//nCols), \n",
    "                      sharex=True, sharey=False)\n",
    "ax = ax.flatten()\n",
    "t = (np.arange(trlLen)-3)*(1/Fs_ca)\n",
    "for iRoi, rn in enumerate(roiNames):\n",
    "    df_sub = df.loc[df.roiName==rn]\n",
    "    ts_head = np.array([np.array(_) for _ in df_sub.loc[df_sub.stimLoc=='h'].ts])\n",
    "    ts_head = ts_head-ts_head[:, :nPre].mean(axis=1)[:, None]\n",
    "    ts_tail = np.array([np.array(_) for _ in df_sub.loc[df_sub.stimLoc=='t'].ts])\n",
    "    ts_tail = ts_tail-ts_tail[:, :nPre].mean(axis=1)[:, None]\n",
    "    boot_head = util.BootstrapStat(combSize=ts_head.shape[0], nCombs=1000, replace=True)\n",
    "    boot_tail = util.BootstrapStat(combSize=ts_head.shape[0], nCombs=1000, replace=True)\n",
    "    mu_head = ts_head.mean(axis=0)\n",
    "    ci_head = 2*np.std(boot_head.fit_transform(ts_head)[0], axis=0)#/(ts_head.shape[0]**0.5)\n",
    "    mu_tail = ts_tail.mean(axis=0)\n",
    "    ci_tail = 2*np.std(boot_head.fit_transform(ts_tail)[0], axis=0)#/(ts_tail.shape[0]**0.5)    \n",
    "    if iRoi==0:\n",
    "        ax[iRoi].fill_between(t, mu_head+ci_head, mu_head-ci_head, \n",
    "                              color=plt.cm.tab10(0), alpha=0.5, label='Head')\n",
    "        ax[iRoi].fill_between(t, mu_tail+ci_tail, mu_tail-ci_tail, \n",
    "                              color=plt.cm.tab10(1), alpha=0.5, label='Tail')\n",
    "        ax[iRoi].legend()\n",
    "    else:\n",
    "        ax[iRoi].fill_between(t, mu_head+ci_head, mu_head-ci_head, \n",
    "                              color=plt.cm.tab10(0), alpha=0.5)\n",
    "        ax[iRoi].fill_between(t, mu_tail+ci_tail, mu_tail-ci_tail, \n",
    "                              color=plt.cm.tab10(1), alpha=0.5)\n",
    "    ax[iRoi].set_title(rn, fontsize=14)\n",
    "    ax[iRoi].set_xlim(t[0], t[-1])\n",
    "ax[iRoi].set_xlabel('Time (s)')\n",
    "ax[iRoi].set_ylabel('Raw intensity')\n",
    "fh.suptitle('Trial averaged Ca2+ responses for head and tail stimulation trials', fontsize=18)\n",
    "plt.subplots_adjust(top=0.95)        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Onto behavior tracking*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *For the sake of showing how it's done, I will actually create a U-net model afresh and train on some data before segmenting fish in this dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDims = (256, 256) # Size of images to be trained on. Will rescale input images to this size before training\n",
    "loss = model.focal_loss # I've recently found that this loss function works better than the custom one\n",
    "optimizer='adam' # ('rmsprop' or 'adam')\n",
    "\n",
    "# Now load a naive U-net model object (pre-compiled)\n",
    "unet = model.get_unet(img_height=imgDims[0], img_width=imgDims[1], img_channels=1, \n",
    "                      loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *If you want to use a pre-trained net, run the cell below instead of the one above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_unet = glob.glob(dir_fish + '/trainedU_headFixed_*.h5')[-1]\n",
    "unet = mlearn.loadPreTrainedUnet(path_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *I have an excel sheet with paths to training images and corresponding masks. It has training data for different imaging conditions so we will read images from only the paths corresponding to the head fixed condition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_xls = r'\\\\Koyama-S2\\Data3\\Avinash\\Ablations and Behavior'\n",
    "path_xls = glob.glob(os.path.join(dir_xls, 'Paths_to*.xlsx'))[-1]\n",
    "xls_train = pd.read_excel(path_xls, sheet_name='Uncropped')\n",
    "xls_train = xls_train.loc[xls_train.exptType=='headFixed']\n",
    "\n",
    "changePath = lambda p: r'\\\\Koyama-S2\\Data3' + p.split(':')[-1]\n",
    "path_imgs = list(map(changePath, np.array(xls_train.pathToImages)))\n",
    "path_masks = list(map(changePath, np.array(xls_train.pathToMasks)))\n",
    "imgDims = unet.input_shape[1:3]\n",
    "imgs_train, masks_train = mlearn.read_training_images_and_masks(np.array(path_imgs), \n",
    "                                                    np.array(path_masks), imgDims=imgDims)\n",
    "masks_train = (masks_train>0).astype(int)\n",
    "print(f'Training on {imgs_train.shape[0]} imgs of dimensions {imgs_train.shape[1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Evaluate performance as a first check. If naive model, then of course score will be low, and if not then a score > 0.85 is usually good*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = unet.evaluate(imgs_train[..., None], masks_train[..., None], batch_size=32, verbose=1)\n",
    "print(np.c_[unet.metrics_names, metrics])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Run the code cell below a few times to randomly select and plot images and masks for checking purposes* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind= np.random.choice(range(imgs_train.shape[0]), size=1)[0]\n",
    "m = montage((imgs_train[ind], masks_train[ind]), rescale_intensity=True, grid_shape=(1,2))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(m, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *We will create a checkpointer callback that monitors performance during the training epochs and automatically store the weights of the best model to a file from which we can load weights for future use*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Checkpointer callback for storing best weights\n",
    "fp = os.path.join(dir_fish, f'best_weights_headFixed_{util.timestamp()}.hdf')\n",
    "checkpointer = ModelCheckpoint(filepath=fp, monitor='val_dice_coef', verbose=1,\\\n",
    "                               save_best_only=True, mode='max', save_weights_only=True)\n",
    "\n",
    "keras_callbacks = [checkpointer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Augment the training image set to include more diverse and challenging training images that can result in more robust training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Augment before training\n",
    "upSample = 5 # This will expand the training set by this much\n",
    "aug_set=('rn', 'sig', 'log', 'inv', 'heq', 'rot', 'rs')\n",
    "%time imgs_aug, masks_aug, augs = mlearn.augmentImageData(imgs_train, masks_train,\\\n",
    "                                                          upsample=upSample, aug_set=aug_set)\n",
    "\n",
    "imgs_aug = mlearn.prepare_imgs_for_unet(imgs_aug, unet)\n",
    "masks_aug = mlearn.prepare_imgs_for_unet(masks_aug, unet)\n",
    "masks_aug= (masks_aug>0).astype(int)\n",
    "print(f'Augmentation: {len(imgs_train)} --> {len(imgs_aug)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Run the cell below a few times to check the augmented images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind= np.random.choice(range(imgs_train.shape[0]), size=1)[0]\n",
    "m = montage((imgs_aug[ind][..., 0], masks_aug[ind][..., 0]), rescale_intensity=True, grid_shape=(1,2))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(m, cmap='viridis')\n",
    "plt.title(augs[ind].upper(), fontsize=30);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Now train! Make sure you have tensorflow-gpu installed in your environment. If not, then training can be rather slow. One way to know that the process is running on GPU is to look at CPU usage, which should not exceed 10%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 32 # Larger batch sizes are usually better, but reduce if you get an OOM error\n",
    "epochs = 250 # Number of training epochs\n",
    "validation_split = 0.1 # Fraction of images from the training set to be used for validation\n",
    "initial_epoch = 190 # 0, if training a naive model. Can be used to retrain from a previous epoch onwards\n",
    "\n",
    "his = unet.fit(imgs_aug, masks_aug, epochs=epochs, batch_size=batch_size,\\\n",
    "               validation_split=validation_split, callbacks=keras_callbacks, verbose=0,\n",
    "               initial_epoch=initial_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot training metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his = unet.history.history\n",
    "print(his.keys())\n",
    "plt.style.use(('seaborn-poster', 'seaborn-white'))\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(his['val_dice_coef'],'.', label='validation set')\n",
    "plt.plot(his['dice_coef'], label='training set')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Dice coefficient', fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(his['val_loss'],'.', label ='validation set')\n",
    "plt.plot(his['loss'], label = 'training set')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Loss', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load the best weights from the saved file and then save the U-net*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the best weights and save\n",
    "path_weights = glob.glob(os.path.join(dir_fish, 'best_weights_headFixed*.hdf'))[-1]\n",
    "unet.load_weights(path_weights)\n",
    "\n",
    "#%% Save the U-net\n",
    "fn = f'trainedU_headFixed_{unet.input_shape[1]}x{unet.input_shape[2]}_{util.timestamp()}.h5'\n",
    "unet.save(os.path.join(dir_fish, fn))\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Evaluate performance on both training and validation set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = unet.evaluate(imgs_aug, masks_aug, batch_size=32, verbose=1)\n",
    "print(np.c_[unet.metrics_names, metrics])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Walk through the directory tree and get all the subdirectories with behavior images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Get the paths to all the behavior directories\n",
    "roots, dirs, files = zip(*[out for out in os.walk(dir_fish)])\n",
    "inds = util.findStrInList('Autosave', roots)\n",
    "dirs_behav = np.array(roots)[inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Check segmentation on a consecutive set of images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iTrl = 10\n",
    "frameRange = (200, 1000)\n",
    "\n",
    "imgNames = ft.findAndSortFilesInDir(dirs_behav[iTrl], ext='bmp')[range(*frameRange)]\n",
    "\n",
    "imgs = volt.img.readImagesInDir(dirs_behav[iTrl], imgNames=imgNames)\n",
    "imgs_rs = volt.img.resize(imgs, unet.input_shape[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Predict on loaded images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_prob = np.squeeze(unet.predict(imgs_rs[..., None], batch_size=32, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Make a movie* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "merge_ch = 0\n",
    "fps = 50\n",
    "\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "imgs_prob_255 = (imgs_prob*255).astype(int)\n",
    "imgs_rs_rgb = np.array([gray2rgb(_, alpha=0.5) for _ in imgs_rs])\n",
    "\n",
    "imgs_rs_rgb[..., merge_ch] = (alpha*imgs_rs_rgb[..., merge_ch] + (1-alpha)*imgs_prob_255).astype(int) \n",
    "ani =volt.animate_images(imgs_rs_rgb, fps=fps, fig_size=(15, 15))\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *If movie looks bad, then select a few images where tracking failed and re-train the U-net, or else segment fish in all behavior images, compute tail angles, and write to the saved HDF file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time hFilePath = hf.extractAndStoreBehaviorData_singleFish(dir_fish, uNet=unet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Resume from here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Read tail angles from HDF file, clean with svd and wavelet, and trialize*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hFilePath = glob.glob(os.path.join(dir_fish, 'procData*.h5'))[-1]\n",
    "path_df = glob.glob(os.path.join(dir_fish, 'dataframe_roi_ts*.pkl'))[-1]\n",
    "df = pd.read_pickle(path_df)\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    stimLoc_behav = util.to_utf(np.array(hFile['behav/stimLoc']))\n",
    "    ta = np.array(hFile['behav/tailAngles'])\n",
    "nTrls = ta.shape[0]//50\n",
    "ta_ser = np.concatenate(np.vsplit(ta, nTrls), axis=1)\n",
    "%time ta_clean, _, svd = hf.cleanTailAngles(ta_ser, dt=1/500)\n",
    "ta_trl = np.array(np.hsplit(ta_clean, nTrls))\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Match $Ca^{2+}$ and behavior trials, put them all in on dataframe and save* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy()\n",
    "sessionIdx, stimLoc = zip(*[(int(trl[:3]), trl[-1]) for trl in stimLoc_behav])\n",
    "sessionIdx, stimLoc = np.array(sessionIdx), np.array(stimLoc)\n",
    "if sessionIdx.min()==1:\n",
    "    sessionIdx = sessionIdx-1\n",
    "trlIdx =[]\n",
    "for sid in np.unique(sessionIdx):\n",
    "    n = len(np.where(sessionIdx==sid)[0])\n",
    "    trlIdx.extend(np.arange(n))\n",
    "trlIdx = np.array(trlIdx)\n",
    "df_ = dict(sessionIdx=sessionIdx, stimLoc=stimLoc, trlIdx=trlIdx, tailAngles=list(ta_trl))\n",
    "df_ = pd.DataFrame(df_)\n",
    "df = pd.merge(df, df_, on = ['stimLoc', 'sessionIdx', 'trlIdx'])\n",
    "df.to_pickle(os.path.join(dir_fish, f'dataframe_roi_ta_{util.timestamp()}.pkl'))\n",
    "print(f'Saved dataframe at\\n{dir_fish}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iTrl=44\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(ta_trl[iTrl][-1])\n",
    "plt.xlim(300, 1500)\n",
    "plt.axvline(500, ls='--', c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
