{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import caiman as cm\n",
    "import h5py\n",
    "# from skimage.external import tifffile as tff\n",
    "from sklearn.decomposition import PCA\n",
    "import tifffile as tff\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "codeDir = r'V:/code/python/code'\n",
    "sys.path.append(codeDir)\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "import apCode.geom as geom\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "from apCode import util as util\n",
    "from apCode import hdf\n",
    "from apCode.imageAnalysis.spim import regress\n",
    "from apCode.behavior import gmm as my_gmm\n",
    "from apCode.machineLearning.preprocessing import Scaler\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducability\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_xls = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging'\n",
    "file_xls = 'GCaMP volumetric imaging summary.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read xl file\n",
    "idx_fish = 8\n",
    "xls = pd.read_excel(os.path.join(dir_xls, file_xls), sheet_name='Sheet1')\n",
    "path_now = np.array(xls.loc[xls.FishIdx == idx_fish].Path)[0]\n",
    "print(path_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue from the saved dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "startFresh = True # Reads hFile and df\n",
    "\n",
    "if (startFresh) & (('hFilePath' in locals()) | ('df' in locals())):\n",
    "    del hFilePath,  df\n",
    "\n",
    "#%% If stored dataframe exists in path read it\n",
    "if 'hFilePath' not in locals():\n",
    "    hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str = 'procData')[-1]\n",
    "    hFilePath = os.path.join(path_now, hFileName)\n",
    "with h5py.File(hFilePath, mode = 'r') as hFile:\n",
    "    print(hFile.keys())\n",
    "\n",
    "if 'df' not in locals():\n",
    "    file_df = ft.findAndSortFilesInDir(path_now, ext = 'pickle', search_str = 'dataFrame')\n",
    "    if len(file_df)>0:\n",
    "        file_df = file_df[-1]\n",
    "        path_df = os.path.join(path_now, file_df)\n",
    "        print(path_df)\n",
    "        print('Reading dataframe...')\n",
    "        %time df = pd.read_pickle(path_df)       \n",
    "    else:\n",
    "        print('No dataframe found in path!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Extract useful info\n",
    "ta_trl = np.array([np.array(_) for _ in df.tailAngles])\n",
    "ta = np.concatenate(ta_trl,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Can I get a clearer and crisper image volume than the offset map from registration to draw ROIs on?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with h5py.File(hFilePath, mode='r') as hFile:\n",
    "    ca = hFile['ca_trls_reg'][()]\n",
    "\n",
    "nTrls = ca.shape[0]\n",
    "trlLen = ca.shape[1]\n",
    "volDims = ca.shape[-3:]  \n",
    "ca = ca.reshape(-1, *volDims)\n",
    "ca = ca[:, 1:]\n",
    "ca_avg = ca.mean(axis=0)\n",
    "ca_avg = ca_avg - ca_avg.min() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Save average stack for future reference\n",
    "tff.imsave(os.path.join(path_now, 'averageCaImgVol.tif'), data=ca_avg.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make simple Ca$^{2+}$ response maps to distinguish head and tail-elicited responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the group data trained GMM model to predict posture labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the trained model\n",
    "dir_group = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging\\Group'\n",
    "file_model = 'gmm_svd-3_env_pca-9_gmm-20_20200129-18.pkl' \n",
    "gmm_model = joblib.load(os.path.join(dir_group, file_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Predict labels using the loaded and pre-trained GMM model\n",
    "\n",
    "subSample = 1\n",
    "alpha = 0.5\n",
    "cmap = plt.cm.tab20\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%time labels, features =  gmm_model.predict(ta)\n",
    "\n",
    "%time x_pca = PCA(n_components = 3, random_state = 143).fit_transform(features)\n",
    "\n",
    "fh,ax = plt.subplots(2,2,figsize = (15,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "\n",
    "# clrs = cmap(spt.standardize(labels))\n",
    "scaler_clrs = Scaler(standardize =True).fit(np.arange(gmm_model.n_gmm_))\n",
    "clrs = cmap(scaler_clrs.transform(labels))\n",
    "\n",
    "ax[0].scatter(x_pca[:,0], x_pca[:,1], s = 10, c = clrs, alpha = alpha)\n",
    "ax[0].set_xlabel('pca 1')\n",
    "ax[0].set_ylabel('pca 2')\n",
    "\n",
    "ax[1].scatter(x_pca[:,0], x_pca[:,2], s = 10, c = clrs, alpha = alpha)\n",
    "ax[1].set_xlabel('pca 1')\n",
    "ax[1].set_ylabel('pca 3')\n",
    "\n",
    "ax[2].scatter(x_pca[:,1], x_pca[:,2], s = 10, c = clrs, alpha = alpha)\n",
    "ax[2].set_xlabel('pca 2')\n",
    "ax[2].set_ylabel('pca 3')\n",
    "fh.tight_layout()\n",
    "\n",
    "x = np.unique(labels)\n",
    "y = np.ones_like(x)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.scatter(x,y, c = cmap(scaler_clrs.transform(x)),s = 4000, marker = 's')\n",
    "plt.yticks([])\n",
    "plt.xticks(x, fontsize = 20);\n",
    "plt.title('Norm-ordered colors', fontsize = 20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check ouf a few trials with predictions from the GMM\n",
    "\n",
    "# iTrl = 6  # (Struggles = {9, 11}\n",
    "iTrl = np.random.choice(np.arange(df.shape[0]),size = 1)[0]\n",
    "yShift = 1.1\n",
    "loop = False\n",
    "xl = (-0.1, 1)\n",
    "# xl = 'auto'\n",
    "figDir = os.path.join(path_now, 'figs/behav_trls_colored_by_gmm_label')\n",
    "onOffThr = 0\n",
    "figExts = ('png','pdf')\n",
    "cmap = plt.cm.tab20\n",
    "dt_behav = 1/500\n",
    "\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "from IPython import display\n",
    "if loop:\n",
    "    trls = np.unique(df.trlNum)\n",
    "else:\n",
    "    trls = [iTrl]\n",
    "\n",
    "maxEnv_full = spt.emd.envelopesAndImf(ta[-1])['env']['max']    \n",
    "for iTrl in trls:\n",
    "    ta_now = ta_trl[iTrl]\n",
    "    trlLen = ta_now.shape[-1]\n",
    "    inds = np.arange(iTrl*trlLen, (iTrl+1)*trlLen)\n",
    "    maxEnv = maxEnv_full[inds]\n",
    "    posInds = np.linspace(0, len(ta_now)-1,4).astype(int)\n",
    "    z = np.diff(ta_now[posInds],axis = 0)\n",
    "    ys = util.yOffMat(z)*yShift\n",
    "    z = z - ys\n",
    "    y = ta_now[-1]\n",
    "    x = (np.arange(len(y))*dt_behav) - 1   \n",
    "    lbls_now, x_now = gmm_model.predict(ta_now)\n",
    "    lbls_norm = scaler_clrs.transform(lbls_now)\n",
    "    clrs = cmap(lbls_norm)\n",
    "    zerInds = np.where(maxEnv <=onOffThr)[0]\n",
    "    clrs[zerInds,:] = 1 # Make this invisible\n",
    "    # clrs[:,-1] = 0.8 # Alpha value of points\n",
    "    fh = plt.figure(figsize = (20,8));    \n",
    "    # plt.scatter(x,y,c = clrs,s= 15)\n",
    "    for iLine, z_ in enumerate(z):\n",
    "        plt.plot(x,z_, c = 'k', alpha = 0.2, label = f'loc = {iLine}')\n",
    "        plt.scatter(x,z_, c = clrs, s = 15)\n",
    "        plt.legend(loc = 'upper right', fontsize= 15)\n",
    "    plt.xlabel('Time (s)', fontsize = 20)\n",
    "    plt.ylabel('Total tail bending for segment ($^o$)', fontsize = 20)\n",
    "    maxInd = []\n",
    "    if np.any(xl == 'auto'):\n",
    "        nonZerInds = np.setdiff1d(np.arange(len(y)),zerInds)        \n",
    "        if len(nonZerInds)>0:\n",
    "            maxInd = np.max(nonZerInds)\n",
    "        else:\n",
    "            maxInd = len(x)-31\n",
    "        plt.xlim(-0.1,x[maxInd])\n",
    "    else:\n",
    "        plt.xlim(xl)\n",
    "    yl = np.min((-100, z.min())), np.max((80, z.max()))\n",
    "    plt.ylim(yl)\n",
    "    plt.yticks([0,-100])\n",
    "    plt.title(f'Total tail curvature with points colored by cluster label (trl = {iTrl})', fontsize = 20)\n",
    "    plt.show()\n",
    "    for fe in figExts:\n",
    "        fn = f'Fig-{util.timestamp()}_Total tail curvature timeseries with colored clustered points_trl-{iTrl}.{fe}'\n",
    "#         fh.savefig(os.path.join(figDir, fn), format = f'{fe}', dpi = 'figure')\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check ouf a few trials with predictions from the GMM with annotated markers\n",
    "\n",
    "# iTrl = 10  # (Struggles = {9, 11}\n",
    "iTrl = np.random.choice(np.arange(df.shape[0]),size = 1)[0]\n",
    "yShift = 1.1\n",
    "loop = False\n",
    "xl = (-0.1, 2)\n",
    "# xl = 'auto'\n",
    "figDir = os.path.join(path_now, 'figs/behav_trls_colored_by_gmm_label')\n",
    "onOffThr = 0\n",
    "figExts = ('png','pdf')\n",
    "cmap = plt.cm.tab20\n",
    "dt_behav = 1/500\n",
    "figSize = (100,10)\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "stimLoc = np.array(df.stimLoc)\n",
    "\n",
    "trls = [iTrl]    \n",
    "maxEnv_full = spt.emd.envelopesAndImf(ta[-1])['env']['max']    \n",
    "for iTrl in trls:   \n",
    "    ta_now = ta_trl[iTrl]\n",
    "    fh = my_gmm._plot_with_labels(gmm_model, ta_now, marker_size=200, figSize=figSize)\n",
    "    plt.xlabel('Frame #', fontsize = 20)\n",
    "    plt.ylabel('Total tail bend amplitude ($^o$)', fontsize = 20)\n",
    "    maxInd = []    \n",
    "    yl = (-100, 100)\n",
    "    plt.xlim(0, ta_now.shape[1])\n",
    "    plt.ylim(yl)\n",
    "    plt.yticks([0,-100])\n",
    "    plt.title(f'Total tail curvature with points colored by cluster label (trl = {iTrl}, stim = {stimLoc[iTrl].upper()})', fontsize = 20)\n",
    "    for fe in figExts:\n",
    "        fn = f'Fig-{util.timestamp()}_Total tail curvature timeseries with colored clustered points_trl-{iTrl}.{fe}'\n",
    "#         fh.savefig(os.path.join(figDir, fn), format = f'{fe}', dpi = 'figure')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check ouf a few trials with predictions from the GMM with annotated markers\n",
    "\n",
    "# iTrl = 10  # (Struggles = {9, 11}\n",
    "# iTrl = np.random.choice(np.arange(df.shape[0]),size = 1)[0]\n",
    "yShift = 1.1\n",
    "loop = False\n",
    "xl = (-0.1, 2)\n",
    "# xl = 'auto'\n",
    "figDir = os.path.join(path_now, 'figs/behav_trls_colored_by_gmm_label')\n",
    "onOffThr = 0\n",
    "figExts = ('png','pdf')\n",
    "cmap = plt.cm.tab20\n",
    "dt_behav = 1/500\n",
    "pre_behav = 500\n",
    "figSize = (100,10)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "    \n",
    "scaler_clrs = Scaler(standardize =True).fit(np.arange(gmm_model.n_gmm_))   \n",
    "stimLoc = np.array(df.stimLoc)\n",
    "\n",
    "trls = np.arange(ta_trl.shape[0])    \n",
    "maxEnv_full = spt.emd.envelopesAndImf(ta[-1])['env']['max']\n",
    "# trls = [1]\n",
    "for iTrl in trls:\n",
    "    ta_now = ta_trl[iTrl]\n",
    "    x = (np.arange(ta_now.shape[1])-500)*dt_behav\n",
    "    y = ta_now[-1]\n",
    "    lbls, _ = gmm_model.predict(ta_now)\n",
    "    line = go.Scatter(x=x, y = y, mode = 'lines', opacity = 0.2, marker = dict(color = 'black'), name = 'ta')\n",
    "    scatters = []\n",
    "    scatters.append(line)\n",
    "    for iLbl, lbl in enumerate(np.unique(lbls)):\n",
    "        clr = f'rgba{cmap(lbl)}'\n",
    "        inds= np.where(lbls==lbl)[0]\n",
    "        scatter = go.Scatter(x=x[inds], y=y[inds], mode='markers', marker = dict(color = clr, symbol = lbl, size = 10), name = f'Lbl-{lbl}')\n",
    "        scatters.append(scatter)\n",
    "    fig = go.Figure(scatters)\n",
    "    fig.update_layout(title = f'Tail angles with GMM labels, trl = {iTrl}, stim = {(stimLoc[iTrl]).upper()}')\n",
    "#     fig.show()\n",
    "    figName = f'Fig-{util.timestamp()}_trl-{iTrl}.html'\n",
    "    fig.write_html(os.path.join(figDir,figName))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot GMM labels --> Markers map\n",
    "x = np.arange(gmm_model.n_gmm_)\n",
    "y = np.ones_like(x)\n",
    "# data = pd.DataFrame(dict(gmm_label = x, y = y))\n",
    "# px.scatter(data, x= 'gmm_label', y = y, symbol_sequence=x)\n",
    "scatters = []\n",
    "for ind, x_ in enumerate(x):\n",
    "    clr = f'rgba{cmap(x_)}'\n",
    "    scatter = go.Scatter(x= [x_], y = [y[ind]], mode = 'markers',\n",
    "                         marker = dict(symbol = x_, size = 20, color = clr), name = f'Lbl-{x_}')\n",
    "    scatters.append(scatter)\n",
    "fig = go.Figure(scatters)\n",
    "fig.update_xaxes(tickvals = x)\n",
    "fig.update_yaxes(tickvals = [])\n",
    "fig.update_layout(title='Symbol map: GMM labels to markers and colors', xaxis_title=\"GMM label\")\n",
    "fig.show()\n",
    "figName = f'Fig-{util.timestamp()}_scatterPlotMarkerGmmLabelLegend.html'\n",
    "fig.write_html(os.path.join(figDir,figName), auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superSample(t, y, tt):\n",
    "    \"\"\"Super sample a signal using interpolation\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.interpolate import interp1d\n",
    "    t = np.concatenate((tt[0].reshape((-1,)), t, tt[-1].reshape((-1,))))\n",
    "    y = np.concatenate((np.array(0).reshape((-1,)), y, np.array(0).reshape((-1,))))\n",
    "    f = interp1d(t,y,kind = 'slinear')\n",
    "    return f(tt)\n",
    "\n",
    "def padIr(ir_trl, pad_pre, pad_post):\n",
    "    \"\"\"\n",
    "    Pads the impulse response timeseries obtained from \n",
    "    predictions on behavioral feature matrix to match\n",
    "    time length with ca responses\n",
    "    \"\"\"\n",
    "    ir_ser = []\n",
    "    for c in ir_trl:\n",
    "        ir_ser.append(np.pad(c,((0,0),(pad_pre, pad_post))).flatten())\n",
    "    return np.array(ir_ser)\n",
    "\n",
    "def serializeHyperstack(vol):\n",
    "    \"\"\"\n",
    "    Given, a hyperstack, returns a 2D array with pixels serialized for regression, etc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vol: array, (nTimePoints, nSlices, nRows, nCols)\n",
    "    Returns\n",
    "    -------\n",
    "    vol_ser: array, (nTimePoints,nPixels)\n",
    "    \"\"\"\n",
    "    vol_trans = np.transpose(vol,(2,3,1,0))\n",
    "    vol_ser = vol_trans.reshape(-1, vol_trans.shape[-1])\n",
    "    vol_ser = np.swapaxes(vol_ser,0,1)\n",
    "    return vol_ser\n",
    "\n",
    "def deserializeToHyperstack(arr, volDims):\n",
    "    \"\"\"\n",
    "    Given an array which \n",
    "    \"\"\"\n",
    "    volDims = (np.array(volDims))[[1,2,0]]\n",
    "    vol = arr.reshape(arr.shape[0],*volDims)\n",
    "    vol = np.transpose(vol,(0,3,1,2))\n",
    "    return vol\n",
    "\n",
    "def pxlsToVol(pxls, volDims):\n",
    "    \"\"\"\n",
    "    Given an array which \n",
    "    \"\"\"\n",
    "    volDims = (np.array(volDims))[[1,2,0]]\n",
    "    vol = pxls.reshape(*volDims)\n",
    "    vol = np.transpose(vol,(2,0,1))\n",
    "    return vol\n",
    "\n",
    "def superSample_arr(t, arr, tt, n_jobs = 32):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: array, (nSignals, nTimePoints)\n",
    "    \"\"\"    \n",
    "    from joblib import Parallel, delayed\n",
    "    n_jobs = np.min((32, os.cpu_count()))\n",
    "#     from dask import delayed, compute\n",
    "#     arr_sup = compute(*[delayed(superSample)(t,y,tt) for y in arr], scheduler = 'processes')\n",
    "    arr_sup = Parallel(n_jobs=n_jobs,verbose=1)(delayed(superSample)(t, y, tt) for y in arr)\n",
    "    return np.array(arr_sup)\n",
    "\n",
    "def betasToVol(betas, volDims):\n",
    "    if np.ndim(betas)<2:\n",
    "        betas = betas[:,np.newaxis]  \n",
    "    nReg = betas.shape[1]\n",
    "    B = betas.T.reshape(nReg, *volDims)\n",
    "    return np.squeeze(B)\n",
    "\n",
    "def head_tail_impulse_trains(df, n_pre=500, thr=5, modulate_by_amp=True):\n",
    "    \"\"\" Return trialized impulse trains for regression along with the names\n",
    "    of the regressors. Not GMM-label based, but rather head and tail stim-based.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "        Must have the columns, 'tailAngles', 'stimLoc', 'sessionIdx'\n",
    "    n_pre: int\n",
    "        Number of pre-stimulus time points in each trial\n",
    "    thr: scalar\n",
    "        Tail angles threshold for determining period of motor activity\n",
    "    modulate_by_amp: bool\n",
    "        If True, head and tail regressors are modulated by amplitude\n",
    "    \"\"\"\n",
    "    ta_trl = np.array([np.array(_) for _ in df.tailAngles])\n",
    "    ta_trl = np.squeeze(ta_trl[:, -1, :])\n",
    "#     ta = np.concatenate(ta_trl, axis=1)\n",
    "    stimLoc = np.array(df.stimLoc)\n",
    "    sessionIdx = np.array(df.sessionIdx)\n",
    "    names_ir = ['head_stim', 'tail_stim', 'head_motor', 'tail_motor', 'session_idx', 'trl_num']\n",
    "    ir_trl = []\n",
    "    for iTrl, trl in enumerate(ta_trl):\n",
    "        trl = trl-trl[0]\n",
    "        ir_now = np.zeros((6, len(trl)))\n",
    "        stim_now = stimLoc[iTrl]\n",
    "        session_now = sessionIdx[iTrl]\n",
    "        inds_supra = np.where(np.abs(trl)>=thr)[0]\n",
    "        if stim_now.lower() == 'h':\n",
    "            ir_now[0, n_pre-1] = 1\n",
    "            if modulate_by_amp:\n",
    "                ir_now[2, inds_supra] = np.abs(trl[inds_supra])\n",
    "            else:\n",
    "                ir_now[2, inds_supra] = 1            \n",
    "        else:\n",
    "            ir_now[1, n_pre-1] = 1\n",
    "            if modulate_by_amp:\n",
    "                ir_now[3, inds_supra] = np.abs(trl[inds_supra])\n",
    "            else:\n",
    "                ir_now[3, inds_supra] = 1\n",
    "        ir_now[4] = session_now\n",
    "        ir_now[5] = iTrl\n",
    "        ir_trl.append(ir_now)        \n",
    "    return np.array(ir_trl), names_ir   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Predict labels on full time series, match lengths of behavior and ca trials, and make full set of impulse \n",
    "### trains and other regressors\n",
    "n_pre_behav = 500\n",
    "thr_amp = 5\n",
    "modulate_by_amp = True\n",
    "\n",
    "\n",
    "%time ir_trl, names_ir = head_tail_impulse_trains(df, n_pre=n_pre_behav, thr=thr_amp,\\\n",
    "                                                  modulate_by_amp=modulate_by_amp)\n",
    "ir_ser = np.concatenate(ir_trl, axis=1)\n",
    "ir_ser_norm = spt.standardize(ir_ser, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Display impulse trains & other regressors\n",
    "Fs_behav = 500 # Sampling frequency of behavor\n",
    "\n",
    "getStimName = lambda s: 'Head' if s == 'h' else 'Tail'\n",
    "t_full = np.arange(ir_ser_norm.shape[-1])*(1/Fs_behav)\n",
    "yOff = util.yOffMat(ir_ser_norm)\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.plot(t_full, (ir_ser_norm-yOff).T);\n",
    "regNames = names_ir.copy()\n",
    "\n",
    "yt = -np.arange(ir_ser_norm.shape[0])\n",
    "plt.yticks(yt, regNames)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Impulse responses & other regressors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% CIRF in slightly subSampled behavAndScan time, followed by convolution to generate regressors\n",
    "tLen = 6 # Length of kernel\n",
    "tau_rise = 0.2 # Rise constant\n",
    "tau_decay = 1 # Decay constant\n",
    "tPeriStim_ca = (-1, 10)\n",
    "tPeriStim_behav = (-1, 6)\n",
    "\n",
    "dt_behav = 1/Fs_behav\n",
    "\n",
    "my_conv = lambda y, cirf, mode: np.convolve(y, cirf, mode = mode)[:len(y)]\n",
    "\n",
    "### CIRF\n",
    "t_cirf = np.arange(0,tLen,dt_behav)\n",
    "cirf = spt.generateEPSP(t_cirf,tau_rise, tau_decay,1,0)\n",
    "\n",
    "ind_conv_upto = util.findStrInList('session_idx', regNames)[0]\n",
    "\n",
    "print('Convolving with CIRF...')\n",
    "regressors = []\n",
    "for iTrl, trl in enumerate(ir_trl):\n",
    "    reg_trl = []\n",
    "    for reg_ in trl[:ind_conv_upto]:\n",
    "        reg_trl.append(dask.delayed(my_conv)(reg_, cirf, 'full'))\n",
    "    regressors.append(reg_trl)\n",
    "regressors = dask.compute(*regressors)\n",
    "regressors = np.concatenate(regressors,axis = 1)   \n",
    "regressors = np.r_[regressors, ir_ser[ind_conv_upto:]]\n",
    "regressors = spt.zscore(regressors,axis=1)\n",
    "\n",
    "print('Reading Ca2+ trials from dataframe')\n",
    "%time ca_trl = np.array([np.array(_) for _ in np.array(df['ca'])])\n",
    "\n",
    "t_ca = np.linspace(*tPeriStim_ca, ca_trl.shape[1])\n",
    "t_behav = np.linspace(*tPeriStim_behav, ir_trl.shape[-1])\n",
    "ind_max = np.where(t_ca>=t_behav[-1])[0][0]\n",
    "ca_trl_sub = ca_trl[:,:ind_max,...]\n",
    "\n",
    "### Re-assign to dataframe\n",
    "df = df.assign(ca = list(ca_trl_sub))\n",
    "df.to_pickle(os.path.join(path_now, file_df))\n",
    "\n",
    "t_behav = np.linspace(0, 1, regressors.shape[1])\n",
    "t_ca = np.linspace(0, 1, ca_trl_sub.shape[0]*ca_trl_sub.shape[1])\n",
    "\n",
    "print('Super sampling...')\n",
    "%time regressors = superSample_arr(t_behav, regressors, t_ca)\n",
    "\n",
    "if 'hFilePath' not in locals():\n",
    "    hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "    hFilePath = os.path.join(path_now, hFileName)\n",
    "\n",
    "print('Saving variables to HDF file')    \n",
    "with h5py.File(hFilePath, mode = 'r+') as hFile:\n",
    "    if 'regression' in hFile:\n",
    "        del hFile['regression']\n",
    "    grp = hFile.create_group('regression')   \n",
    "    grp.create_dataset('regressors', data = regressors.T)\n",
    "    grp.create_dataset('regressor_names', data = util.to_ascii(regNames))\n",
    "    grp.create_dataset('impulse_trains', data = ir_ser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%% Serialize Ca2+ volumes into pixel timeseries for regression\n",
    "# print('Serializing Ca2+ data for regression')\n",
    "# %time ca_ser = serializeHyperstack(np.concatenate(ca_trl_sub,axis = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot all regressors\n",
    "nTrls = ca_trl_sub.shape[0]\n",
    "nTimePts = ca_trl_sub.shape[0]*ca_trl_sub.shape[1]\n",
    "t_ca = np.linspace(0, (tPeriStim_behav[-1]-tPeriStim_behav[0])*nTrls, nTimePts)\n",
    "yOff = util.yOffMat(regressors)\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.plot(t_ca,(regressors-yOff).T)\n",
    "plt.xlim(t_ca.min(), t_ca.max())\n",
    "plt.yticks(-yOff, names_ir)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Regressors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read saved dataframe if continuing from here\n",
    "file_df = ft.findAndSortFilesInDir(path_now, ext = 'pickle', search_str='dataFrame')[-1]\n",
    "%time df = pd.read_pickle(os.path.join(path_now,file_df))\n",
    "%time ca_trl = np.array([np.array(_) for _ in np.array(df['ca'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Denoise and filter images before regression\n",
    "filtSize = 1\n",
    "n_comps = 50\n",
    "\n",
    "\n",
    "def den_flt(slc, n_comps = 50, filtSize = 1):\n",
    "    if n_comps is None:\n",
    "        slc_den = slc\n",
    "    else:\n",
    "        slc_den = volt.denoise_ipca(slc, components=n_comps)\n",
    "    slc_flt = volt.img.gaussFilt(slc_den, sigma=filtSize)\n",
    "    return slc_flt\n",
    "\n",
    "\n",
    "images_reg_ser = ca_trl.reshape(-1, *ca_trl.shape[2:])\n",
    "images_reg_ipca_flt = []\n",
    "for iSlc, slc in enumerate(np.swapaxes(images_reg_ser,0,1)):\n",
    "    print(f'{iSlc + 1}/{images_reg_ser.shape[1]}')\n",
    "    slc_flt = den_flt(slc, n_comps=n_comps, filtSize=filtSize)\n",
    "    images_reg_ipca_flt.append(slc_flt)\n",
    "\n",
    "images_reg_ipca_flt = np.swapaxes(np.array(images_reg_ipca_flt),0,1)\n",
    "\n",
    "if 'hFilePath' not in locals():\n",
    "    hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "    hFilePath = os.path.join(path_now, hFileName)\n",
    "    \n",
    "with h5py.File(hFilePath, mode = 'r+') as hFile:\n",
    "    keyName = f'images_reg_ipca_flt_sigma-{int(filtSize*100)}'\n",
    "    if keyName in hFile:\n",
    "        del hFile[keyName]\n",
    "    %time hFile.create_dataset(keyName, data = images_reg_ipca_flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Read relevant variables for regression\n",
    "filtSize = 1\n",
    "\n",
    "hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "hFilePath = os.path.join(path_now, hFileName)\n",
    "with h5py.File(hFilePath, mode = 'r') as hFile:\n",
    "    print(hFile.keys())\n",
    "    images = np.array(hFile[f'images_reg_ipca_flt_sigma-{int(filtSize*100)}'])\n",
    "    X_reg = np.array(hFile['regression/regressors'])\n",
    "    regNames = util.to_utf(np.array(hFile['regression/regressor_names']))\n",
    "    if 'images_reg_ipca_flt' in locals():\n",
    "        del images_reg_ipca_flt\n",
    "\n",
    "### Correct for last-point edge artifact        \n",
    "foo = []\n",
    "for x in X_reg.T:\n",
    "    x[-1] = x[-2]\n",
    "    foo.append(x)\n",
    "X_reg= np.array(foo).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Regress\n",
    "def betasToVol(betas, volDims):\n",
    "    if np.ndim(betas)<2:\n",
    "        betas = betas[:,np.newaxis]  \n",
    "    nReg = betas.shape[1]\n",
    "    B = betas.T.reshape(nReg, *volDims)\n",
    "    return np.squeeze(B)\n",
    "\n",
    "print('Serializing pixels for regression...')\n",
    "ca_ser = images.reshape(images.shape[0],-1)\n",
    "%time regObj = regress(X_reg, ca_ser, n_jobs=-1, fit_intercept=True)\n",
    "\n",
    "betas_vol = betasToVol(regObj.coef_, images.shape[-3:])\n",
    "intercept_vol = betasToVol(regObj.intercept_, images.shape[-3:])\n",
    "t_vol = betasToVol(regObj.T_, images.shape[-3:])\n",
    "r_vol = betasToVol(regObj.Rsq_,images.shape[-3:])\n",
    "mse_vol = betasToVol(regObj.mse_, images.shape[-3:])\n",
    "se_vol = betasToVol(regObj.se_, images.shape[-3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iReg = 3\n",
    "q_max = 99\n",
    "q_min = 5\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "\n",
    "beta_ = betas_vol[iReg][1:]\n",
    "plt.subplot(311)\n",
    "plt.imshow(spt.stats.saturateByPerc(beta_.max(axis= 0), perc_up = q_max, perc_low = q_min))\n",
    "plt.title(regNames[iReg], fontsize = 20)\n",
    "plt.ylabel('Beta', fontsize = 20)\n",
    "plt.colorbar()\n",
    "\n",
    "se_ = se_vol[iReg][1:]\n",
    "plt.subplot(312)\n",
    "plt.imshow(spt.stats.saturateByPerc(se_.max(axis= 0), perc_up = q_max, perc_low = q_min))\n",
    "plt.title(regNames[iReg], fontsize = 20)\n",
    "plt.colorbar()\n",
    "plt.ylabel('SE', fontsize = 20)\n",
    "\n",
    "\n",
    "t_ = t_vol[iReg][1:]\n",
    "beta_bool = volt.img.otsu(beta_, binary=True)\n",
    "# t_ = t_*beta_bool\n",
    "plt.subplot(313)\n",
    "# plt.imshow(spt.stats.saturateByPerc(t_vol[iReg+1][1:].max(axis= 0), perc_up = q_max, perc_low = q_min))\n",
    "plt.imshow(spt.stats.saturateByPerc(t_[1:].max(axis= 0), perc_up = q_max))\n",
    "plt.colorbar()\n",
    "plt.ylabel('T-value', fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save regression images\n",
    "figDir = os.path.join(path_now, f'figs/regression_ipca_flt_sigma-{int(filtSize*100)}_{util.timestamp(\"min\")}')\n",
    "t_mult = 1 # Multiply t-values by this value before converting to integer type because of low bit-depth otherwise\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "### First save coefficients\n",
    "foo = betas_vol.astype(int)\n",
    "dir_now = os.path.join(figDir, 'betas')\n",
    "if not os.path.exists(dir_now):\n",
    "    os.mkdir(dir_now)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_coef.tif'),vol[1:])\n",
    "tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor_intercept_coef.tif'),intercept_vol[1:].astype('int'))\n",
    "    \n",
    "foo = ((t_vol*t_mult).astype(int))[1:]\n",
    "dir_now = os.path.join(figDir, 'tValues')\n",
    "if not os.path.exists(dir_now):\n",
    "    os.mkdir(dir_now)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_tVals.tif'),vol[1:])\n",
    "tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor_intercept_T.tif'),foo[0])\n",
    "\n",
    "print(f'Saved at {dir_now}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read ROIs\n",
    "# dir_rois= r'Y:\\Avinash\\Head-fixed tail free\\GCaMP imaging\\2020-01-11\\f1\\figs\\regression_ipca_flt_sigma-100_20200317-0507\\betas\\RoiSet.zip'\n",
    "\n",
    "dir_rois = os.path.join(path_now, 'RoiSet2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtSize = 1\n",
    "\n",
    "hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "hFilePath = os.path.join(path_now, hFileName)\n",
    "\n",
    "if 'images' not in locals():\n",
    "    with h5py.File(hFilePath, mode = 'r') as hFile:\n",
    "        images = np.array(hFile[f'images_reg_ipca_flt_sigma-{int(filtSize*100)}'])\n",
    "\n",
    "if 'df' not in locals():\n",
    "    file_df = ft.findAndSortFilesInDir(path_now, ext = 'pickle', search_str='dataFrame')[-1]\n",
    "    %time df = pd.read_pickle(os.path.join(path_now, file_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Some functions and reading of ROIs\n",
    "\n",
    "def strip_suffices(strList):\n",
    "    strList_new = []\n",
    "    for _ in strList:\n",
    "        a, b, c = _.split('.')\n",
    "        strList_new.append(a + '.' + b)\n",
    "    return np.array(strList_new)\n",
    "\n",
    "def consolidate_rois(rois, volDims):\n",
    "    roiNames_orig = list(rois.keys())\n",
    "    roiNames = strip_suffices(roiNames_orig)\n",
    "    roiNames_unique = np.unique(roiNames)\n",
    "    masks = []\n",
    "    for rn in roiNames_unique:\n",
    "        inds = util.findStrInList(rn, roiNames)\n",
    "        mask = np.zeros(volDims)\n",
    "        for ind in inds:\n",
    "            roi_ = rois[roiNames_orig[ind]]\n",
    "            z = roi_['position']\n",
    "            mask[z] = roi_['mask']\n",
    "        masks.append(mask)\n",
    "    return np.array(masks), roiNames_unique\n",
    "\n",
    "\n",
    "imgDims = images.shape[-2:]\n",
    "volDims = images.shape[-3:]\n",
    "\n",
    "_, rois = mlearn.readImageJRois(dir_rois, imgDims, multiLevel=False)\n",
    "masks, roiNames = consolidate_rois(rois, volDims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "roi_ts = []\n",
    "for iMask, mask in enumerate(masks):\n",
    "    print(f'{iMask+1}/{masks.shape[0]}')\n",
    "    ts = np.apply_over_axes(np.mean, images*mask, [1, 2, 3]).flatten()\n",
    "    roi_ts.append(ts)\n",
    "roi_ts = np.array(roi_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_head.shape, mu_tail.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrls = df.shape[0]\n",
    "roi_ts_trls = roi_ts.reshape(roi_ts.shape[0], nTrls, -1)\n",
    "roi_ts_trls -= roi_ts_trls[...,0][...,None]\n",
    "stimLoc = np.array(df.stimLoc)\n",
    "trls_head = np.where(stimLoc=='h')[0]\n",
    "trls_tail = np.where(stimLoc=='t')[0]\n",
    "\n",
    "roi_ts_head = roi_ts_trls[:, trls_head]\n",
    "roi_ts_tail = roi_ts_trls[:, trls_tail]\n",
    "\n",
    "mu_head = roi_ts_head.mean(axis=1)\n",
    "sem_head = roi_ts_head.std(axis=1)/np.sqrt(mu_head.shape[0])\n",
    "mu_tail = roi_ts_tail.mean(axis=1)\n",
    "sem_tail = roi_ts_tail.std(axis=1)/np.sqrt(mu_tail.shape[0])\n",
    "\n",
    "# plt.figure(figsize=(20, 20*nRows/nCols))\n",
    "nCols = 3\n",
    "nRows = int(np.ceil(len(roiNames)/nCols))\n",
    "fh, ax = plt.subplots(nrows=nRows, ncols=nCols, sharex=True, figsize=(20, 20*nRows/nCols))\n",
    "ax = ax.flatten()\n",
    "fh.tight_layout()\n",
    "\n",
    "t = np.arange(mu_head.shape[1])*(1/2)\n",
    "for iRoi, roi_ in enumerate(mu_head):\n",
    "#     ax[iRoi].plot(mu_head[iRoi], c=plt.cm.tab10(0), label='Head')\n",
    "    ax[iRoi].fill_between(t, mu_head[iRoi]-sem_head[iRoi], mu_head[iRoi]+sem_head[iRoi],\n",
    "                          color=plt.cm.tab10(0), alpha=0.5, label='Head')\n",
    "    ax[iRoi].fill_between(t, mu_tail[iRoi]-sem_tail[iRoi], mu_tail[iRoi]+sem_tail[iRoi],\n",
    "                          color=plt.cm.tab10(1), alpha=0.5, label='Tail')\n",
    "#     ax[iRoi].plot(t,mu_tail[iRoi], c=plt.cm.tab10(1), label='Tail')\n",
    "    ax[iRoi].set_yticks([])\n",
    "    ax[iRoi].set_title(r'${}$'.format(roiNames[iRoi]), fontsize=20)\n",
    "    if iRoi==0:\n",
    "        ax[iRoi].legend(loc='upper left', fontsize=20)\n",
    "fh.suptitle('Average Ca$^{2+}$ response for escape trials_Head vs tail stimulation\\n R = ipi, L = contra', \\\n",
    "           fontsize=24);\n",
    "fh.subplots_adjust(top=0.955, hspace=0.12)\n",
    "\n",
    "dir_figs = os.path.join(path_now, 'figs')\n",
    "if not os.path.exists(dir_figs):\n",
    "    os.mkdir(dir_figs)\n",
    "fn = f'Fig-{util.timestamp(\"minute\")}_Trial-averaged Ca2+ responses_head and tail trials'\n",
    "fh.savefig(os.path.join(dir_figs, fn + '.pdf'), dpi='figure', format='pdf')\n",
    "fh.savefig(os.path.join(dir_figs, fn + '.png'), dpi='figure', format='png')\n",
    "print(f'Saved at \\n{dir_figs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrls = ca_trl.shape[0]\n",
    "roi_ts_trls = roi_ts.reshape(roi_ts.shape[0], nTrls, -1)\n",
    "roi_ts_trls -= roi_ts_trls[...,0][...,None]\n",
    "trls_head = np.where(stimLoc=='h')[0]\n",
    "trls_tail = np.where(stimLoc=='t')[0]\n",
    "\n",
    "roi_ts_head = roi_ts_trls[:, trls_head]\n",
    "roi_ts_tail = roi_ts_trls[:, trls_tail]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "mu_head = roi_ts_head.mean(axis=1)\n",
    "sigma_head = roi_ts_head.std(axis=1)\n",
    "mu_tail = roi_ts_tail.mean(axis=1)\n",
    "sigma_tail = roi_ts_tail.std(axis=1)\n",
    "\n",
    "\n",
    "yOff = 2*np.max(mu_head)*np.arange(roi_ts_trls.shape[0])[:, None]\n",
    "# yOff = util.yOffMat(mu_head)*2\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot((mu_head-yOff).T);\n",
    "plt.plot((mu_head+sigma_head-yOff).T, c='k', alpha=0.25);\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot((mu_tail-yOff).T);\n",
    "plt.plot((mu_tail+sigma_tail-yOff).T, c='k', alpha=0.25);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% NMF in ROI-masked areas\n",
    "masks_zProj = masks==1\n",
    "masks_zPproj = masks.max(axis=1).max(axis=0)\n",
    "images_zProj = images.mean(axis=1)\n",
    "images_zProj_mask = masks_zPproj[None, ...]*images_zProj\n",
    "mov = cm.movie(images_zProj_mask)\n",
    "mov -= mov.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nmf_space, nmf_time = mov.NonnegativeMatrixFactorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iComp = 10\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "plt.imshow(spt.standardize(nmf_space[iComp]), vmax=0.5)\n",
    "plt.subplot(212)\n",
    "plt.plot(nmf_time.T[iComp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Try CNMF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bpl\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "from caiman.utils.utils import download_demo\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "bpl.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *CNMF* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *If data is small enough use a single patch approach*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_now = images[:,1:]\n",
    "images_ser = images_now.reshape(images_now.shape[0], -1)\n",
    "nTrls = df.shape[0]\n",
    "trlLen = images_now.shape[0]/nTrls\n",
    "\n",
    "# %time regObj = regress(X_reg[:,-2:], images_ser, n_jobs=-1, fit_intercept=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(X_reg.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_now = images[:,1:]\n",
    "imgs_proj = images_now.mean(axis=1)\n",
    "mov = cm.movie(imgs_proj, fr=2)\n",
    "# mov -= mov.min()\n",
    "df_ca, baseline = mov.computeDFF()\n",
    "# df = mov.bilateral_blur_2D()\n",
    "# df = mov.copy()\n",
    "df_ca = np.array(df_ca)\n",
    "df_ca -= df_ca.min()\n",
    "print(df_ca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Save as memory mapped file\n",
    "fn_new = cm.save_memmap([df_ca], order='C', base_name='Yr9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load the file\n",
    "Yr, dims, T = cm.load_memmap(fn_new)\n",
    "images_now = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "print(images_now.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% start a cluster for parallel processing (if a cluster already exists it will be closed and a new session will be opened)\n",
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(backend='local', n_processes=None,\\\n",
    "                                                 single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Inititalize CNMF object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import bokeh.plotting as bpl\n",
    "# bpl.output_notebook()\n",
    "\n",
    "# set parameters\n",
    "fr = 2\n",
    "# K = 20  # number of neurons expected per patch\n",
    "# gSig = [2, 2]  # expected half size of neurons\n",
    "merge_thresh = 0.9  # merging threshold, max correlation allowed\n",
    "p = 2  # order of the autoregressive system\n",
    "\n",
    "gnb = 2                     # number of global background components\n",
    "rf = 45                     # half-size of the patches in pixels. e.g., if rf=25, patches are 50x50\n",
    "stride_cnmf = 10             # amount of overlap between the patches in pixels\n",
    "K = 4                       # number of components per patch\n",
    "gSig = [20, 20]               # expected half size of neurons in pixels\n",
    "method_init = 'greedy_roi'  # initialization method (if analyzing dendritic data using 'sparse_nmf')\n",
    "ssub = 1                    # spatial subsampling during initialization\n",
    "tsub = 1                    # temporal subsampling during intialization\n",
    "\n",
    "# parameters for component evaluation\n",
    "min_SNR = 2.0               # signal to noise ratio for accepting a component\n",
    "rval_thr = 1.0              # space correlation threshold for accepting a component\n",
    "\n",
    "remove_very_bad_comps = False\n",
    "cnn_thr = 0.99              # threshold for CNN based classifier\n",
    "cnn_lowest = 0.1 # neurons with cnn probability lower than this value are rejected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "cnm = cnmf.CNMF(n_processes, fr=fr, k=K, gSig=gSig, merge_thresh=merge_thresh, p=p, rf=rf, dview=dview,\\\n",
    "                min_SNR=min_SNR, rval_thr=rval_thr, remove_very_bad_comps=remove_very_bad_comps)\n",
    "\n",
    "%time cnm = cnm.fit(images_now)\n",
    "nComps = cnm.estimates.A.shape[-1]\n",
    "print(f'{nComps} components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot contours of found components\n",
    "Cn = cm.local_correlations(images_now.transpose(1,2,0))\n",
    "Cn[np.isnan(Cn)] = 0\n",
    "cnm.estimates.plot_contours(img=Cn, thr=0.8);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 23\n",
    "plt.figure(figsize=(10, 5)); \n",
    "plt.subplot(211)\n",
    "plt.imshow(np.reshape(cnm.estimates.A[:,i-1].toarray(), dims, order='F'))\n",
    "\n",
    "nmf_time = cnm.estimates.C\n",
    "plt.subplot(212)\n",
    "plt.plot(nmf_time[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "#%% RE-RUN seeded CNMF on accepted patches to refine and perform deconvolution \n",
    "%time cnm2 = cnm.refit(images_now, dview=dview)\n",
    "print(f'{cnm2.estimates.A.shape[-1]} components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the components are evaluated in three ways:\n",
    "#   a) the shape of each component must be correlated with the data\n",
    "#   b) a minimum peak SNR is required over the length of a transient\n",
    "#   c) each shape passes a CNN based classifier\n",
    "\n",
    "# cnm2.estimates.evaluate_components(images_now, cnm2.params, dview=dview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2.estimates.plot_contours(img=Cn, idx=cnm2.estimates.idx_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%% Extract DF/F values\n",
    "# cnm2 = cnm2.estimates.detrend_df_f(quantileMin=8, frames_window=250)\n",
    "# dff = cnm2.F_dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iSlc = 16\n",
    "slc = images[:,iSlc,...]\n",
    "plt.imshow(slc.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Standard NMF\n",
    "mov -= mov.min()\n",
    "nmf_space, nmf_time = mov.NonnegativeMatrixFactorization(n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iComp = 14\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(211)\n",
    "plt.imshow(nmf_space[iComp])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(nmf_time[iComp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3D version*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Rearrange dimensions to put txyz format\n",
    "images_txyz = np.transpose(images, (0, 2, 3, 1))[...,1:]\n",
    "images_txyz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Save as memory mapped file\n",
    "fn_new = cm.save_memmap([images_txyz], order='C', base_name='Yr_3d2', is_3D=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load the file\n",
    "Yr, dims, T = cm.load_memmap(fn_new)\n",
    "Y = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cn = cm.local_correlations(Y)\n",
    "plt.imshow(Cn.max(0) if len(Cn.shape) == 3 else Cn, cmap='viridis',\n",
    "           vmin=np.percentile(Cn, 70), vmax=np.percentile(Cn, 99.9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Single patch approach for small data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "K = 20  # number of neurons expected per patch\n",
    "gSig = [2, 2, 2]  # expected half size of neurons\n",
    "merge_thresh = 0.8  # merging threshold, max correlation allowed\n",
    "p = 2  # order of the autoregressive system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "cnm = cnmf.CNMF(n_processes, k=K, gSig=gSig, merge_thresh=merge_thresh, p=p, dview=dview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# %%capture\n",
    "# FIT\n",
    "images_now = np.reshape(Yr.T, [T] + list(dims), order='F')    # reshape data in Python format (T x X x Y x Z)\n",
    "cnm = cnm.fit(images_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm.estimates.nb_view_components_3d(image_type='mean', dims=dims);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Patch approach for larger datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "rf = 18  # half-size of the patches in pixels. rf=25, patches are 50x50\n",
    "stride = 10  # amounpl.it of overlap between the patches in pixels\n",
    "K = 12  # number of neurons expected per patch\n",
    "gSig = [8, 8, 2]  # expected half size of neurons\n",
    "merge_thresh = 0.8  # merging threshold, max correlation allowed\n",
    "p = 2  # order of the autoregressive system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "#%% RUN ALGORITHM ON PATCHES\n",
    "\n",
    "cnm = cnmf.CNMF(n_processes, k=K, gSig=gSig, merge_thresh=merge_thresh, p=p, dview=dview,\n",
    "                rf=rf, stride=stride, only_init_patch=True)\n",
    "\n",
    "%time cnm = cnm.fit(images)\n",
    "print(('Number of components:' + str(cnm.estimates.A.shape[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnm.estimates.nb_view_components_3d(image_type='mean', dims=dims);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% COMPONENT EVALUATION\n",
    "# the components are evaluated in two ways:\n",
    "#   a) the shape of each component must be correlated with the data\n",
    "#   b) a minimum peak SNR is required over the length of a transient\n",
    "\n",
    "fr = 2 # approx final rate  (after eventual downsampling )\n",
    "decay_time = 1.  # length of typical transient in seconds \n",
    "use_cnn = False  # CNN classifier is designed for 2d (real) data\n",
    "min_SNR = 3      # accept components with that peak-SNR or higher\n",
    "rval_thr = 0.7   # accept components with space correlation threshold or higher\n",
    "cnm.params.change_params(params_dict={'fr': fr,\n",
    "                                      'decay_time': decay_time,\n",
    "                                      'min_SNR': min_SNR,\n",
    "                                      'rval_thr': rval_thr,\n",
    "                                      'use_cnn': use_cnn});\n",
    "%time cnm.estimates.evaluate_components(images, cnm.params, dview=dview)\n",
    "\n",
    "print(('Keeping ' + str(len(cnm.estimates.idx_components)) +\n",
    "       ' and discarding  ' + str(len(cnm.estimates.idx_components_bad))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "cnm.params.set('temporal', {'p': p})\n",
    "%time cnm2 = cnm.refit(images_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnm2.estimates.nb_view_components_3d(image_type='corr', dims=dims, Yr=Yr,\\\n",
    "                                     denoised_color='red', max_projection=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm2.estimates.nb_view_components_3d(image_type='max', dims=dims, Yr=Yr,\\\n",
    "                                     denoised_color='red', max_projection=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnm.estimates.nb_view_components_3d(image_type='max', dims=dims, Yr=Yr,\\\n",
    "#                                      denoised_color='red', max_projection=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = cnm2.estimates.A.max(1).toarray()\n",
    "m = m.reshape(*images_now.shape[-3:])\n",
    "m = m.transpose(2, 0, 1)\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
