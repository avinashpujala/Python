{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import dask\n",
    "import caiman as cm\n",
    "import h5py\n",
    "# from skimage.external import tifffile as tff\n",
    "from sklearn.decomposition import PCA\n",
    "import tifffile as tff\n",
    "import joblib\n",
    "\n",
    "codeDir = r'V:/code/python/code'\n",
    "sys.path.append(codeDir)\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "import apCode.geom as geom\n",
    "# import seaborn as sns\n",
    "import importlib\n",
    "from apCode import util as util\n",
    "from apCode import hdf\n",
    "from apCode.imageAnalysis.spim import regress\n",
    "from apCode.behavior import gmm as my_gmm\n",
    "\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducability\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_xls = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging'\n",
    "file_xls = 'GCaMP volumetric imaging summary.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read xl file\n",
    "idx_fish = 4\n",
    "xls = pd.read_excel(os.path.join(dir_xls, file_xls), sheet_name='Sheet1')\n",
    "path_now = np.array(xls.loc[xls.FishIdx == idx_fish].Path)[0]\n",
    "print(path_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and register Ca$^{2+}$ images first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Read ScanImage tif files and store in trialized format to HDF file\n",
    "regex = r'\\d{1,5}_[ht]'\n",
    "regMethod = 'cpwr' # ('st', 'cr', 'cpwr')\n",
    "\n",
    "subDirs = [os.path.join(path_now, sd) for sd in ft.subDirsInDir(path_now)\\\n",
    "           if re.match(regex, sd)]\n",
    "hFileName = f'procData_{util.timestamp()}.h5'\n",
    "hFilePath = os.path.join(path_now, hFileName)\n",
    "stimLoc = []\n",
    "with h5py.File(hFilePath, mode='a') as hFile:\n",
    "    for iSub, sd in enumerate(subDirs):\n",
    "        hort= os.path.split(sd)[-1]            \n",
    "        fn = ft.findAndSortFilesInDir(sd,ext = '16ch')\n",
    "        if len(fn)>1:        \n",
    "            idx = input('Enter index of \"bas\" file: ')\n",
    "            idx = int(idx)\n",
    "            path_bas = os.path.join(sd, fn[idx])\n",
    "        else:\n",
    "            path_bas = os.path.join(sd,fn[-1])\n",
    "        path_tif = os.path.join(sd,'ca')\n",
    "        out = hf.readPeriStimulusTifImages_volumetric(path_tif, path_bas)\n",
    "        nTrls = out['images_trl'].shape[0]\n",
    "        stimLoc.extend([hort]*nTrls)\n",
    "        if iSub ==0:\n",
    "            nImgsInTrl = out['images_trl'].shape[1]\n",
    "        if not 'nImgsInTrl' in hFile:\n",
    "            hFile.create_dataset('nImgsInTrl', data = nImgsInTrl)\n",
    "        else:\n",
    "            foo = hFile['nImgsInTrl']\n",
    "            foo[...] = nImgsInTrl\n",
    "        keyName = 'ca_trls_raw'\n",
    "        if not keyName in hFile:\n",
    "            print(f'Creating {keyName} in h5 file')            \n",
    "            hFile.create_dataset(keyName, data = out['images_trl'],\\\n",
    "                                 maxshape = (None, *out['images_trl'].shape[1:]),\\\n",
    "                                 compression = 'lzf')\n",
    "        else:\n",
    "            print(f'Appending to {keyName} in h5 file')\n",
    "            hFile[keyName].resize((hFile[keyName].shape[0] + out['images_trl'].shape[0]),\\\n",
    "                                  axis = 0)\n",
    "            hFile[keyName][-out['images_trl'].shape[0]:] = out['images_trl']\n",
    "        keyName = f'inds_excluded/{os.path.split(sd)[-1]}'\n",
    "        if not keyName in hFile:\n",
    "            hFile.create_dataset(keyName, data = out['inds_excluded'])\n",
    "        else:\n",
    "            foo = hFile[keyName]\n",
    "            foo[...] = out['inds_excluded']\n",
    "    stimLoc_ascii = util.to_ascii(stimLoc)\n",
    "    if not 'stimLocVec' in hFile:\n",
    "        hFile.create_dataset('stimLocVec', data = np.array(stimLoc_ascii))\n",
    "    else:\n",
    "        foo = hFile['stimLocVec']\n",
    "        foo[...] = np.array(stimLoc_ascii)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from apCode.volTools import Register\n",
    "# nTrls = images.shape[0]\n",
    "# trlLen = images.shape[1]\n",
    "# volDims = images.shape[-3:]\n",
    "# images_ser = np.swapaxes(images.reshape(-1, *volDims), 0, 1)\n",
    "# images_reg = []\n",
    "# for iSlc, slc in enumerate(images_ser):\n",
    "#     print(f'{iSlc+1}/{volDims[0]}')\n",
    "#     img = Register(regMethod='cpwr').fit(slc).transform(slc)\n",
    "#     images_reg.append(img)\n",
    "# images_reg = np.swapaxes(np.array(images_reg), 0, 1)\n",
    "# images_reg = images_reg.reshape(nTrls, trlLen, *volDims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Read stored images from HDF file and register these images\n",
    "filtSize = 1\n",
    "regMethod = 'st' # ('st', 'cr', 'cpwr')\n",
    "\n",
    "if not 'hFilePath' in locals():\n",
    "    hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "    hFilePath = os.path.join(path_now, hFileName)\n",
    "if not 'images' in locals():\n",
    "    with h5py.File(os.path.join(path_now, hFileName), mode = 'r') as hFile:\n",
    "        %time images = np.array(hFile['ca_trls_raw'])\n",
    "nTrls = images.shape[0]\n",
    "nTimePts = images.shape[1]\n",
    "volDims = images.shape[-3:]\n",
    "# try:\n",
    "#     %time images_reg, regObj = hf.register_volumes_by_slices_and_trials(images, regMethod= regMethod,\\\n",
    "#                                                                     filtSize = filtSize)\n",
    "# except:\n",
    "#     print(f'{regMethod} failed, trying \"st\"...')\n",
    "#     %time images_reg, regObj = hf.register_volumes_by_slices_and_trials(images, regMethod= 'st',\\\n",
    "#                                                                     filtSize = filtSize)\n",
    "\n",
    "if regMethod.lower() =='st':\n",
    "    images_reg, regObj = hf.register_trialized_volumes_by_slices(images, filtSize = filtSize,\\\n",
    "                                                                 regMethod = regMethod)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save registered images to HDF file\n",
    "# hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "with h5py.File(hFilePath, mode = 'a') as hFile:\n",
    "    keyName = 'ca_trls_reg'\n",
    "    if not keyName in hFile:\n",
    "        hFile.create_dataset(keyName, data=images_reg)\n",
    "    else:\n",
    "        foo = hFile[keyName]\n",
    "        foo[...]= images_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = images_reg.reshape(-1,*images_reg.shape[2:])\n",
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iSlice = 14\n",
    "fps = 20\n",
    "# cm.movie(foo[:,iSlice,...],fr = fps).play(magnification=1.5, q_min = 5, q_max = 99)\n",
    "foo_max = foo.max(axis=1)\n",
    "cm.movie(foo_max, fr = fps).play(magnification=2, q_max = 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time foo_max_den = volt.denoise_ipca(foo_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.movie(foo_max_den, fr = fps).play(magnification=2, q_max=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save each of the registered slices in a separate .tif file as well as the max-int movie\n",
    "saveDir = os.path.join(path_now, f'registered_slices_{regMethod}_{util.timestamp()}')\n",
    "if not os.path.exists(saveDir):\n",
    "    os.mkdir(saveDir) \n",
    "from skimage.io import imsave\n",
    "foo = images_reg.reshape(-1,*images_reg.shape[2:])\n",
    "foo_int= foo.copy()\n",
    "foo_int = np.swapaxes(foo_int,0,1).astype('int16')\n",
    "for z, img in enumerate(foo_int):\n",
    "    tff.imsave(os.path.join(saveDir,r'slice_{:03d}.tif'.format(z)),img)\n",
    "\n",
    "#-- Also, save the movie of the max-z projection of slices    \n",
    "foo_max_int = foo.max(axis = 1).astype('int16')\n",
    "tff.imsave(os.path.join(os.path.split(saveDir)[0],r'maxInt_z_movie.tif'), foo_max_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Serialize and make caiman movie object from mean intensity projection\n",
    "# iSlices = np.array([14, 15, 16])\n",
    "iSlices = np.arange(1,30)\n",
    "\n",
    "images_reg_ser = images_reg.reshape(-1, *images_reg.shape[-3:])\n",
    "# mov = cm.movie(images_reg_ser.mean(axis = 1), fr = 10)\n",
    "mov = cm.movie(images_reg_ser[:,iSlices].max(axis = 1), fr = 20)\n",
    "%time mov_flt = cm.movie(volt.img.gaussFilt(mov, sigma = 0.75), fr = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_flt = mov_flt - mov_flt.min()\n",
    "mov_flt.play(magnification=1.5, q_min = 5, q_max = 95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff.imsave(os.path.join(path_now, 'Movie_maxIntProjZ_flt.tif'), mov_flt.astype('int16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Run NMF on filtered\n",
    "%time nmf_flt = mov_flt.NonnegativeMatrixFactorization(n_components=50)\n",
    "np.save(os.path.join(path_now, 'nmf_flt.npy'), nmf_flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_max = 99\n",
    "\n",
    "from skimage.util import montage\n",
    "plt.figure(figsize = (20,60))\n",
    "nmf_flt_space = np.array([spt.stats.saturateByPerc(_, perc_up=q_max) for _ in nmf_flt[0]])\n",
    "m = montage(nmf_flt_space, rescale_intensity = True, grid_shape=(len(nmf_flt[0])//4+1, 4))\n",
    "plt.imshow(m)\n",
    "\n",
    "\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iComp = 30\n",
    "nPre = 1\n",
    "\n",
    "trlLen = images_reg.shape[1]\n",
    "nTrls = images.shape[0]\n",
    "sl = [_[-1] for _ in stimLoc]\n",
    "\n",
    "stimInds = np.arange(nTrls)*trlLen + nPre\n",
    "plt.figure(figsize = (20,15))\n",
    "plt.subplot(211)\n",
    "plt.imshow(spt.stats.saturateByPerc(nmf_flt[0][iComp], perc_up = 95))\n",
    "plt.subplot(212)\n",
    "plt.plot(nmf_flt[1][:,iComp])\n",
    "for si in stimInds:\n",
    "    plt.axvline(si, ls = '--', c = 'r', alpha = 0.3)\n",
    "plt.xticks(stimInds, sl)\n",
    "plt.xlim(0, nmf_flt[1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Save nmf spatial and temporal component component images\n",
    "saveDir = os.path.join(path_now, 'proc/nmf_flt/spatial comps/')\n",
    "tifDir = os.path.join(saveDir,'tifs')\n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)\n",
    "    os.mkdir(tifDir)\n",
    "for iComp, comp in enumerate(nmf_flt[0]):\n",
    "    plt.imshow(comp, cmap = 'viridis')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'nmf {iComp}')\n",
    "    plt.savefig(os.path.join(saveDir, r'nmf_{:03}.pdf'.format(iComp)), format = 'pdf', dpi = 'figure')\n",
    "    plt.savefig(os.path.join(saveDir, r'nmf_{:03}.png'.format(iComp)), format = 'png', dpi = 'figure')\n",
    "    foo = (nmf[0][iComp]*(2**16-1)).astype('int')\n",
    "    tff.imsave(os.path.join(tifDir, f'nmf_{iComp}.tif'),foo)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "hFilePath = os.path.join(path_now,hFileName)\n",
    "with h5py.File(hFilePath, mode = 'r') as hFile:\n",
    "#     %time images_reg = np.array(hFile['ca_trls_reg_trialByTrial'])\n",
    "    %time images_reg = np.array(hFile['ca_trls_reg'])\n",
    "    stimLoc = util.to_utf(hFile['stimLocVec'])\n",
    "    print(hFile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Now save individual trial volumes as \n",
    "nPre = 3\n",
    "n_comps = 5\n",
    "\n",
    "def denoise_stack(stack, n_comps = 10):\n",
    "    foo = dask.compute(*[dask.delayed(volt.ipca_denoise)(s, components = n_comps)\\\n",
    "                         for s in np.swapaxes(stack,0,1)])\n",
    "    return np.swapaxes(np.array(foo),0,1)\n",
    "saveDir = os.path.join(path_now, f'trialVolumes_avg2')\n",
    "if not os.path.exists(saveDir):\n",
    "    os.mkdir(saveDir)\n",
    "\n",
    "for iTrl, stack in enumerate(images_reg):\n",
    "#     pre = stack[:nPre].mean(axis = 0)\n",
    "#     post = stack[nPre:].mean(axis = 0)\n",
    "#     vol = post-pre\n",
    "#     stack = denoise_stack(stack, n_comps = n_comps)\n",
    "    vol = stack.mean(axis = 0)    \n",
    "#     vol = np.median(stack, axis = 0)\n",
    "    fn =  '{}_{:03d}.tif'.format(stimLoc[iTrl][-1],iTrl) \n",
    "    tff.imsave(os.path.join(saveDir,fn), vol[1:].astype('int16'), metadata = {'axes': 'ZYX'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveDir = r'Y:\\Avinash\\Head-fixed tail free\\GCaMP imaging\\2019-11-07\\f1_alx-gal4_xa316_uas-gcamp6s\\registered_images_cpwr'\n",
    "\n",
    "# reg = volt.Register(backend='dask', regMethod='cpwr')\n",
    "# foo = np.swapaxes(images.reshape(-1,*images.shape[2:]), 0, 1)\n",
    "# foo_reg = []\n",
    "# for z, img in enumerate(foo):\n",
    "#     print(f'{z+1}/{len(foo)}')\n",
    "#     foo_reg.append(reg.fit(img).transform(img))\n",
    "\n",
    "# foo_reg = np.array(foo_reg)\n",
    "# foo_int = (spt.standardize(foo_reg)*(2**8)).astype('uint8')\n",
    "# print('Saving...')\n",
    "# for z, img in enumerate(foo_int):\n",
    "#     tff.imsave(os.path.join(saveDir,r'slice_{:03d}.tif'.format(z)),img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pre-trained U-net first, assess performance, and retrain if need be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_u = os.path.split(path_now)[0]\n",
    "path_unet = os.path.join(dir_u,ft.findAndSortFilesInDir(dir_u, search_str='trainedU', ext = 'h5')[-1])\n",
    "\n",
    "%time unet = mlearn.loadPreTrainedUnet(path_unet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get the paths to all the behavior directories\n",
    "roots, dirs, files = zip(*[out for out in os.walk(dir_u)])\n",
    "inds = util.findStrInList('Autosave', roots)\n",
    "behavDirs =np.array(roots)[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Copy images for additional training\n",
    "prefFrameRangeInTrl = (510, 550)\n",
    "nImgsForTraining = 100\n",
    "_, dir_train = mlearn.copyImgsNNTraining(behavDirs, prefFrameRangeInTrl=prefFrameRangeInTrl,\\\n",
    "                                         nImgsForTraining=nImgsForTraining)\n",
    "\n",
    "print(f'Saved at {dir_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Predict on copied images to see how good pre-trained network is\n",
    "%time images_raw = volt.img.readImagesInDir(dir_train)\n",
    "images = volt.img.resize(images_raw, unet.input_shape[1:3])\n",
    "images_pred = np.squeeze(unet.predict(images[...,np.newaxis],verbose = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlearn.plotMontageOfImageCollections(images, images_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'(\\d.*\\d)'\n",
    "dir_train_imgs = dir_train\n",
    "p, s = os.path.split(dir_train_imgs)\n",
    "sffx = re.findall(regex, s)[0]\n",
    "dir_train_masks =  os.path.join(os.path.split(dir_train)[0], f'masks_train_{sffx}.zip')\n",
    "\n",
    "# aug_set = ('rn','sig','log','inv','heq','rot', 'et', 'rs')\n",
    "aug_set = ('rn','sig','log','inv','heq','rot', 'rs') # For some reason 'et' augmentation stopped working (some opencv error)\n",
    "\n",
    "unet = mlearn.retrainU(unet, dir_train_imgs, dir_train_masks, upSample=15, epochs=100, verbose=2,\\\n",
    "                       aug_set = aug_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Test trained u-net on a set of continuous images\n",
    "iTrl = 1\n",
    "images_raw = volt.img.readImagesInDir(behavDirs[iTrl])\n",
    "images = volt.img.resize(images_raw, unet.input_shape[1:3])\n",
    "images_pred = np.squeeze(unet.predict(images[...,np.newaxis],verbose = 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Look at a few images by plotting\n",
    "imgInds = np.arange(510,640,2)\n",
    "mlearn.plotMontageOfImageCollections(images[imgInds], images_pred[imgInds]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and store behavior data in HDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time hFilePath = hf.extractAndStoreBehaviorData_singleFish(path_now,uNet=unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with h5py.File(hFilePath, mode = 'r') as hFile:\n",
    "    ta = np.array(hFile['behav/tailAngles'])\n",
    "    ta = ta.reshape(-1,50,ta.shape[-1])\n",
    "    ta_trl = np.array([_[-1] for _ in ta])\n",
    "    ta_ser = np.concatenate(ta,axis = 1)\n",
    "    %time ta_ser_clean, ta_svd, svd = hf.cleanTailAngles(ta_ser, None, dt = 1/500)\n",
    "    ta_trl_clean = (ta_ser_clean[-1]).reshape(-1,ta_trl.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_behav= 1/500\n",
    "t_stim = 1\n",
    "xl = (-0.15, 4.5)\n",
    "# inds = np.random.permutation(ta_trl.shape[0])\n",
    "inds = np.arange(ta_trl.shape[0])\n",
    "yOff = util.yOffMat(ta_trl[inds])\n",
    "t_trl = (np.arange(ta_trl.shape[1])*dt_behav) - t_stim\n",
    "plt.figure(figsize = (20, 60))\n",
    "plt.plot(t_trl,(ta_trl[inds]-yOff).T, c = plt.cm.tab10(0))\n",
    "plt.plot(t_trl,(ta_trl_clean[inds]-yOff).T, c = plt.cm.tab10(1));\n",
    "plt.xlim(t_trl.min(), t_trl.max())\n",
    "plt.yticks(-yOff, np.arange(len(inds)))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Trial #')\n",
    "\n",
    "# plt.xlim(xl);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_behav= 1/500\n",
    "t_stim = 1\n",
    "xl = (-0.15,4.5)\n",
    "figName = f'Fig-{util.timestamp()}_Behavior trials_tail bend amplitude'\n",
    "figDir = os.path.join(path_now,'figs')\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "# inds = np.random.permutation(ta_trl.shape[0])\n",
    "inds = np.arange(ta_trl.shape[0]-1)\n",
    "inds = np.random.permutation(inds)\n",
    "yOff = util.yOffMat(ta_trl_clean[inds])\n",
    "t_trl = (np.arange(ta_trl.shape[1])*dt_behav) - t_stim\n",
    "plt.figure(figsize = (20, 30))\n",
    "# plt.plot(t_trl,(ta_trl[inds]-yOff).T, c = plt.cm.tab10(0))\n",
    "plt.plot(t_trl,(ta_trl_clean[inds]-yOff).T);\n",
    "plt.xlim(xl)\n",
    "plt.yticks(-yOff.ravel(), np.arange(len(inds))+1);\n",
    "plt.ylabel('Trial #')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Behavior trials (total tail bend angle)')\n",
    "plt.savefig(os.path.join(figDir,figName + '.pdf'), format = 'pdf', dpi = 'figure')\n",
    "plt.savefig(os.path.join(figDir,figName + '.png'), format = 'png', dpi = 'figure')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Ca and behavior trials from HDF file and put in a datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Read Ca2+ and matching behavior trials from HDF file and create a dataframe\n",
    "\n",
    "def get_session_inds(stimLocs, regex = r'\\d{1,5}'):\n",
    "    import re\n",
    "    inds = []\n",
    "    for s in stimLocs:\n",
    "        inds.append(np.int(re.findall(regex, s)[0]))\n",
    "    return np.array(inds)\n",
    "\n",
    "if not 'hFilePath' in locals():\n",
    "    \n",
    "with h5py.File(hFilePath, mode = 'r') as hFile:\n",
    "    ta_trl = hFile['behav/tailAngles'][()]\n",
    "    ta_trl = ta_trl.reshape(-1, 50, ta_trl.shape[-1])\n",
    "    ta_ser = np.concatenate(ta_trl,axis = 1)\n",
    "    %time ta_ser_clean, ta_svd, svd = hf.cleanTailAngles(ta_ser, None, dt = 1/500)\n",
    "    ta_trl_clean = np.transpose(ta_ser_clean.reshape(ta_ser_clean.shape[0],-1,ta_trl.shape[-1]),\\\n",
    "                                (1,0,2))\n",
    "    stimLoc_ca = [sl for sl in np.array(util.to_utf(hFile['stimLocVec']))]\n",
    "    stimLoc_behav = util.to_utf(hFile['behav/stimLoc'][()])\n",
    "    ca_trl = np.array(hFile['ca_trls_reg'][()])\n",
    "    blocks, inds_blocks = util.get_blocks_of_repeats(stimLoc_behav)\n",
    "    inds_excluded = hFile['inds_excluded']\n",
    "    keys = list(inds_excluded.keys())\n",
    "    foo = []\n",
    "    for iSub, inds_ in enumerate(inds_blocks):\n",
    "        k = keys[iSub]\n",
    "        inds_del = inds_excluded[k]\n",
    "        inds_now = np.delete(inds_, inds_del, axis = 0)\n",
    "        foo.extend(ta_trl_clean[inds_now])\n",
    "    ta_trl_clean = np.array(foo)\n",
    "    \n",
    "\n",
    "# blocks_ca, inds_ca = util.get_blocks_of_repeats(stimLoc_ca)\n",
    "# blocks_behav, inds_behav = util.get_blocks_of_repeats(stimLoc_behav)\n",
    "# inds_keep_ca, inds_keep_behav = [],[]\n",
    "# for iBlock in range(len(blocks_behav)):\n",
    "#     ic, ib = inds_ca[iBlock], inds_behav[iBlock]\n",
    "#     n = np.minimum(len(ic), len(ib))\n",
    "#     inds_keep_ca.extend(ic[:n])\n",
    "#     inds_keep_behav.extend(ib[:n])\n",
    "# stimLoc_ca, stimLoc_behav = np.array(stimLoc_ca)[inds_keep_ca], np.array(stimLoc_behav)[inds_keep_behav]\n",
    "# inds_session = get_session_inds(stimLoc_ca)[inds_keep_ca]\n",
    "# getLastChar = lambda x: np.array([_[-1] for _ in x])\n",
    "# stimLoc_ca, stimLoc_behav = map(getLastChar, (stimLoc_ca, stimLoc_behav))\n",
    "# ca_trl = np.take(ca_trl, inds_keep_ca, axis = 0)\n",
    "# ta_trl_clean = np.take(ta_trl_clean, inds_keep_behav, axis = 0)\n",
    "\n",
    "# dic = dict(tailAngles = list(ta_trl_clean), ca = list(ca_trl), \\\n",
    "#            trlNum = np.arange(ta_trl_clean.shape[0]), stimLoc = stimLoc_ca,\\\n",
    "#            sessionIdx = list(inds_session), hFilePath = hFilePath)\n",
    "\n",
    "# df = pd.DataFrame(dic, columns= dic.keys())\n",
    "\n",
    "# path_df = os.path.join(path_now, 'dataFrame.pickle')\n",
    "# %time df.to_pickle(path_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue from the saved dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read saved pickle file for continuing analysis from here\n",
    "file_df = ft.findAndSortFilesInDir(path_now, ext = 'pickle', search_str='dataFrame')[-1]\n",
    "%time df = pd.read_pickle(os.path.join(path_now, file_df))\n",
    "ta_trl = np.array([np.array(_) for _ in df.tailAngles])\n",
    "ta = np.concatenate(ta_trl,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the group data trained GMM model to predict posture labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the trained model\n",
    "\n",
    "dir_group = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging\\Group'\n",
    "fName = 'gmm_fitter_svd-3_gmm-22.pkl' \n",
    "fitter = joblib.load(os.path.join(dir_group, fName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *This is a 22 Gaussian components model because this number appears to be a global minimum for the function of AIC/BIC vs number # of gaussian components*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fit GMM model with specified number of components, predict labels for data, \n",
    "## and plot in low dimensions with PCA using labels as colors\n",
    "\n",
    "subSample = 1\n",
    "alpha = 0.5\n",
    "cmap = plt.cm.gist_ncar\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%time labels, arr_feat =  my_gmm.predict_on_tailAngles_svd(ta, fitter, subSample=subSample)\n",
    "\n",
    "%time x_pca = PCA(n_components = 3, random_state = 143).fit_transform(arr_feat)\n",
    "\n",
    "fh,ax = plt.subplots(2,2,figsize = (15,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "\n",
    "clrs = cmap(spt.standardize(labels))\n",
    "\n",
    "ax[0].scatter(x_pca[:,0], x_pca[:,1], s = 10, c = clrs, alpha = alpha)\n",
    "ax[0].set_xlabel('pca 1')\n",
    "ax[0].set_ylabel('pca 2')\n",
    "\n",
    "ax[1].scatter(x_pca[:,0], x_pca[:,2], s = 10, c = clrs, alpha = alpha)\n",
    "ax[1].set_xlabel('pca 1')\n",
    "ax[1].set_ylabel('pca 3')\n",
    "\n",
    "ax[2].scatter(x_pca[:,1], x_pca[:,2], s = 10, c = clrs, alpha = alpha)\n",
    "ax[2].set_xlabel('pca 2')\n",
    "ax[2].set_ylabel('pca 3')\n",
    "fh.tight_layout()\n",
    "\n",
    "x = np.unique(labels)\n",
    "y = np.ones_like(x)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.scatter(x,y, c = cmap(spt.standardize(x)),s = 4000, marker = 's')\n",
    "plt.yticks([])\n",
    "plt.xticks(x, fontsize = 20);\n",
    "plt.title('Norm-ordered colors', fontsize = 20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check ouf a few trials with predictions from the GMM\n",
    "\n",
    "iTrl = 6  # (Struggles = {9, 11}\n",
    "# iTrl = np.random.choice(np.arange(df.shape[0]),size = 1)[0]\n",
    "yShift = 1.1\n",
    "loop = False\n",
    "xl = (-0.1, 1)\n",
    "# xl = 'auto'\n",
    "figDir = os.path.join(path_now, 'figs/behav_trls_colored_by_gmm_label')\n",
    "onOffThr = 0\n",
    "figExts = ('png','pdf')\n",
    "cmap = plt.cm.gist_ncar\n",
    "dt_behav = 1/500\n",
    "\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "from IPython import display\n",
    "if loop:\n",
    "    trls = np.unique(df.trlNum)\n",
    "else:\n",
    "    trls = [iTrl]\n",
    "\n",
    "maxEnv_full = spt.emd.envelopesAndImf(ta[-1])['env']['max']    \n",
    "for iTrl in trls:\n",
    "    df_now = df.loc[df.trlNum == iTrl]\n",
    "#     arr = np.array(df_features.loc[df_features.trlNum == iTrl].drop(columns = 'trlNum'))\n",
    "#     ta_now = np.array(df_now.iloc[0]['tailAngles'])\n",
    "    ta_now = ta_trl[iTrl]\n",
    "    trlLen = ta_now.shape[-1]\n",
    "    inds = np.arange(iTrl*trlLen, (iTrl+1)*trlLen)\n",
    "    maxEnv = maxEnv_full[inds]\n",
    "    posInds = np.linspace(0, len(ta_now)-1,4).astype(int)\n",
    "    z = np.diff(ta_now[posInds],axis = 0)\n",
    "    ys = util.yOffMat(z)*yShift\n",
    "    z = z - ys\n",
    "    y = ta_now[-1]\n",
    "    x = (np.arange(len(y))*dt_behav) - 1   \n",
    "    lbls_now, x_now = my_gmm.predict_on_tailAngles_svd(ta_now, fitter)\n",
    "    lbls_norm = lbls_now/(labels.max()-labels.min())\n",
    "    clrs = cmap(lbls_norm)\n",
    "    zerInds = np.where(maxEnv <=onOffThr)[0]\n",
    "    clrs[zerInds,:] = 1 # Make this invisible\n",
    "    # clrs[:,-1] = 0.8 # Alpha value of points\n",
    "    fh = plt.figure(figsize = (20,8));    \n",
    "    # plt.scatter(x,y,c = clrs,s= 15)\n",
    "    for iLine, z_ in enumerate(z):\n",
    "        plt.plot(x,z_, c = plt.cm.tab10(iLine), alpha = 0.2, label = f'loc = {iLine}')\n",
    "        plt.scatter(x,z_, c = clrs, s = 15)\n",
    "        plt.legend(loc = 'upper right', fontsize= 15)\n",
    "    plt.xlabel('Time (s)', fontsize = 20)\n",
    "    plt.ylabel('Total tail bending for segment ($^o$)', fontsize = 20)\n",
    "    maxInd = []\n",
    "    if np.any(xl == 'auto'):\n",
    "        nonZerInds = np.setdiff1d(np.arange(len(y)),zerInds)        \n",
    "        if len(nonZerInds)>0:\n",
    "            maxInd = np.max(nonZerInds)\n",
    "        else:\n",
    "            maxInd = len(x)-31\n",
    "        plt.xlim(-0.1,x[maxInd])\n",
    "    else:\n",
    "        plt.xlim(xl)\n",
    "    yl = np.min((-100, z.min())), np.max((80, z.max()))\n",
    "    plt.ylim(yl)\n",
    "    plt.yticks([0,-100])\n",
    "    plt.title(f'Total tail curvature with points colored by cluster label (trl = {iTrl})', fontsize = 20)\n",
    "    plt.show()\n",
    "    for fe in figExts:\n",
    "        fn = f'Fig-{util.timestamp()}_Total tail curvature timeseries with colored clustered points_trl-{iTrl}.{fe}'\n",
    "#         fh.savefig(os.path.join(figDir, fn), format = f'{fe}', dpi = 'figure')\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the trained model\n",
    "\n",
    "dir_group = r'Y:\\Avinash\\Projects\\RS recruitment\\GCaMP imaging\\Group'\n",
    "file_model = 'gmm_svd-3_env_pca-9_gmm-20_20200129-18.pkl' \n",
    "gmm_model = joblib.load(os.path.join(dir_group, file_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fit GMM model with specified number of components, predict labels for data, \n",
    "## and plot in low dimensions with PCA using labels as colors\n",
    "\n",
    "subSample = 1\n",
    "alpha = 0.5\n",
    "cmap = plt.cm.tab20\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%time labels, features =  gmm_model.predict(ta[:,::subSample])\n",
    "\n",
    "%time x_pca = PCA(n_components = 3, random_state = 143).fit_transform(features)\n",
    "\n",
    "fh,ax = plt.subplots(2,2,figsize = (15,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "\n",
    "clrs = cmap(spt.standardize(labels))\n",
    "\n",
    "ax[0].scatter(x_pca[:,0], x_pca[:,1], s = 10, c = clrs, alpha = alpha)\n",
    "ax[0].set_xlabel('pca 1')\n",
    "ax[0].set_ylabel('pca 2')\n",
    "\n",
    "ax[1].scatter(x_pca[:,0], x_pca[:,2], s = 10, c = clrs, alpha = alpha)\n",
    "ax[1].set_xlabel('pca 1')\n",
    "ax[1].set_ylabel('pca 3')\n",
    "\n",
    "ax[2].scatter(x_pca[:,1], x_pca[:,2], s = 10, c = clrs, alpha = alpha)\n",
    "ax[2].set_xlabel('pca 2')\n",
    "ax[2].set_ylabel('pca 3')\n",
    "fh.tight_layout()\n",
    "\n",
    "x = np.unique(labels)\n",
    "y = np.ones_like(x)\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.scatter(x,y, c = cmap(spt.standardize(x)),s = 4000, marker = 's')\n",
    "plt.yticks([])\n",
    "plt.xticks(x, fontsize = 20);\n",
    "plt.title('Norm-ordered colors', fontsize = 20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check ouf a few trials with predictions from the GMM\n",
    "\n",
    "# iTrl = 10  # (Struggles = {9, 11}\n",
    "iTrl = np.random.choice(np.arange(df.shape[0]),size = 1)[0]\n",
    "yShift = 1.1\n",
    "loop = False\n",
    "xl = (-0.1, 4.5)\n",
    "# xl = 'auto'\n",
    "figDir = os.path.join(path_now, 'figs/behav_trls_colored_by_gmm_label')\n",
    "onOffThr = 5\n",
    "figExts = ('png','pdf')\n",
    "cmap = plt.cm.tab20\n",
    "dt_behav = 1/500\n",
    "annotate_markers = True\n",
    "\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "from IPython import display\n",
    "if loop:\n",
    "    trls = np.unique(df.trlNum)\n",
    "else:\n",
    "    trls = [iTrl]\n",
    "\n",
    "maxEnv_full = spt.emd.envelopesAndImf(ta[-1])['env']['max']    \n",
    "for iTrl in trls:\n",
    "    df_now = df.loc[df.trlNum == iTrl]\n",
    "    ta_now = ta_trl[iTrl]\n",
    "    trlLen = ta_now.shape[-1]\n",
    "    inds = np.arange(iTrl*trlLen, (iTrl+1)*trlLen)\n",
    "    maxEnv = maxEnv_full[inds]\n",
    "    posInds = np.linspace(0, len(ta_now)-1,4).astype(int)\n",
    "    z = np.diff(ta_now[posInds],axis = 0)\n",
    "    ys = util.yOffMat(z)*yShift\n",
    "    z = z - ys\n",
    "    y = ta_now[-1]\n",
    "    x = (np.arange(len(y))*dt_behav) - 1   \n",
    "    lbls_now, _ = gmm_model.predict(ta_now)\n",
    "    lbls_norm = lbls_now/(labels.max()-labels.min())\n",
    "    clrs = cmap(lbls_norm)\n",
    "    zerInds = np.where(maxEnv <=onOffThr)[0]\n",
    "    clrs[zerInds,:] = 1 # Make this invisible\n",
    "    # clrs[:,-1] = 0.8 # Alpha value of points\n",
    "    fh = plt.figure(figsize = (20,8))    \n",
    "    for iLine, z_ in enumerate(z):\n",
    "#         plt.plot(x,z_, c = cmap(iLine), alpha = 0.2, label = f'loc = {iLine}')\n",
    "        plt.plot(x,z_, c = 'k', alpha = 0.2, label = f'loc = {iLine}')\n",
    "        plt.scatter(x,z_, c = clrs, s = 15)\n",
    "        plt.legend(loc = 'upper right', fontsize= 15)\n",
    "    plt.xlabel('Time (s)', fontsize = 20)\n",
    "    plt.ylabel('Total tail bending for segment ($^o$)', fontsize = 20)\n",
    "    maxInd = []\n",
    "    if np.any(xl == 'auto'):\n",
    "        nonZerInds = np.setdiff1d(np.arange(len(y)),zerInds)        \n",
    "        if len(nonZerInds)>0:\n",
    "            maxInd = np.max(nonZerInds)\n",
    "        else:\n",
    "            maxInd = len(x)-31\n",
    "        plt.xlim(-0.1,x[maxInd])\n",
    "    else:\n",
    "        plt.xlim(xl)\n",
    "    yl = np.min((-100, z.min())), np.max((80, z.max()))\n",
    "    plt.ylim(yl)\n",
    "    plt.yticks([0,-100])\n",
    "    plt.title(f'Total tail curvature with points colored by cluster label (trl = {iTrl})', fontsize = 20)\n",
    "    plt.show()\n",
    "    for fe in figExts:\n",
    "        fn = f'Fig-{util.timestamp()}_Total tail curvature timeseries with colored clustered points_trl-{iTrl}.{fe}'\n",
    "#         fh.savefig(os.path.join(figDir, fn), format = f'{fe}', dpi = 'figure')\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Check ouf a few trials with predictions from the GMM with annotated markers\n",
    "\n",
    "# iTrl = 10  # (Struggles = {9, 11}\n",
    "iTrl = np.random.choice(np.arange(df.shape[0]),size = 1)[0]\n",
    "yShift = 1.1\n",
    "loop = False\n",
    "xl = (-0.1, 4.5)\n",
    "# xl = 'auto'\n",
    "figDir = os.path.join(path_now, 'figs/behav_trls_colored_by_gmm_label')\n",
    "onOffThr = 0\n",
    "figExts = ('png','pdf')\n",
    "cmap = plt.cm.tab20\n",
    "dt_behav = 1/500\n",
    "\n",
    "%matplotlib qt\n",
    "# %matplotlib notebook\n",
    "\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "trls = [iTrl]    \n",
    "maxEnv_full = spt.emd.envelopesAndImf(ta[-1])['env']['max']    \n",
    "for iTrl in trls:\n",
    "    df_now = df.loc[df.trlNum == iTrl]\n",
    "    ta_now = ta_trl[iTrl]\n",
    "    trlLen = ta_now.shape[-1]\n",
    "    inds = np.arange(iTrl*trlLen, (iTrl+1)*trlLen)\n",
    "    maxEnv = maxEnv_full[inds]\n",
    "    posInds = np.linspace(0, len(ta_now)-1,4).astype(int)\n",
    "    y = ta_now[-1]\n",
    "    x = (np.arange(len(y))*dt_behav) - 1   \n",
    "    lbls_now, _ = gmm_model.predict(ta_now)\n",
    "    lbls_norm = lbls_now/(labels.max()-labels.min())\n",
    "    clrs = cmap(lbls_norm)\n",
    "    zerInds = np.where(maxEnv <=onOffThr)[0]\n",
    "    clrs[zerInds,:] = 1 # Make this invisible\n",
    "    # clrs[:,-1] = 0.8 # Alpha value of points\n",
    "    fh = plt.figure(figsize = (20,8))\n",
    "    plt.plot(x,y, c = 'k', alpha = 0.2)\n",
    "    for lbl_ in np.unique(lbls_now):        \n",
    "        inds = np.where(lbls_now == lbl_)[0]\n",
    "        inds = inds[::2]\n",
    "        if len(inds)>0:\n",
    "            plt.scatter(x[inds],y[inds], c = clrs[inds], s = 150, marker = f\"${str(lbl_)}$\")    \n",
    "    plt.xlabel('Time (s)', fontsize = 20)\n",
    "    plt.ylabel('Total tail bending for segment ($^o$)', fontsize = 20)\n",
    "    maxInd = []\n",
    "    if np.any(xl == 'auto'):\n",
    "        nonZerInds = np.setdiff1d(np.arange(len(y)),zerInds)        \n",
    "        if len(nonZerInds)>0:\n",
    "            maxInd = np.max(nonZerInds)\n",
    "        else:\n",
    "            maxInd = len(x)-31\n",
    "        plt.xlim(-0.1,x[maxInd])\n",
    "    else:\n",
    "        plt.xlim(xl)\n",
    "    yl = np.min((-100, z.min())), np.max((80, z.max()))\n",
    "    plt.ylim(yl)\n",
    "    plt.yticks([0,-100])\n",
    "    plt.title(f'Total tail curvature with points colored by cluster label (trl = {iTrl})', fontsize = 20)\n",
    "    for fe in figExts:\n",
    "        fn = f'Fig-{util.timestamp()}_Total tail curvature timeseries with colored clustered points_trl-{iTrl}.{fe}'\n",
    "#         fh.savefig(os.path.join(figDir, fn), format = f'{fe}', dpi = 'figure')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def labelsToIrs(labels, maxEnv, nClust:int = 5, thr = 10, subSample:int = 1):\n",
    "#     \"\"\"Impulse resonse array from labels, (nLabels, nTimePoints)\"\"\"\n",
    "#     lbls_unique = np.unique(labels)\n",
    "#     zerInds = np.where(maxEnv<thr)[0]\n",
    "#     ir = np.zeros((nClust, len(labels)))\n",
    "#     for lbl in np.sort(lbls_unique):\n",
    "#         inds = np.where(labels == lbl)[0]\n",
    "#         ir[lbl,inds] = 1\n",
    "#     subVec = np.zeros_like(labels)\n",
    "#     subVec[::subSample] = 1\n",
    "#     ir[:,zerInds] = 0     \n",
    "#     ir = ir*subVec[np.newaxis,:]    \n",
    "#     return ir*subVec[np.newaxis,:]\n",
    "\n",
    "def superSample(t,y,tt):\n",
    "    \"\"\"Super sample a signal using interpolation\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.interpolate import interp1d\n",
    "    t = np.concatenate((tt[0].reshape((-1,)), t, tt[-1].reshape((-1,))))\n",
    "    y = np.concatenate((np.array(0).reshape((-1,)),y,np.array(0).reshape((-1,))))\n",
    "    f = interp1d(t,y,kind = 'slinear')\n",
    "    return f(tt)\n",
    "\n",
    "def padIr(ir_trl, pad_pre, pad_post):\n",
    "    \"\"\"\n",
    "    Pads the impulse response timeseries obtained from \n",
    "    predictions on behavioral feature matrix to match\n",
    "    time length with ca responses\n",
    "    \"\"\"\n",
    "    ir_ser = []\n",
    "    for c in ir_trl:\n",
    "        ir_ser.append(np.pad(c,((0,0),(pad_pre, pad_post))).flatten())\n",
    "    return np.array(ir_ser)\n",
    "\n",
    "def serializeHyperstack(vol):\n",
    "    \"\"\"\n",
    "    Given, a hyperstack, returns a 2D array with pixels serialized for regression, etc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vol: array, (nTimePoints, nSlices, nRows, nCols)\n",
    "    Returns\n",
    "    -------\n",
    "    vol_ser: array, (nTimePoints,nPixels)\n",
    "    \"\"\"\n",
    "    vol_trans = np.transpose(vol,(2,3,1,0))\n",
    "    vol_ser = vol_trans.reshape(-1, vol_trans.shape[-1])\n",
    "    vol_ser = np.swapaxes(vol_ser,0,1)\n",
    "    return vol_ser\n",
    "\n",
    "def deserializeToHyperstack(arr, volDims):\n",
    "    \"\"\"\n",
    "    Given an array which \n",
    "    \"\"\"\n",
    "    volDims = (np.array(volDims))[[1,2,0]]\n",
    "    vol = arr.reshape(arr.shape[0],*volDims)\n",
    "    vol = np.transpose(vol,(0,3,1,2))\n",
    "    return vol\n",
    "\n",
    "def pxlsToVol(pxls, volDims):\n",
    "    \"\"\"\n",
    "    Given an array which \n",
    "    \"\"\"\n",
    "    volDims = (np.array(volDims))[[1,2,0]]\n",
    "    vol = pxls.reshape(*volDims)\n",
    "    vol = np.transpose(vol,(2,0,1))\n",
    "    return vol\n",
    "\n",
    "def superSample_arr(t, arr, tt, n_jobs = 32):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: array, (nSignals, nTimePoints)\n",
    "    \"\"\"    \n",
    "    from joblib import Parallel, delayed\n",
    "    n_jobs = np.min((32, os.cpu_count()))\n",
    "#     from dask import delayed, compute\n",
    "#     arr_sup = compute(*[delayed(superSample)(t,y,tt) for y in arr], scheduler = 'processes')\n",
    "    arr_sup = Parallel(n_jobs=n_jobs,verbose=1)(delayed(superSample)(t, y, tt) for y in arr)\n",
    "    return np.array(arr_sup)\n",
    "\n",
    "def betasToVol(betas, volDims):\n",
    "    if np.ndim(betas)<2:\n",
    "        betas = betas[:,np.newaxis]  \n",
    "    nReg = betas.shape[1]\n",
    "    B = betas.T.reshape(nReg, *volDims)\n",
    "    return np.squeeze(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Select labels based on amplitude threshold because many labels are fitting background activity.\n",
    "cmap= plt.cm.tab20\n",
    "%matplotlib inline\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "ampMeans = np.zeros((len(labels_unique),))\n",
    "for lbl in labels_unique:\n",
    "    inds = np.where(labels == lbl)[0]\n",
    "    ampMeans[lbl] = maxEnv_full[inds].mean()\n",
    "sortInds = np.argsort(ampMeans)\n",
    "plt.figure(figsize = (14,8))\n",
    "clrs = cmap(spt.standardize(labels_unique))\n",
    "x = np.arange(len(ampMeans))\n",
    "for i in x:\n",
    "    plt.scatter(i, ampMeans[sortInds][i],c =clrs[i].reshape(1,-1),  marker = f\"${str(sortInds[i])}$\", s= 200)\n",
    "plt.xticks(x)\n",
    "plt.xlabel('Label #')\n",
    "plt.ylabel('Mean crest envelope amplitude for label')\n",
    "plt.grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Predict labels on full time series, match lengths of behavior and ca trials, and make full set of impulse \n",
    "### trains and other regressors\n",
    "thr_labelAmp = 10 # Set amplitude threshold based on the above graph\n",
    "tPeriStim_behav = (-1,6) # Pre- and pos-stim periods in seconds for behavior trials\n",
    "tPeriStim_ca = (-1,10) # Pre- and post-stim periods in seconds for ca trials\n",
    "Fs_behav = 500\n",
    "\n",
    "labels_sel = np.where(ampMeans>=thr_labelAmp)[0]\n",
    "ir, names_ir = hf.impulse_trains_from_labels(labels, ta, labels_sel= labels_sel)\n",
    "# yOff = util.yOffMat(ir)\n",
    "# plt.figure(figsize = (20,20))\n",
    "# plt.plot((ir-yOff).T);\n",
    "# plt.yticks(-yOff, names_ir);\n",
    "\n",
    "pad_post = int((tPeriStim_ca[-1]-tPeriStim_behav[-1])*Fs_behav)\n",
    "n_pre_behav = int(np.abs(tPeriStim_behav[0])*Fs_behav)\n",
    "stimLoc = np.array(df.stimLoc)\n",
    "stimLoc_unique = np.unique(stimLoc)\n",
    "sessionIdx  = np.array(df.sessionIdx)\n",
    "sessionIdx_unique = np.unique(sessionIdx)\n",
    "nSessions = len(sessionIdx_unique)\n",
    "\n",
    "nTrls = df.shape[0]\n",
    "ir_trl = np.transpose(ir.reshape(ir.shape[0], nTrls,-1),(1,0,2))\n",
    "foo = []\n",
    "count = 1\n",
    "for sl, trl in zip(stimLoc, ir_trl):\n",
    "    ht = np.zeros((len(stimLoc_unique),trl.shape[-1]))\n",
    "    ind = np.where(stimLoc_unique == sl)[0]\n",
    "    ht[ind,n_pre_behav-1]=1 \n",
    "    trl_ht = np.r_[trl,ht]\n",
    "    blah = np.pad(trl_ht,((0,0),(0,pad_post)), mode = 'constant')\n",
    "    trl_prog = np.ones((1,blah.shape[-1]))*(count/ir_trl.shape[0])\n",
    "    session_now = sessionIdx[count-1]\n",
    "    session_idx = np.zeros((nSessions,blah.shape[-1]))*(count/ir_trl.shape[0])\n",
    "    session_idx[session_now-1,:] = 1\n",
    "    foo.append(np.r_[blah,trl_prog, session_idx])\n",
    "    count += 1\n",
    "ir_trl = np.array(foo)\n",
    "ir_ser = np.concatenate(ir_trl,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Display impulse trains & other regressors\n",
    "getStimName = lambda s: 'Head' if s == 'h' else 'Tail'\n",
    "t_full = np.arange(ir_ser.shape[-1])*(1/Fs_behav)\n",
    "yOff = util.yOffMat(ir_ser)\n",
    "# ytl = np.arange(ir_ser.shape[0])\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.plot(t_full, (ir_ser-yOff).T);\n",
    "# ytl = [f'Mot-{i}' for i in np.arange(ir_ser.shape[0]-len(stimLoc_unique)-1)]\n",
    "# ytl = [f'Mot-{i}' for i in np.arange(len(names_ir))]\n",
    "ytl = list(names_ir)\n",
    "ytl.extend([getStimName(s) for s in stimLoc_unique])\n",
    "ytl.extend(['Trial progress'])\n",
    "for idx in sessionIdx_unique:\n",
    "    ytl.extend([f'Session-{idx}'])\n",
    "regNames = ytl\n",
    "yt = -np.arange(ir_ser.shape[0])\n",
    "plt.yticks(yt, regNames)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Impulse responses & other regressors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read saved dataframe if continuing from here\n",
    "file_df = ft.findAndSortFilesInDir(path_now, ext = 'pickle', search_str='dataFrame')[-1]\n",
    "%time df = pd.read_pickle(os.path.join(path_now,file_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% CIRF in slightly subSampled behavAndScan time, followed by convolution to generate regressors\n",
    "tLen = 6 # Length of kernel\n",
    "tau_rise = 0.2 # Rise constant\n",
    "tau_decay = 1 # Decay constant\n",
    "dt_behav = 1/500\n",
    "# dt_behav_new = 1/50\n",
    "\n",
    "### CIRF\n",
    "t_cirf = np.arange(0,tLen,dt_behav)\n",
    "cirf = spt.generateEPSP(t_cirf,tau_rise, tau_decay,1,0)\n",
    "\n",
    "regressors= []\n",
    "for y in ir_ser[:-1]:\n",
    "    regressors.append(spt.standardize(np.convolve(y, cirf, mode = 'full')[:len(y)]))\n",
    "regressors.append(ir_ser[-1])\n",
    "regressors = np.array(regressors)\n",
    "# t = np.arange(ir_ser.shape[-1])*dt_behav\n",
    "# t_sub = np.arange(t[0], t[-1],dt_behav_new)\n",
    "\n",
    "ca_trl = np.array([np.array(_) for _ in np.array(df['ca'])])\n",
    "t_behav = np.linspace(0,1,regressors.shape[1])\n",
    "t_ca = np.linspace(0,1,ca_trl.shape[0]*ca_trl.shape[1])\n",
    "\n",
    "regressors = superSample_arr(t_behav, regressors, t_ca)\n",
    "\n",
    "%time ca_ser = serializeHyperstack(np.concatenate(ca_trl,axis = 0))\n",
    "# t_behav = np.arange(regressors.shape[-1])*dt_behav_new\n",
    "# t_ca = np.linspace(t_behav[0], t_behav[-1], ca_ser.shape[1])\n",
    "\n",
    "if 'hFilePath' not in locals():\n",
    "    hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "    hFilePath = os.path.join(path_now, hFileName)\n",
    "    \n",
    "with h5py.File(hFilePath, mode = 'r+') as hFile:\n",
    "    if 'regression' in hFile:\n",
    "        del hFile['regression']\n",
    "    grp = hFile.create_group('regression')   \n",
    "    grp.create_dataset('regressors', data = regressors.T)\n",
    "    grp.create_dataset('regressor_names', data = util.to_ascii(regNames))\n",
    "    grp.create_dataset('impulse_trains', data = ir_ser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot all regressors\n",
    "yOff = util.yOffMat(regressors)\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.plot(t_ca,(regressors-yOff).T)\n",
    "plt.xlim(t_ca.min(), t_ca.max())\n",
    "plt.yticks(-yOff, ytl)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Regressors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = [10,11]\n",
    "# inds_diff = np.setdiff1d(np.arange(regressors.shape[0]), inds)\n",
    "# V = regressors[inds].T\n",
    "# W = regressors[inds_diff].T\n",
    "# W_orth = spt.linalg.orthogonalizeOnSpace(V,W).T\n",
    "# X_stimHT_orth = np.c_[V,W_orth]\n",
    "# print(X_stimHT_orth.shape)\n",
    "\n",
    "# yOff = util.yOffMat(X.T).T\n",
    "# plt.figure(figsize = (20,8))\n",
    "# plt.plot(X-yOff)\n",
    "\n",
    "# yOff = util.yOffMat(X_stimHT_orth.T).T\n",
    "# plt.figure(figsize = (20,8))\n",
    "# plt.plot(X_stimHT_orth-yOff);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read saved dataframe if continuing from here\n",
    "file_df = ft.findAndSortFilesInDir(path_now, ext = 'pickle', search_str='dataFrame')[-1]\n",
    "%time df = pd.read_pickle(os.path.join(path_now,file_df))\n",
    "%time ca_trl = np.array([np.array(_) for _ in np.array(df['ca'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Denoise and filter images before regression\n",
    "filtSize = 0.75\n",
    "\n",
    "images_reg_ser = ca_trl.reshape(-1, *ca_trl.shape[2:])\n",
    "images_reg_ipca_flt = []\n",
    "for iSlc, slc in enumerate(np.swapaxes(images_reg_ser,0,1)):\n",
    "    print(f'{iSlc + 1}/{images_reg_ser.shape[1]}')\n",
    "    slc_den = volt.denoise_ipca(slc)\n",
    "    slc_flt = volt.img.gaussFilt(slc_den, sigma = filtSize)\n",
    "    images_reg_ipca_flt.append(slc_flt)\n",
    "\n",
    "images_reg_ipca_flt = np.swapaxes(np.array(images_reg_ipca_flt),0,1)\n",
    "\n",
    "if 'hFilePath' not in locals():\n",
    "    hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "    hFilePath = os.path.join(path_now, hFileName)\n",
    "    \n",
    "with h5py.File(hFilePath, mode = 'r+') as hFile:\n",
    "    keyName = f'images_reg_ipca_flt_sigma-{int(filtSize*100)}'\n",
    "    if keyName in hFile:\n",
    "        del hFile[keyName]\n",
    "    %time hFile.create_dataset(keyName, data = images_reg_ipca_flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Read relevant variables for regression\n",
    "hFileName = ft.findAndSortFilesInDir(path_now, ext = 'h5', search_str='procData')[-1]\n",
    "hFilePath = os.path.join(path_now, hFileName)\n",
    "with h5py.File(hFilePath, mode = 'r') as hFile:\n",
    "    print(hFile.keys())\n",
    "    images = np.array(hFile[f'images_reg_ipca_flt_sigma-{int(filtSize*100)}'])\n",
    "    X_reg = np.array(hFile['regression/regressors'])\n",
    "    regNames = util.to_utf(np.array(hFile['regression/regressor_names']))\n",
    "    if 'images_reg_ipca_flt' in locals():\n",
    "        del images_reg_ipca_flt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Regress\n",
    "\n",
    "ca_ser = images.reshape(images.shape[0],-1)\n",
    "%time regObj = regress(X_reg,ca_ser, n_jobs=-1, fit_intercept=True)\n",
    "\n",
    "betas_vol = betasToVol(regObj.coef_, images.shape[-3:])\n",
    "intercept_vol = betasToVol(regObj.intercept_, images.shape[-3:])\n",
    "t_vol = betasToVol(regObj.T_, images.shape[-3:])\n",
    "r_vol = betasToVol(regObj.Rsq_adj_,images.shape[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iReg = 23\n",
    "q_max = 99\n",
    "q_min = 10\n",
    "print(regNames[iReg])\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(spt.stats.saturateByPerc(betas_vol[iReg][1:].max(axis= 0), perc_up = q_max, perc_low = q_min))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save regression images\n",
    "figDir = os.path.join(path_now, f'figs/regression_ipca_flt_sigma-{int(filtSize*100)}')\n",
    "t_mult = 1000 # Multiply t-values by this value before converting to integer type because of low bit-depth otherwise\n",
    "\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "### First save coefficients\n",
    "foo = betas_vol.astype(int)\n",
    "dir_now = os.path.join(figDir, 'betas')\n",
    "if not os.path.exists(dir_now):\n",
    "    os.mkdir(dir_now)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_coef.tif'),vol[1:])\n",
    "tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor_intercept_coef.tif'),intercept_vol)\n",
    "    \n",
    "foo = ((t_vol*t_mult).astype(int))[1:]\n",
    "dir_now = os.path.join(figDir, 'tValues')\n",
    "if not os.path.exists(dir_now):\n",
    "    os.mkdir(dir_now)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_tVals.tif'),vol[1:])\n",
    "tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor_intercept_T.tif'),foo[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run regression with normalize parameter set to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Regress\n",
    "\n",
    "ca_ser = images.reshape(images.shape[0],-1)\n",
    "%time regObj = regress(X_reg,ca_ser, n_jobs=-1, fit_intercept=True, normalize = True) # NB: If fit_intercept= False, then normalize is ignored\n",
    "\n",
    "betas_vol = betasToVol(regObj.coef_, images.shape[-3:])\n",
    "intercept_vol = betasToVol(regObj.intercept_, images.shape[-3:])\n",
    "t_vol = betasToVol(regObj.T_, images.shape[-3:])\n",
    "r_vol = betasToVol(regObj.Rsq_adj_,images.shape[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iReg = 0\n",
    "q_max = 95\n",
    "print(regNames[iReg])\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(spt.stats.saturateByPerc(betas_vol[iReg].max(axis= 0), perc_up = q_max))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save regression images\n",
    "figDir = os.path.join(path_now, f'figs/regression_ipca_flt_sigma-{int(filtSize*100)}_normalizedRegressors')\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "### First save coefficients\n",
    "foo = np.round(betas_vol).astype(int)\n",
    "dir_now = os.path.join(figDir, 'betas')\n",
    "if not os.path.exists(dir_now):\n",
    "    os.mkdir(dir_now)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_coef.tif'),vol[1:])\n",
    "tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor_intercept_coef.tif'),\n",
    "           intercept_vol.astype(int)[1:])\n",
    "    \n",
    "foo = (np.round(t_vol).astype(int))[1:]\n",
    "dir_now = os.path.join(figDir, 'tValues')\n",
    "if not os.path.exists(dir_now):\n",
    "    os.mkdir(dir_now)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_tVals.tif'),vol[1:])\n",
    "tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor_intercept_T.tif'),foo[0][1:])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrls = df.shape[0]\n",
    "trlLen = X_reg.shape[0]//nTrls\n",
    "X_reg_trl = X_reg.reshape(nTrls, trlLen, -1)\n",
    "stimLoc = np.array(df.stimLoc)\n",
    "inds_mot = util.findStrInList('Mot',util.to_utf(regNames))\n",
    "inds_nonMot = np.setdiff1d(np.arange(X_reg.shape[1]),inds_mot)\n",
    "inds_head = np.where(stimLoc== 'h')[0]\n",
    "inds_tail = np.where(stimLoc == 't')[0]\n",
    "bool_head = np.zeros_like(X_reg_trl)\n",
    "bool_tail = np.zeros_like(X_reg_trl)\n",
    "bool_head[inds_head] = 1\n",
    "bool_tail[inds_tail]= 1\n",
    "reg_head = (bool_head*X_reg_trl).sum(axis = 2).flatten()\n",
    "reg_tail = (bool_tail*X_reg_trl).sum(axis = 2).flatten()\n",
    "X_reg_ht_motor = spt.standardize(np.c_[reg_head,reg_tail, X_reg[:,inds_nonMot]],axis = 0)\n",
    "regNames_ht_motor = np.union1d(['head_motor', 'tail_motor'], regNames[inds_nonMot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(X_reg_ht_motor-np.arange(X_reg_ht_motor.shape[1]).reshape(1,-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save regression images\n",
    "figDir = os.path.join(path_now, 'figs/regression_den_flt_ht_motor')\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "### First save coefficients\n",
    "foo = betas_vol.astype(int)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(figDir,f'Fig-{util.timestamp()}_regressor-{regNames_ht_motor[iReg]}_coef.tif'),vol)\n",
    "\n",
    "foo = t_vol.astype(int)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(figDir,f'Fig-{util.timestamp()}_regressor-{regNames_ht_motor[iReg]}_tVals.tif'),vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = regObj.T_.T[1:]\n",
    "for iReg, r in enumerate(foo):\n",
    "    vol = pxlsToVol(r, volDims)\n",
    "    vol_int = (spt.zscore(vol)*(2**16)-1).astype('int32')\n",
    "#     vol_int  = np.round(vol).astype('int32')\n",
    "    tff.imsave(os.path.join(figDir,f'Fig-{util.timestamp()}_regressor-{iReg}_T.tif'),vol_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_real = np.random.choice(np.arange(-100,100), size = X_reg.shape[1], replace = True)\n",
    "Y_real = X_reg@betas_real.reshape(-1,1)\n",
    "Y_real = Y_real + np.random.randn(*Y_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVals = np.array([r.tvalues for r in reg_ols])\n",
    "tVals = tVals[:,1:]\n",
    "tVals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.linspace(-1,1,1000)\n",
    "t2 = np.linspace(-1,1,1000)\n",
    "X,Y = np.meshgrid(t1,t2)\n",
    "pos = np.dstack([X,Y])\n",
    "x = spt.gaussFun(t1, mu = 1)\n",
    "y = spt.gaussFun(t2, sigma = 1.1)\n",
    "# x = np.sin(2*np.pi*t1)\n",
    "# y = np.cos(2*np.pi*t1)\n",
    "V = np.c_[x,y].T\n",
    "S = np.abs(np.cov(V))\n",
    "# S = np.eye(2)\n",
    "N = np.linalg.pinv(S)\n",
    "T = np.c_[t1,t2].T\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = multivariate_normal(cov = S, mean = [0,0])\n",
    "foo = mv.pdf(pos)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.contourf(foo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
