{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import caiman as cm\n",
    "import h5py\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tifffile as tff\n",
    "import joblib\n",
    "\n",
    "codeDir = r'V:/code/python/code'\n",
    "sys.path.append(codeDir)\n",
    "import apCode.FileTools as ft\n",
    "import apCode.volTools as volt\n",
    "from apCode.machineLearning import ml as mlearn\n",
    "import apCode.behavior.FreeSwimBehavior as fsb\n",
    "import apCode.behavior.headFixed as hf\n",
    "import apCode.SignalProcessingTools as spt\n",
    "import apCode.geom as geom\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "from apCode import util as util\n",
    "from apCode import hdf\n",
    "from apCode.imageAnalysis.spim import regress\n",
    "from apCode.behavior import gmm as my_gmm\n",
    "\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Setting seed for reproducability\n",
    "seed = 143\n",
    "random.seed = seed\n",
    "\n",
    "print(time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Read the excel sheet with the data paths*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Path to excel sheet storing paths to data and other relevant info\n",
    "dir_xls = r'\\\\Koyama-S2\\Data3\\Avinash\\Projects\\RS recruitment\\GCaMP imaging'\n",
    "path_xls =  glob.glob(os.path.join(dir_xls, 'GCaMP volumetric imaging summary*.xlsx'))[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Select a fish to run regression analysis on*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read xl file\n",
    "idx_fish = 14\n",
    "xls = pd.read_excel(path_xls, sheet_name='Sheet1')\n",
    "dir_fish = np.array(xls.loc[xls.FishIdx == idx_fish].Path)[0]\n",
    "print(dir_fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Read registered Ca$^{2+}$ images. If not already registered, then register and read*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_hdf = glob.glob(dir_fish + \"/procData*.h5\")[-1]\n",
    "reg_ca = False\n",
    "with h5py.File(path_hdf, mode='r') as hFile:\n",
    "    if 'ca_reg' not in hFile:\n",
    "        reg_ca = True \n",
    "if reg_ca:\n",
    "    %time path_hdf = hf.register_piecewise_from_hdf(path_hdf)\n",
    "\n",
    "df_ca = dict(ca=[], trlIdx=[], sessionIdx=[], stimLoc=[], trlLen_ca=[])\n",
    "with h5py.File(path_hdf, mode='r') as hFile:\n",
    "    trlIdx = np.array(hFile['trlIdx_ca'])\n",
    "    sessionIdx = np.array(hFile['sessionIdx'])\n",
    "    stimLoc = util.to_utf(np.array(hFile['stimLoc']))\n",
    "    trlLen_ca = np.min(hFile['nImgsInTrl_ca'])\n",
    "    tss = [[trl, sess, stim] for trl, sess, stim in zip(trlIdx, sessionIdx, stimLoc)]\n",
    "    tss_un, inds_un = np.unique(tss, axis=0, return_index=True)\n",
    "    tss_un = tss_un[np.argsort(inds_un)]\n",
    "    nTrls = len(tss_un)\n",
    "    for iTrl, tss_ in enumerate(tss_un):\n",
    "        print(f'Trl {iTrl+1}/{nTrls}' )\n",
    "        trl, sess, stim = int(tss_[0]), int(tss_[1]), tss_[2]\n",
    "        inds = np.where((trlIdx==trl) & (sessionIdx==sess) & (stimLoc==stim))[0]\n",
    "        ca_ = np.array(hFile['ca_reg'][:, inds]).swapaxes(0, 1)[:trlLen_ca]\n",
    "        df_ca['ca'].append([ca_])\n",
    "        df_ca['trlIdx'].append(trl)\n",
    "        df_ca['sessionIdx'].append(sess)\n",
    "        df_ca['stimLoc'].append(stim)\n",
    "        df_ca['trlLen_ca'].append(trlLen_ca)\n",
    "df_ca['trlIdx_glob'] = np.arange(nTrls)\n",
    "df_ca = pd.DataFrame(df_ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Now read behavior and merge imaging and behavior into a single dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ta = {}\n",
    "with h5py.File(path_hdf, mode='r') as hFile:\n",
    "    stimLoc = util.to_utf(np.array(hFile['behav/stimLoc']))\n",
    "    sess, stim = zip(*[sl.split(\"_\") for sl in stimLoc])\n",
    "    sess = np.array(sess).astype(int)-1\n",
    "    stim = np.array(stim)\n",
    "    df_ta['sessionIdx'] = sess\n",
    "    df_ta['stimLoc'] = stim\n",
    "    df_ta['trlIdx_glob'] = np.arange(len(sess))\n",
    "    ta = np.array(hFile['behav/tailAngles'])\n",
    "    nTrls = ta.shape[0]//50\n",
    "    ta_trl = np.vsplit(ta, nTrls)\n",
    "    df_ta['tailAngles'] = ta_trl\n",
    "    df_ta['trlLen_behav'] = np.repeat(ta.shape[1], nTrls)\n",
    "df_ta = pd.DataFrame(df_ta)\n",
    "df = pd.merge(df_ca, df_ta, how='inner')\n",
    "\n",
    "# Delete large redundant variable from memory\n",
    "if 'df' in locals():\n",
    "    if 'df_ca' in locals():\n",
    "        del df_ca\n",
    "    if 'df_ta' in locals():\n",
    "        del df_ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Optionally, denoise $Ca^{2+}$ images (can take upto an hour)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ca = np.squeeze([np.array(_) for _ in np.array(df.ca)])\n",
    "ca = ca.reshape(-1, *ca.shape[-3:]).swapaxes(0, 1)\n",
    "\n",
    "ca_den =[]\n",
    "for iSlc, slc in enumerate(ca):\n",
    "    print(f'Slc {iSlc+1}/{ca.shape[0]}')\n",
    "    ca_den.append(volt.denoise_ipca(slc))\n",
    "ca_den = np.array(ca_den)\n",
    "ca_den = ca_den.swapaxes(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Play movie to see how the raw and filted images compare*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iSlc = 16\n",
    "slc = np.concatenate((ca[iSlc], ca_den[:, iSlc]), axis=1)\n",
    "cm.movie(slc, fr=10).play(magnification=1.5, q_max=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Start by defining some useful functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Some useful functions\n",
    "\n",
    "\n",
    "def padIr(ir_trl, pad_pre, pad_post):\n",
    "    \"\"\"\n",
    "    Pads the impulse response timeseries obtained from \n",
    "    predictions on behavioral feature matrix to match\n",
    "    time length with ca responses\n",
    "    \"\"\"\n",
    "    ir_ser = []\n",
    "    for c in ir_trl:\n",
    "        ir_ser.append(np.pad(c,((0,0),(pad_pre, pad_post))).flatten())\n",
    "    return np.array(ir_ser)\n",
    "\n",
    "def serializeHyperstack(vol):\n",
    "    \"\"\"\n",
    "    Given, a hyperstack, returns a 2D array with pixels serialized for regression, etc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vol: array, (nTimePoints, nSlices, nRows, nCols)\n",
    "    Returns\n",
    "    -------\n",
    "    vol_ser: array, (nTimePoints, nPixels)\n",
    "    \"\"\"\n",
    "    vol_trans = np.transpose(vol,(2,3,1,0))\n",
    "    vol_ser = vol_trans.reshape(-1, vol_trans.shape[-1])\n",
    "    vol_ser = np.swapaxes(vol_ser,0,1)\n",
    "    return vol_ser\n",
    "\n",
    "def deserializeToHyperstack(arr, volDims):\n",
    "    \"\"\"\n",
    "    Given an array which \n",
    "    \"\"\"\n",
    "    volDims = (np.array(volDims))[[1,2,0]]\n",
    "    vol = arr.reshape(arr.shape[0],*volDims)\n",
    "    vol = np.transpose(vol,(0,3,1,2))\n",
    "    return vol\n",
    "\n",
    "def pxlsToVol(pxls, volDims):\n",
    "    \"\"\"    \n",
    "    \"\"\"\n",
    "    volDims = (np.array(volDims))[[1, 2, 0]]\n",
    "    vol = pxls.reshape(*volDims)\n",
    "    vol = np.transpose(vol,(2,0,1))\n",
    "    return vol\n",
    "\n",
    "\n",
    "def resample(t, y, tt):\n",
    "    \"\"\"Super sample a signal using interpolation\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.interpolate import interp1d\n",
    "    t = np.concatenate((tt[0].reshape((-1,)), t, tt[-1].reshape((-1,))))\n",
    "    y = np.concatenate((np.array(0).reshape((-1,)),y,np.array(0).reshape((-1,))))\n",
    "    f = interp1d(t,y,kind = 'slinear')\n",
    "    return f(tt)\n",
    "\n",
    "\n",
    "def regOutsToVol(ro, volDims):\n",
    "    if np.ndim(ro)<2:\n",
    "        ro = ro[:, np.newaxis]  \n",
    "    ro = ro.T\n",
    "    vol = []\n",
    "    for _ in ro:\n",
    "        vol.append(pxlsToVol(_, volDims))\n",
    "    vol = np.squeeze(vol)\n",
    "    return vol\n",
    "\n",
    "\n",
    "def convolve_trlwise(ir_trl, ker, regInds):\n",
    "    \"\"\"\n",
    "    Convolve impulse trains with Ca kenel, trial-by_tril\n",
    "    Parameters\n",
    "    ----------\n",
    "    ir_trl: array, (nTrls, nRegressors, nTimePtsInTrl)\n",
    "    ker: array, (kernelLen, )\n",
    "    regInds: array, (n, )\n",
    "        Indices of regressors to convolve\n",
    "    Returns\n",
    "    --------\n",
    "    reg_trl: array, (*ir_trl.shape)\n",
    "    \"\"\"\n",
    "    func = lambda x, ker: np.convolve(x, ker, mode='full')[:len(x)]\n",
    "    reg_trl=[]\n",
    "    for trl in ir_trl:\n",
    "        reg_reg=[]\n",
    "        for iReg, reg in enumerate(trl):\n",
    "            if iReg in regInds:\n",
    "                y = dask.delayed(func)(reg, ker)\n",
    "            else:\n",
    "                y = reg\n",
    "            reg_reg.append(y)\n",
    "        reg_trl.append(reg_reg)\n",
    "    reg_trl = dask.compute(*reg_trl)\n",
    "    return np.array(reg_trl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Load the GMM model and predict labels on tail angles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_gmm = os.path.join(dir_xls, 'Group')\n",
    "path_gmm = glob.glob(os.path.join(dir_gmm, 'gmm_headFixed_*.pkl'))[-1]\n",
    "gmm_model = joblib.load(path_gmm)\n",
    "\n",
    "ta_trl = np.array([np.array(_) for _ in df.tailAngles])\n",
    "nTrls = len(ta_trl)\n",
    "ta = np.concatenate(ta_trl, axis=1)\n",
    "%time ta = hf.cleanTailAngles(ta, svd=gmm_model.svd)[0]\n",
    "ta_trl = np.array(np.hsplit(ta, nTrls))\n",
    "labels, features = gmm_model.predict(ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Make a set of labels-based impulse response functions for regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tPeriStim_behav = (-1, 6) # Pre- and pos-stim periods in seconds for behavior trials\n",
    "tPeriStim_ca = (-1, 10) # Pre- and post-stim periods in seconds for ca trials\n",
    "Fs_behav = 500\n",
    "\n",
    "getStimName = lambda s: 'Head' if s == 'h' else 'Tail'\n",
    "\n",
    "ir, names_ir = hf.impulse_trains_from_labels(labels, ta, split_lr=False)\n",
    "\n",
    "pad_post = int((tPeriStim_ca[-1]-tPeriStim_behav[-1])*Fs_behav)\n",
    "n_pre_behav = int(np.abs(tPeriStim_behav[0])*Fs_behav)\n",
    "stimLoc = np.array(df.stimLoc)\n",
    "stimLoc_unique = np.unique(stimLoc)\n",
    "sessionIdx  = np.array(df.sessionIdx)\n",
    "sessionIdx_unique = np.unique(sessionIdx)\n",
    "nSessions = len(sessionIdx_unique)\n",
    "\n",
    "nTrls = df.shape[0]\n",
    "ir_trl = np.transpose(ir.reshape(ir.shape[0], nTrls,-1),(1 ,0, 2))\n",
    "names_ir = list(names_ir)\n",
    "foo = []\n",
    "count = 1\n",
    "for sl, trl in zip(stimLoc, ir_trl):\n",
    "    ht = np.zeros((len(stimLoc_unique), trl.shape[-1]))\n",
    "    ind = np.where(stimLoc_unique == sl)[0]\n",
    "    ht[ind, n_pre_behav-1]=1 \n",
    "    trl_ht = np.r_[trl, ht]\n",
    "    blah = np.pad(trl_ht,((0,0),(0,pad_post)), mode = 'constant')\n",
    "    session_now = sessionIdx[count-1]\n",
    "    session_idx = np.zeros((nSessions,blah.shape[-1]))*(count/ir_trl.shape[0])\n",
    "    session_idx[session_now-1,:] = 1\n",
    "    foo.append(np.r_[blah, session_idx])\n",
    "    count += 1\n",
    "ir_trl = np.array(foo)\n",
    "ir_ser = np.concatenate(ir_trl,axis = 1)\n",
    "\n",
    "names_ir = list(names_ir)\n",
    "names_ir.extend([getStimName(s) for s in stimLoc_unique])\n",
    "for idx in sessionIdx_unique:\n",
    "    names_ir.extend([f'Session-{idx}'])\n",
    "regNames = names_ir.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot to see what these look like* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Display impulse trains and other regressors\n",
    "t_full = np.arange(ir_ser.shape[-1])*(1/Fs_behav)\n",
    "yOff = util.yOffMat(ir_ser)\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(t_full, (ir_ser-yOff).T)\n",
    "yt = -np.arange(ir_ser.shape[0])\n",
    "plt.yticks(yt, regNames)\n",
    "plt.xlim(t_full[0], t_full[-1])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Impulse responses & other regressors', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Convolve trial-by-trial with $Ca^{2+}$ kernel to produce final regressors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% CIRF in slightly subSampled behavAndScan time, followed by convolution to generate regressors\n",
    "tLen = 6 # Length of kernel\n",
    "tau_rise = 0.2 # Rise constant\n",
    "tau_decay = 1 # Decay constant\n",
    "dt_behav = 1/500\n",
    "\n",
    "### CIRF\n",
    "t_cirf = np.arange(0, tLen, dt_behav)\n",
    "cirf = spt.generateEPSP(t_cirf, tau_rise, tau_decay, 1, 0)\n",
    "\n",
    "ind = util.findStrInList('session', regNames, case_sensitive=False)[0]\n",
    "regInds = np.arange(ind)\n",
    "%time regressors = convolve_trlwise(ir_trl, cirf, regInds)\n",
    "regressors = np.concatenate(regressors, axis=1)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "regressors = scaler.fit_transform(regressors.T).T\n",
    "\n",
    "\n",
    "%time ca_ser = serializeHyperstack(ca_den)\n",
    "\n",
    "t_behav = np.linspace(0, 1, regressors.shape[1])\n",
    "t_ca = np.linspace(0, 1, ca_ser.shape[0])\n",
    "\n",
    "\n",
    "regressors = dask.compute(*[dask.delayed(resample)(t_behav, reg, t_ca) for reg in regressors])\n",
    "regressors = np.array(regressors)\n",
    "\n",
    "\n",
    "if 'path_hdf' not in locals():\n",
    "    path_hdf = glob.glob(os.path.join(dir_fish, 'procData*.h5'))[-1]\n",
    "       \n",
    "with h5py.File(path_hdf, mode = 'r+') as hFile:\n",
    "    if 'regression' in hFile:\n",
    "        del hFile['regression']\n",
    "    grp = hFile.create_group('regression')   \n",
    "    grp.create_dataset('regressors', data=regressors.T)\n",
    "    grp.create_dataset('regressor_names', data=util.to_ascii(regNames))\n",
    "    grp.create_dataset('impulse_trains', data=ir_ser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot regressors* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot all regressors\n",
    "yOff = util.yOffMat(regressors)\n",
    "plt.figure(figsize = (20, 15))\n",
    "plt.plot(t_ca, (regressors-yOff).T)\n",
    "plt.xlim(t_ca.min(), t_ca.max())\n",
    "plt.yticks(-yOff, regNames)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Regressors');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Filter images a bit to improve regression (optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filtSize = 0.5\n",
    "\n",
    "ca_den_flt = []\n",
    "for iSlc, slc in enumerate(ca_den.swapaxes(0, 1)):\n",
    "    print(f'{iSlc + 1}/{ca_den.shape[1]}')\n",
    "    slc_flt = volt.img.gaussFilt(slc, sigma=filtSize)\n",
    "    ca_den_flt.append(slc_flt)\n",
    "ca_den_flt = np.array(ca_den_flt).swapaxes(0, 1)\n",
    "\n",
    "ca_ser = serializeHyperstack(ca_den)\n",
    "\n",
    "if 'path_hdf' not in locals():\n",
    "    path_hdf = glob.glob(os.path.join(dir_fish, 'procData*.h5'))[-1]\n",
    "    \n",
    "with h5py.File(path_hd, mode = 'r+') as hFile:\n",
    "    keyName = f'ca_den_flt_sigma-{int(filtSize*100)}'\n",
    "    if keyName in hFile:\n",
    "        del hFile[keyName]\n",
    "    %time hFile.create_dataset(keyName, data=ca_den_flt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Regress*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Regress\n",
    "%time regObj = regress(regressors.T, ca_ser, n_jobs=-1, fit_intercept=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reshape regression outputs into volumes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_vol = regOutsToVol(regObj.coef_, ca_den.shape[-3:])\n",
    "intercept_vol = regOutsToVol(regObj.intercept_, ca_den.shape[-3:])\n",
    "t_vol = regOutsToVol(regObj.T_, ca_den.shape[-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Plot max-int z projections of regression outputs for quick visual examination*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iReg = 5\n",
    "q_max = 99\n",
    "q_min = 10\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(spt.stats.saturateByPerc(betas_vol[iReg].max(axis=0), perc_up=q_max, perc_low=q_min))\n",
    "plt.title(f'Regressors: {regNames[iReg]}')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save beta and t-value maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Save regression images\n",
    "figDir = os.path.join(dir_fish, f'figs/regression')\n",
    "os.makedirs(figDir, exist_ok=True)\n",
    "\n",
    "### First save coefficients\n",
    "foo = betas_vol.astype('float32')\n",
    "dir_now = os.path.join(figDir, 'betas')\n",
    "os.makedirs(dir_now, exist_ok=True)\n",
    "\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now, f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_coef.tif'),vol[1:])\n",
    "tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor_intercept.tif'), intercept_vol)\n",
    "    \n",
    "foo = t_vol.astype('float32')[1:]\n",
    "dir_now = os.path.join(figDir, 'tValues')\n",
    "os.makedirs(dir_now, exist_ok=True)\n",
    "for iReg, vol in enumerate(foo):\n",
    "    tff.imsave(os.path.join(dir_now,f'Fig-{util.timestamp()}_regressor-{regNames[iReg]}_tVals.tif'),vol[1:])\n",
    "\n",
    "print(f'Saved at \\n{figDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
